[
["index.html", "Foundations of Applied Statistics Data Analysis, Inference, and Modeling", " Foundations of Applied Statistics Data Analysis, Inference, and Modeling John D. Storey Created 2017-02-01; Last modified 2020-02-19 "],
["preface.html", "Preface", " Preface Hello, current and future statistics enthusiasts. I’m a professor at Princeton University who works at the interface of statistics, genetics, and genomics. I have worked with undergraduate and graduate students who come from many backgrounds: biology, computer science, engineering, mathematics, physics, and statistics. The goal for my students is to turn them into skilled applied statsiticians who are interested in tackling real world problems. The content of this book is what I want them to learn as soon as possible after joining my lab. More generally, this book establishes a foundation in applied statistics and data science for those interested in pursuing data-driven research. I have included many different types of data sets here, but the book places a special emphasis on modern biological problems and data sets due to my particular interests. This book also serves as the primary reference for my Princeton University course titled, Foundations of Statistical Genomics, which has a web site, http://jdstorey.org/fsg/. The content of this book started with slides I created using revealjs; these slides can be found at http://jdstorey.org/asdscourse2017/lectures/, and the source code at https://github.com/jdstorey/asdslectures. Although, I no longer use slides to teach this course, I was nevertheless able to easily move the R Markdown used to make the slides into a book format, using the bookdown package. The current draft of this book contains most of the statistics and R code that I intend to include. However, because it is being build from slides, there are two things to keep in mind. First, I will be adding a substantial amount of exposition and data analyses in the book. It currently mostly reads as terse slides. I will also be modifying some of the formatting to better suit bookdown. Second, it is common to quote from sources verbatim when making slides (citing the source, of course). It is not so common to do this in a book. Threfore, you will see more material quoted verbatim from outside sources than is typical in a book. I intend to remedy this over time. This books is organized into several parts: Introduction Explortatory data analysis Probability Frequentist inference Bayesian inference Numerical methods for likelihood Nonparametric inference Statistical models High-dimensional inference Latent variable models Source Files The source files for this book are maintained on GitHub: https://github.com/jdstorey/fas Feel free to visit this repository to help me make the book better. About The Author John Storey received his PhD from Stanford University in statistics with a PhD minor in genetics. He then held faculty positions at the University of California, Berkeley and the University of Washington. Since 2008, he has a been a professor in the Lewis-Sigler Institute for Integrative Genomics at Princeton University. Storey’s research has been concerned with developing and applying statistical methods in genetics and genomics. He has made pioneering contributions to the development and application of methods for significance testing and inference on high-dimensional data. In 2014, Storey was appointed the founding Director of the Center for Statistics and Machine Learning at Princeton University and he was also named the William R. Harman ‘63 and Mary-Love Harman Professor in Genomics. He is an elected fellow of the American Association for the Advancement of Science as well as the Institute of Mathematical Statistics. He is the winner of the 2015 COPSS Presidents’ Award. He is also the winner of the 2015 Mortimer Spiegelman Award given by the American Public Health Association for outstanding contributions to public health statistics. "],
["stat-overview.html", "1 Statistics 1.1 History 1.2 Definition 1.3 Relationship to Machine Learning 1.4 Relationship to Data Science 1.5 Some History On Data Science", " 1 Statistics 1.1 History The practice of statistics has been around for hundreds of years. The early application of statistics was centered around collecting demographic and economic data for governments. The terms “statistics” was coined in the mid-1700s, which is derived from its relationship to state or government data. Early methodological ideas introduced in statistics involved procedures such as calculating the mean or median, representing data graphically, and trying to model the distribution of “errors” between observed data and a model. In the 1800s, probability started to become integrated into statistical thinking, which lead to the era of inferential statistics; it gave rise to deep results on how we design and analyze studies to use finite amounts of data that have been collected through a probabilistic mechanism. During the early to mid 1900s, inferential statistics became a well developed and understood component to statistics. Its ties to probability were deepened, especially with the formal axiomatization and rigorous development of probability. The latter half of the 1900s resulted in another major leap forward for the field of statistics, with the introduction of modern computing. This had a massive impact on how we collect and analyze data. Statistics became less reliant on mathematical models and more immersed in computational approaches. In the 2000s we have witnessed yet another major leap forward. Data collection has never been faster, cheaper, or larger than today. We are able to collect and analyze massive amounts of data in most areas of science and industry. This has led to a sea-change in statistics, both in its scope and in its importance. The field of statistics has never been more challenging or impactful than it is today. 1.2 Definition The modern definition of Statistics is the study of how to extract information from data, including how to collect, organize, analyze, and present information in data. Applied Statistics is concerned with the practical considerations and implementations needed to carry out a statistical analysis. In Chapter 2 below, we make this definition concrete by discussing the various problems that statistics tackles. 1.3 Relationship to Machine Learning From https://en.wikipedia.org/wiki/Machine_learning: Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Machine learning is closely related to and often overlaps with computational statistics; a discipline which also focuses in prediction-making through the use of computers. 1.4 Relationship to Data Science From https://en.wikipedia.org/wiki/Data_science: Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured, which is a continuation of some of the data analysis fields such as statistics, data mining, and predictive analytics. 1.5 Some History On Data Science John Tukey John Tukey pioneered a field called “exploratory data analysis” (EDA) From The Future of Data Analysis (1962) Annals of Mathematical Statistics … For a long time I have thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt. All in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. Data analysis is a larger and more varied field than inference, or incisive procedures, or allocation. Jeff Wu From https://en.wikipedia.org/wiki/Data_science: In November 1997, C.F. Jeff Wu gave the inaugural lecture entitled “Statistics = Data Science?”. In this lecture, he characterized statistical work as a trilogy of data collection, data modeling and analysis, and decision making. In his conclusion, he initiated the modern, non-computer science, usage of the term “data science” and advocated that statistics be renamed data science and statisticians data scientists. William Cleveland From https://en.wikipedia.org/wiki/Data_science: In 2001, William Cleveland introduced data science as an independent discipline, extending the field of statistics to incorporate “advances in computing with data” in his article Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics in International Statistical Review Cleveland establishes six technical areas which he believed to encompass the field of data science: multidisciplinary investigations models and methods for data computing with data pedagogy tool evaluation theory Industry Individuals working in industry began to call themselves “data scientists” in the late 2000’s, but this was far after statisticians had introduced the field. "],
["challenges.html", "2 Components of Applied Statistics 2.1 Study Design 2.2 Data Wrangling 2.3 Data Analysis 2.4 Communication", " 2 Components of Applied Statistics Let’s first cosnider the central challenges of applied statistics, shown in the following schematic I’ll call the “central dogma of applied statistics”, shown in Figure 2.1 Figure 2.1: Central Dogma of Statistics 2.1 Study Design An applied statistics project is usually preceded by a scientific question that involves the collection and analysis of data. The design of the data should involve careful application of statistical principles to design which data are to be collected and how the data will be measured. The study design should also be driven by the questions that will be answered and the type of applied statistical analysis techniques will be employed. Study design is an area that is almost solely studied by statisticians and it is one of the core strengths of the field of statistics. We will be considering study design throughout this book. 2.2 Data Wrangling Data wrangling is a new terms that refers to the process of convertng raw data, which is often very messy, into data that can be readily analyzed. The importance of this activity has grown substantially in recent years as data sets have becoem larger and more comoplex. Data wrangling 2.3 Data Analysis 2.3.1 Exploratory Data Analysis 2.3.2 Modeling 2.3.3 Inference 2.3.4 Prediciton 2.4 Communication "],
["data-sets-used-in-this-book.html", "3 Data Sets Used in this Book", " 3 Data Sets Used in this Book I will eventually include several data sets that will be used throughout the book. These data sets will be described here. As of now, all of the R examples are completed with a variety of smaller data sets. Threfore, the R code I demonstrate will not change much, but there will be a more consistent use of data sets that I find interesting. "],
["exploratory-data-analysis-1.html", "4 Exploratory Data Analysis 4.1 What is EDA? 4.2 Descriptive Statistics Examples 4.3 Components of EDA 4.4 Data Sets", " 4 Exploratory Data Analysis 4.1 What is EDA? Exploratory data analysis (EDA) is the process of analzying data to uncover their key features. John Tukey pioneered this framework, writing a seminal book on the topic (called Exploratory Data Analysis). EDA involves calculating numerical summaries of data, visualizing data in a variety of ways, and considering interesting data points. Before any model fitting is done to data, some exploratory data analysis should always be performed. Data science seems to focus much more on EDA than traditional statistics. 4.2 Descriptive Statistics Examples Facebook’s Visualizing Fake Friendships (side note: a discussion) Hans Rosling: Debunking third-world myths with the best stats you’ve ever seen Flowing Data’s A Day in the Life of Americans 4.3 Components of EDA EDA involves calculating quantities and visualizing data for: Checking the n’s Checking for missing data Characterizing the distributional properties of the data Characterizing relationships among variables and observations Dimension reduction Model formulation Hypothesis generation … and there are possible many more activities one can do. 4.4 Data Sets For the majority of this chapter, we will use some simple data sets to demonstrate the ideas. 4.4.1 Data mtcars Load the mtcars data set: &gt; library(&quot;tidyverse&quot;) # why load tidyverse? &gt; data(&quot;mtcars&quot;, package=&quot;datasets&quot;) &gt; mtcars &lt;- as_tibble(mtcars) &gt; head(mtcars) # A tibble: 6 x 11 mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 4.4.2 Data mpg Load the mpg data set: &gt; data(&quot;mpg&quot;, package=&quot;ggplot2&quot;) &gt; head(mpg) # A tibble: 6 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 audi a4 1.8 1999 4 auto(… f 18 29 p comp… 2 audi a4 1.8 1999 4 manua… f 21 29 p comp… 3 audi a4 2 2008 4 manua… f 20 31 p comp… 4 audi a4 2 2008 4 auto(… f 21 30 p comp… 5 audi a4 2.8 1999 6 auto(… f 16 26 p comp… 6 audi a4 2.8 1999 6 manua… f 18 26 p comp… 4.4.3 Data diamonds Load the diamonds data set: &gt; data(&quot;diamonds&quot;, package=&quot;ggplot2&quot;) &gt; head(diamonds) # A tibble: 6 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 4.4.4 Data gapminder Load the gapminder data set: &gt; library(&quot;gapminder&quot;) &gt; data(&quot;gapminder&quot;, package=&quot;gapminder&quot;) &gt; gapminder &lt;- as_tibble(gapminder) &gt; head(gapminder) # A tibble: 6 x 6 country continent year lifeExp pop gdpPercap &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 2 Afghanistan Asia 1957 30.3 9240934 821. 3 Afghanistan Asia 1962 32.0 10267083 853. 4 Afghanistan Asia 1967 34.0 11537966 836. 5 Afghanistan Asia 1972 36.1 13079460 740. 6 Afghanistan Asia 1977 38.4 14880372 786. "],
["numerical-summaries-of-data.html", "5 Numerical Summaries of Data 5.1 Useful Summaries 5.2 Measures of Center 5.3 Mean, Median, and Mode in R 5.4 Quantiles and Percentiles 5.5 Five Number Summary 5.6 Measures of Spread 5.7 Variance, SD, and IQR in R 5.8 Identifying Outliers 5.9 Application to mtcars Data 5.10 Measuring Symmetry 5.11 skewness() Function 5.12 Measuring Tails 5.13 Excess Kurtosis 5.14 kurtosis() Function 5.15 Visualizing Skewness and Kurtosis 5.16 Covariance and Correlation", " 5 Numerical Summaries of Data 5.1 Useful Summaries Center: mean, median, mode Quantiles: percentiles, five number summaries Spread: standard deviation, variance, interquartile range Outliers Shape: skewness, kurtosis Concordance: correlation, quantile-quantile plots 5.2 Measures of Center Suppose we have data points \\(x_1, x_2, \\ldots, x_n\\). Mean: \\[\\overline{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\] Median: Order the points \\(x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}\\). The median is the middle value: - \\(x_{((n+1)/2)}\\) if \\(n\\) is odd - \\((x_{(n/2)} + x_{(n/2+1)})/2\\) if \\(n\\) is even Mode: The most frequently repeated value among the data (if any). If there are ties, then there is more than one mode. 5.3 Mean, Median, and Mode in R Let’s calculate these quantities in R. &gt; mean(mtcars$mpg) [1] 20.09062 &gt; median(mtcars$mpg) [1] 19.2 &gt; &gt; sample_mode &lt;- function(x) { + as.numeric(names(which(table(x) == max(table(x))))) + } &gt; &gt; sample_mode(round(mtcars$mpg)) [1] 15 21 It appears there is no R base function for calculating the mode. 5.4 Quantiles and Percentiles The \\(p\\)th percentile of \\(x_1, x_2, \\ldots, x_n\\) is a number such that \\(p\\)% of the data are less than this number. The 25th, 50th, and 75th percentiles are called 1st, 2nd, and 3rd “quartiles”, respectively. These are sometimes denoted as Q1, Q2, and Q3. The median is the 50th percentile aka the 2nd quartile aka Q2. In general, \\(q\\)-quantiles are cut points that divide the data into \\(q\\) approximately equally sized groups. The cut points are the percentiles \\(1/q, 2/q, \\ldots, (q-1)/q.\\) 5.5 Five Number Summary The “five number summary” is the minimum, the three quartiles, and the maximum. This can be calculated via fivenum() and summary(). They can produce different values. Finally, quantile() extracts any set of percentiles. &gt; fivenum(mtcars$mpg) [1] 10.40 15.35 19.20 22.80 33.90 &gt; summary(mtcars$mpg) Min. 1st Qu. Median Mean 3rd Qu. Max. 10.40 15.43 19.20 20.09 22.80 33.90 &gt; &gt; quantile(mtcars$mpg, prob=seq(0, 1, 0.25)) 0% 25% 50% 75% 100% 10.400 15.425 19.200 22.800 33.900 5.6 Measures of Spread The variance, standard deviation (SD), and interquartile range (IQR) measure the “spread” of the data. Variance: \\[s^2 = \\frac{\\sum_{i=1}^n \\left(x_i - \\overline{x}\\right)^2}{n-1}\\] Standard Deviation: \\(s = \\sqrt{s^2}\\) Iterquartile Range: IQR \\(=\\) Q3 \\(-\\) Q1 The SD and IQR have the same units as the observed data, but the variance is in squared units. 5.7 Variance, SD, and IQR in R Variance: &gt; var(mtcars$mpg) [1] 36.3241 Standard deviation: &gt; sd(mtcars$mpg) [1] 6.026948 Interquartile range: &gt; IQR(mtcars$mpg) [1] 7.375 &gt; diff(fivenum(mtcars$mpg)[c(2,4)]) [1] 7.45 5.8 Identifying Outliers An outlier is an unusual data point. Outliers can be perfectly valid but they can also be due to errors (as can non-outliers). One must define what is meant by an outlier. One definition is a data point that less than Q1 or greater than Q3 by 1.5 \\(\\times\\) IQR or more. Another definition is a data point whose difference from the mean is greater than 3 \\(\\times\\) SD or more. For Normal distributed data (bell curve shaped), the probability of this is less than 0.27%. 5.9 Application to mtcars Data &gt; sd_units &lt;- abs(mtcars$wt - mean(mtcars$wt))/sd(mtcars$wt) &gt; sum(sd_units &gt; 3) [1] 0 &gt; max(sd_units) [1] 2.255336 &gt; &gt; iqr_outlier_cuts &lt;- fivenum(mtcars$wt)[c(2,4)] + + c(-1.5, 1.5)*diff(fivenum(mtcars$wt)[c(2,4)]) &gt; sum(mtcars$wt &lt; iqr_outlier_cuts[1] | + mtcars$wt &gt; iqr_outlier_cuts[2]) [1] 2 5.10 Measuring Symmetry The skewness statistic measures symmetry of the data. It is calculated by: \\[ \\gamma = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^3/n}{s^3} \\] A negative number is left-skewed, and a positive number is right-skewed. Note: Use of \\(n\\) vs. \\(n-1\\) may vary – check the code. 5.11 skewness() Function In R, there is a function call skewness() from the moments package for calculating this statistic on data. &gt; library(moments) &gt; gapminder %&gt;% filter(year==2007) %&gt;% select(gdpPercap) %&gt;% + skewness() gdpPercap 1.211228 &gt; gapminder %&gt;% filter(year==2007) %&gt;% select(gdpPercap) %&gt;% + log() %&gt;% skewness() gdpPercap -0.1524203 &gt; rnorm(10000) %&gt;% skewness() [1] 0.005799917 5.12 Measuring Tails The tails of a distribution are often described as being heavy or light depending on how slowly they descend. This can be measured through statistic called kurtosis: \\[ \\kappa = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^4/n}{s^4} \\] As with skewness \\(\\gamma\\), use of \\(n\\) vs \\(n-1\\) may vary. 5.13 Excess Kurtosis For a standard Normal distribution (mean 0 and standard deviation 1), the kurtosis is on average 3. Therefore, a measure called “excess kurtosis” is defined to be \\(\\kappa - 3\\). A positive value implies heavier tails and a negative value implies lighter tails. 5.14 kurtosis() Function In R, there is a function call kurtosis() from the moments package for calculating this statistic on data. &gt; library(moments) &gt; gapminder %&gt;% filter(year==2007) %&gt;% select(gdpPercap) %&gt;% + kurtosis() gdpPercap 3.29593 &gt; gapminder %&gt;% filter(year==2007) %&gt;% select(gdpPercap) %&gt;% + log() %&gt;% kurtosis() gdpPercap 1.871608 &gt; rnorm(10000) %&gt;% kurtosis() [1] 2.955853 5.15 Visualizing Skewness and Kurtosis 5.16 Covariance and Correlation It is often the case that two or more quantitative variables are measured on each unit of observation (such as an individual). We are then often interested in characterizing how pairs of variables are associated or how they vary together. Two common measures for this are called “covariance” and “correlation”, both of which are most well suited for measuring linear associations 5.16.1 Covariance Suppose we observe \\(n\\) pairs of data \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\). Their sample covariance is \\[ {\\operatorname{cov}}_{xy} = \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{(n-1)}, \\] which meausers how the two variables “covary” about their respective means. Large positive numbers indicate concordance of deviations from the mean, and large negative numbers indicated discordance (so opposite sides of the mean). 5.16.2 Pearson Correlation Pearson correlation is sample covariance scaled by the variables’ standard deviations, meaning correlation is a unitless measure of variation about the mean. It is defined by \\[\\begin{eqnarray} r_{xy} &amp; = &amp; \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2 \\sum_{i=1}^n (y_i - \\overline{y})^2}} \\\\ \\ &amp; = &amp; \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{(n-1) s_x s_y} \\\\ \\ &amp; = &amp; \\frac{ \\operatorname{cov}_{xy}}{s_x s_y} \\end{eqnarray}\\] where \\(s_x\\) and \\(s_y\\) are the sample standard deviations of each measured variable. Note that \\(-1 \\leq r_{xy} \\leq 1\\). 5.16.3 Spearman Correlation There are other ways to measure correlation that are less reliant on linear trends in covariation and are also more robust to outliers. Specifically, one can convert each measured variable to ranks by size (1 for the smallest, \\(n\\) for the largest) and then use a formula for correlation designed for these ranks. One popular measure of rank-based correlation is the Spearman correlation. &gt; x &lt;- rnorm(500) &gt; y &lt;- x + rnorm(500) &gt; cor(x, y, method=&quot;pearson&quot;) [1] 0.7542651 &gt; cor(x, y, method=&quot;spearman&quot;) [1] 0.7499555 &gt; x &lt;- rnorm(500) &gt; y &lt;- x + rnorm(500, sd=2) &gt; cor(x, y, method=&quot;pearson&quot;) [1] 0.5164903 &gt; cor(x, y, method=&quot;spearman&quot;) [1] 0.5093092 &gt; x &lt;- c(rnorm(499), 100) &gt; y &lt;- c(rnorm(499), 100) &gt; cor(x, y, method=&quot;pearson&quot;) [1] 0.9528564 &gt; cor(x, y, method=&quot;spearman&quot;) [1] -0.02133551 "],
["data-visualization-basics.html", "6 Data Visualization Basics 6.1 Plots 6.2 R Base Graphics 6.3 Read the Documentation 6.4 Barplot 6.5 Boxplot 6.6 Constructing Boxplots 6.7 Boxplot with Outliers 6.8 Histogram 6.9 Histogram with More Breaks 6.10 Density Plot 6.11 Boxplot (Side-By-Side) 6.12 Stacked Barplot 6.13 Scatterplot 6.14 Quantile-Quantile Plots 6.15 A Grammar of Graphics", " 6 Data Visualization Basics 6.1 Plots Single variables: Barplot Boxplot Histogram Density plot Two or more variables: Side-by-Side Boxplots Stacked Barplot Scatterplot 6.2 R Base Graphics We’ll first plodding through “R base graphics”, which means graphics functions that come with R. By default they are very simple. However, they can be customized a lot, but it takes a lot of work. Also, the syntax varies significantly among plot types and some think the syntax is not user-friendly. We will consider a very highly used graphics package next week, called ggplot2 that provides a “grammar of graphics”. It hits a sweet spot of “flexibility vs. complexity” for many data scientists. 6.3 Read the Documentation For all of the plotting functions covered below, read the help files. &gt; ?barplot &gt; ?boxplot &gt; ?hist &gt; ?density &gt; ?plot &gt; ?legend 6.4 Barplot &gt; cyl_tbl &lt;- table(mtcars$cyl) &gt; barplot(cyl_tbl, xlab=&quot;Cylinders&quot;, ylab=&quot;Count&quot;) 6.5 Boxplot &gt; boxplot(mtcars$mpg, ylab=&quot;MPG&quot;, col=&quot;lightgray&quot;) 6.6 Constructing Boxplots The top of the box is Q3 The line through the middle of the box is the median The bottom of the box is Q1 The top whisker is the minimum of Q3 + 1.5 \\(\\times\\) IQR or the largest data point The bottom whisker is the maximum of Q1 - 1.5 \\(\\times\\) IQR or the smallest data point Outliers lie outside of (Q1 - 1.5 \\(\\times\\) IQR) or (Q3 + 1.5 \\(\\times\\) IQR), and they are shown as points Outliers are calculated using the fivenum() function 6.7 Boxplot with Outliers &gt; boxplot(mtcars$wt, ylab=&quot;Weight (1000 lbs)&quot;, + col=&quot;lightgray&quot;) 6.8 Histogram &gt; hist(mtcars$mpg, xlab=&quot;MPG&quot;, main=&quot;&quot;, col=&quot;lightgray&quot;) 6.9 Histogram with More Breaks &gt; hist(mtcars$mpg, breaks=12, xlab=&quot;MPG&quot;, main=&quot;&quot;, col=&quot;lightgray&quot;) 6.10 Density Plot &gt; plot(density(mtcars$mpg), xlab=&quot;MPG&quot;, main=&quot;&quot;) &gt; polygon(density(mtcars$mpg), col=&quot;lightgray&quot;, border=&quot;black&quot;) 6.11 Boxplot (Side-By-Side) &gt; boxplot(mpg ~ cyl, data=mtcars, xlab=&quot;Cylinders&quot;, + ylab=&quot;MPG&quot;, col=&quot;lightgray&quot;) 6.12 Stacked Barplot &gt; counts &lt;- table(mtcars$cyl, mtcars$gear) &gt; counts 3 4 5 4 1 8 2 6 2 4 1 8 12 0 2 &gt; barplot(counts, main=&quot;Number of Gears and Cylinders&quot;, + xlab=&quot;Gears&quot;, col=c(&quot;blue&quot;,&quot;red&quot;, &quot;lightgray&quot;)) &gt; legend(x=&quot;topright&quot;, title=&quot;Cyl&quot;, + legend = rownames(counts), + fill = c(&quot;blue&quot;,&quot;red&quot;, &quot;lightgray&quot;)) 6.13 Scatterplot &gt; plot(mtcars$wt, mtcars$mpg, xlab=&quot;Weight (1000 lbs)&quot;, + ylab=&quot;MPG&quot;) 6.14 Quantile-Quantile Plots Quantile-quantile plots display the quantiles of: two samples of data a sample of data vs a theoretical distribution The first type allows one to assess how similar the distributions are of two samples of data. The second allows one to assess how similar a sample of data is to a theoretical distribution (often Normal with mean 0 and standard deviation 1). &gt; qqnorm(mtcars$mpg, main=&quot; &quot;) &gt; qqline(mtcars$mpg) # line through Q1 and Q3 &gt; before1980 &lt;- gapminder %&gt;% filter(year &lt; 1980) %&gt;% + select(lifeExp) %&gt;% unlist() &gt; after1980 &lt;- gapminder %&gt;% filter(year &gt; 1980) %&gt;% + select(lifeExp) %&gt;% unlist() &gt; qqplot(before1980, after1980); abline(0,1) &gt; ggplot(mtcars) + stat_qq(aes(sample = mpg)) &gt; ggplot(gapminder) + stat_qq(aes(sample=lifeExp)) &gt; ggplot(gapminder) + + stat_qq(aes(sample=lifeExp, color=continent)) 6.15 A Grammar of Graphics There are many advanced graphics packages and extensions of R. One popular example is ggplot2, which is a grammar based graphics framework. An introduction to ggplot2 is provided in (YARP, Yet Another R Primer)[https://jdstorey.org/yarp/a-grammar-of-graphics.html]. "],
["eda-of-high-dimensional-data.html", "7 EDA of High-Dimensional Data 7.1 Definition 7.2 Examples 7.3 Big Data vs HD Data 7.4 Definition of HD Data 7.5 Rationale", " 7 EDA of High-Dimensional Data 7.1 Definition High-dimensional data (HD data) typically refers to data sets where many variables are simultaneously measured on any number of observations. The number of variables is often represented by \\(p\\) and the number of observations by \\(n\\). HD data are collected into a \\(p \\times n\\) or \\(n \\times p\\) matrix. Many methods exist for “large \\(p\\), small \\(n\\)” data sets. 7.2 Examples Clinical studies Genomics (e.g., gene expression) Neuroimaging (e.g., fMRI) Finance (e.g., time series) Environmental studies Internet data (e.g., Netflix movie ratings) 7.3 Big Data vs HD Data “Big data” are data sets that cannot fit into a standard computer’s memory. HD data were defined above. They are not necessarily equivalent. 7.4 Definition of HD Data High-dimesional data is a data set where the number of variables measured is many. Large same size data is a data set where few variables are measured, but many observations are measured. Big data is a data set where there are so many data points that it cannot be managed straightforwardly in memory, but must rather be stored and accessed elsewhere. Big data can be high-dimensional, large sample size, or both. We will abbreviate high-dimensional with HD. 7.5 Rationale Exploratory data analysis (EDA) of high-dimensional data adds the additional challenge that many variables must be examined simultaneously. Therefore, in addition to the EDA methods we discussed earlier, methods are often employed to organize, visualize, or numerically capture high-dimensional data into lower dimensions. Examples of EDA approaches applied to HD data include: Traditional EDA methods covered earlier Cluster analysis Dimensionality reduction "],
["cluster-analysis.html", "8 Cluster Analysis 8.1 Definition 8.2 Types of Clustering 8.3 Top-Down vs Bottom-Up 8.4 Challenges 8.5 Illustrative Data Sets 8.6 Distance Measures 8.7 Hierarchical Clustering 8.8 K-Means Clustering", " 8 Cluster Analysis 8.1 Definition Cluster analysis is the process of grouping objects (variables or observations) into groups based on measures of similarity. Similar objects are placed in the same cluster, and dissimilar objects are placed in different clusters. Cluster analysis methods are typically described by algorithms (rather than models or formulas). 8.2 Types of Clustering Clustering can be categorized in various ways: Hard vs. soft Top-down vs bottom-up Partitioning vs. hierarchical agglomerative 8.3 Top-Down vs Bottom-Up We will discuss two of the major clustering methods – hierarchical clustering and K-means clustering. Hierarchical clustering is an example of bottom-up clustering in that the process begings with each object being its own cluster and then objects are joined in a hierarchical manner into larger and larger clusters. \\(K\\)-means clustering is an example of top-down clustering in that the number of clusters is chosen beforehand and then object are assigned to one of the \\(K\\) clusters. 8.4 Challenges Cluster analysis method Distance measure Number of clusters Convergence issues 8.5 Illustrative Data Sets 8.5.1 Simulated data1 8.5.2 “True” Clusters data1 8.5.3 Simulated data2 8.5.4 “True” Clusters data2 8.6 Distance Measures 8.6.1 Objects Most clustering methods require calculating a “distance” between two objects. Let \\(\\pmb{a} = (a_1, a_2, \\ldots, a_n)\\) be one object and \\(\\pmb{b} = (b_1, b_2, \\ldots, b_n)\\) be another object. We will assume both objects are composed of real numbers. 8.6.2 Euclidean Euclidean distance is the shortest spatial distance between two objects in Euclidean space. Euclidean distance is calculated as: \\[d(\\pmb{a}, \\pmb{b}) = \\sqrt{\\sum_{i=1}^n \\left(a_i - b_i \\right)^2}\\] 8.6.3 Manhattan Manhattan distance is sometimes called taxicab distance. If you picture two locations in a city, it is the distance a taxicab must travel to get from one location to the other. Manhattan distance is calculated as: \\[d(\\pmb{a}, \\pmb{b}) = \\sum_{i=1}^n \\left| a_i - b_i \\right|\\] 8.6.4 Euclidean vs Manhattan Green is Euclidean. All others are Manhattan (and equal). Figure from Exploratory Data Analysis with R. 8.6.5 dist() A distance matrix – which is the set of values resulting from a distance measure applied to all pairs of objects – can be obtained through the function dist(). Default arguments for dist(): &gt; str(dist) function (x, method = &quot;euclidean&quot;, diag = FALSE, upper = FALSE, p = 2) The key argument for us is method= which can take values method=&quot;euclidean&quot; and method=&quot;manhattan&quot; among others. See ?dist. 8.6.6 Distance Matrix data1 &gt; sub_data1 &lt;- data1[1:4, c(1,2)] &gt; sub_data1 x y 1 2.085818 2.248086 2 1.896636 1.369547 3 2.097729 2.386383 4 1.491026 2.029814 &gt; mydist &lt;- dist(sub_data1) &gt; print(mydist) 1 2 3 2 0.8986772 3 0.1388086 1.0365293 4 0.6335776 0.7749019 0.7037257 &gt; (sub_data1[1,] - sub_data1[2,])^2 %&gt;% sum() %&gt;% sqrt() [1] 0.8986772 8.7 Hierarchical Clustering 8.7.1 Strategy Hierarchical clustering is a hierarchical agglomerative, bottom-up clustering method that strategically joins objects into larger and larger clusters, until all objects are contained in a single cluster. Hierarchical clustering results are typically displayed as a dendrogram. The number of clusters does not necessarily need to be known or chosen by the analyst. 8.7.2 Example: Cancer Subtypes Figure from Alizadeh et al. (2000) Nature. 8.7.3 Algorithm The algorithm for hierarchical clustering works as follows. Start with each object assigned as its own cluster. Calculate a distance between all pairs of clusters. Join the two clusters with the smallest distance. Repeat steps 2–3 until there is only one cluster. At the very first iteration of the algorithm, all we need is some distance function (e.g., Euclidean or Manhattan) to determine the two objects that are closest. But once clusters with more than one object are present, how do we calculate the distance between two clusters? This is where a key choice called the linkage method or criterion is needed. 8.7.4 Linkage Criteria Suppose there are two clusters \\(A\\) and \\(B\\) and we have a distance function \\(d(\\pmb{a}, \\pmb{b})\\) for all objects \\(\\pmb{a} \\in A\\) and \\(\\pmb{b} \\in B\\). Here are three ways (among many) to calculate a distance between clusters \\(A\\) and \\(B\\): \\[\\begin{eqnarray} \\mbox{Complete: } &amp; \\max \\{d(\\pmb{a}, \\pmb{b}): \\pmb{a} \\in A, \\pmb{b} \\in B\\} \\\\ \\mbox{Single: } &amp; \\min \\{d(\\pmb{a}, \\pmb{b}): \\pmb{a} \\in A, \\pmb{b} \\in B\\} \\\\ \\mbox{Average: } &amp; \\frac{1}{|A| |B|} \\sum_{\\pmb{a} \\in A} \\sum_{\\pmb{b} \\in B} d(\\pmb{a}, \\pmb{b}) \\end{eqnarray}\\] 8.7.5 hclust() The hclust() function produces an R object that contains all of the information needed to create a complete hierarchical clustering. Default arguments for hclust(): &gt; str(hclust) function (d, method = &quot;complete&quot;, members = NULL) The primary input for hclust() is the d argument, which is a distance matrix (usually obtained from dist()). The method argument takes the linkage method, which includes method=&quot;complete&quot;, method=&quot;single&quot;, method=&quot;average&quot;, etc. See ?hclust. 8.7.6 Hierarchical Clustering of data1 8.7.7 Standard hclust() Usage &gt; mydist &lt;- dist(data1, method = &quot;euclidean&quot;) &gt; myhclust &lt;- hclust(mydist, method=&quot;complete&quot;) &gt; plot(myhclust) 8.7.8 as.dendrogram() &gt; plot(as.dendrogram(myhclust)) 8.7.9 Modify the Labels &gt; library(dendextend) &gt; dend1 &lt;- as.dendrogram(myhclust) &gt; labels(dend1) &lt;- data1$true_clusters &gt; labels_colors(dend1) &lt;- + c(&quot;red&quot;, &quot;blue&quot;, &quot;gray47&quot;)[as.numeric(data1$true_clusters)] &gt; plot(dend1, axes=FALSE, main=&quot; &quot;, xlab=&quot; &quot;) 8.7.10 Color the Branches &gt; dend2 &lt;- as.dendrogram(myhclust) &gt; labels(dend2) &lt;- rep(&quot; &quot;, nrow(data1)) &gt; dend2 &lt;- color_branches(dend2, k = 3, col=c(&quot;red&quot;, &quot;blue&quot;, &quot;gray47&quot;)) &gt; plot(dend2, axes=FALSE, main=&quot; &quot;, xlab=&quot; &quot;) 8.7.11 Cluster Assignments (\\(K = 3\\)) &gt; est_clusters &lt;- cutree(myhclust, k=3) &gt; est_clusters [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [71] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 [106] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [141] 3 3 3 3 3 3 3 3 3 3 &gt; est_clusters &lt;- factor(est_clusters) &gt; p &lt;- data1 %&gt;% + mutate(est_clusters=est_clusters) %&gt;% + ggplot() &gt; p + geom_point(aes(x=x, y=y, color=est_clusters)) 8.7.12 Cluster Assignments (\\(K = 3\\)) 8.7.13 Cluster Assignments (\\(K = 2\\)) &gt; (data1 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=2))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 8.7.14 Cluster Assignments (\\(K = 4\\)) &gt; (data1 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=4))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 8.7.15 Cluster Assignments (\\(K = 6\\)) &gt; (data1 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=6))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 8.7.16 Linkage: Complete (Default) &gt; data1 %&gt;% dist() %&gt;% hclust(method=&quot;complete&quot;) %&gt;% + as.dendrogram() %&gt;% plot(axes=FALSE) 8.7.17 Linkage: Average &gt; data1 %&gt;% dist() %&gt;% hclust(method=&quot;average&quot;) %&gt;% + as.dendrogram() %&gt;% plot(axes=FALSE) 8.7.18 Linkage: Single &gt; data1 %&gt;% dist() %&gt;% hclust(method=&quot;single&quot;) %&gt;% + as.dendrogram() %&gt;% plot(axes=FALSE) 8.7.19 Linkage: Ward &gt; data1 %&gt;% dist() %&gt;% hclust(method=&quot;ward.D&quot;) %&gt;% + as.dendrogram() %&gt;% plot(axes=FALSE) 8.7.20 Hierarchical Clustering of data2 8.7.21 as.dendrogram() &gt; mydist &lt;- dist(data2, method = &quot;euclidean&quot;) &gt; myhclust &lt;- hclust(mydist, method=&quot;complete&quot;) &gt; plot(as.dendrogram(myhclust)) 8.7.22 Modify the Labels &gt; library(dendextend) &gt; dend1 &lt;- as.dendrogram(myhclust) &gt; labels(dend1) &lt;- data2$true_clusters &gt; labels_colors(dend1) &lt;- + c(&quot;red&quot;, &quot;blue&quot;)[as.numeric(data2$true_clusters)] &gt; plot(dend1, axes=FALSE, main=&quot; &quot;, xlab=&quot; &quot;) 8.7.23 Color the Branches &gt; dend2 &lt;- as.dendrogram(myhclust) &gt; labels(dend2) &lt;- rep(&quot; &quot;, nrow(data2)) &gt; dend2 &lt;- color_branches(dend2, k = 2, col=c(&quot;red&quot;, &quot;blue&quot;)) &gt; plot(dend2, axes=FALSE, main=&quot; &quot;, xlab=&quot; &quot;) 8.7.24 Cluster Assignments (\\(K = 2\\)) &gt; (data2 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=2))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 8.7.25 Cluster Assignments (\\(K = 3\\)) &gt; (data2 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=3))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 8.7.26 Cluster Assignments (\\(K = 4\\)) &gt; (data2 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=4))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 8.7.27 Cluster Assignments (\\(K = 5\\)) &gt; (data2 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=6))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 8.8 K-Means Clustering 8.8.1 Strategy K-means clustering is a top-down, partitioning cluster analysis method that assigns each object to one of \\(K\\) clusters based on the distance between each object and the cluster centers, called centroids. This is an iterative algorithm with potential random initial values. The value of \\(K\\) is typically unknown and must be determined by the analyst. 8.8.2 Centroid A centroid is the coordinate-wise average of all objects in a cluster. Let \\(A\\) be a given cluster with objects \\(\\pmb{a} \\in A\\). Its centroid is: \\[\\overline{\\pmb{a}} = \\frac{1}{|A|} \\sum_{\\pmb{a} \\in A} \\pmb{a}\\] 8.8.3 Algorithm The number of clusters \\(K\\) must be chosen beforehand. Initialize \\(K\\) cluster centroids. Assign each object to a cluster by choosing the cluster with the smalllest distance (e.g., Euclidean) between the object and the cluster centroid. Calculate new centroids based on the cluster assignments from Step 2. Repeat Steps 2–3 until convergence. 8.8.4 Notes The initialization of the centroids is typically random, so often the algorithm is run several times with new, random initial centroids. Convergence is usually defined in terms of neglible changes in the centroids or no changes in the cluster assignments. 8.8.5 kmeans() K-means clustering can be accomplished through the following function: &gt; str(kmeans) function (x, centers, iter.max = 10L, nstart = 1L, algorithm = c(&quot;Hartigan-Wong&quot;, &quot;Lloyd&quot;, &quot;Forgy&quot;, &quot;MacQueen&quot;), trace = FALSE) x: the data to clusters, objects along rows centers: either the number of clusters \\(K\\) or a matrix giving initial centroids iter.max: the maximum number of iterations allowed nstart: how many random intial \\(K\\) centroids, where the best one is returned 8.8.6 fitted() The cluster centroids or assigments can be extracted through the function fitted(), which is applied to the output of kmeans(). The input of fitted() is the object returned by kmeans(). The key additional argument is called method. When method=&quot;centers&quot; it returns the centroids. When method=&quot;classes&quot; it returns the cluster assignments. 8.8.7 K-Means Clustering of data1 &gt; km1 &lt;- kmeans(x=data1[,-3], centers=3, iter.max=100, nstart=5) &gt; est_clusters &lt;- fitted(km1, method=&quot;classes&quot;) &gt; est_clusters [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [71] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 [106] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [141] 2 2 2 2 2 2 2 2 2 2 8.8.8 Centroids of data1 &gt; centroids1 &lt;- fitted(km1, method=&quot;centers&quot;) %&gt;% unique() &gt; centroids1 x y 1 1.943184 2.028062 3 2.042872 4.037987 2 4.015934 2.962279 &gt; est_clusters &lt;- fitted(km1, method=&quot;classes&quot;) &gt; data1 %&gt;% mutate(est_clusters = factor(est_clusters)) %&gt;% + group_by(est_clusters) %&gt;% summarize(mean(x), mean(y)) # A tibble: 3 x 3 est_clusters `mean(x)` `mean(y)` &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 1.94 2.03 2 2 4.02 2.96 3 3 2.04 4.04 8.8.9 Cluster Assignments (\\(K = 3\\)) &gt; est_clusters &lt;- factor(est_clusters) &gt; ggplot(data1) + geom_point(aes(x=x, y=y, color=est_clusters)) 8.8.10 Cluster Assignments (\\(K = 2\\)) 8.8.11 Cluster Assignments (\\(K = 6\\)) 8.8.12 K-Means Clustering of data2 &gt; km2 &lt;- kmeans(x=data2[,-3], centers=2, iter.max=100, nstart=5) &gt; est_clusters &lt;- fitted(km2, method=&quot;classes&quot;) &gt; est_clusters [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 [71] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [106] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 8.8.13 Cluster Assignments (\\(K = 2\\)) &gt; est_clusters &lt;- factor(est_clusters) &gt; ggplot(data2) + geom_point(aes(x=x, y=y, color=est_clusters)) 8.8.14 Cluster Assignments (\\(K = 3\\)) 8.8.15 Cluster Assignments (\\(K = 5\\)) "],
["principal-component-analysis.html", "9 Principal Component Analysis 9.1 Dimensionality Reduction 9.2 Goal of PCA 9.3 Defining the First PC 9.4 Calculating All PCs 9.5 Singular Value Decomposition 9.6 A Simple PCA Function 9.7 The Ubiquitous PCA Example 9.8 PC Biplots 9.9 PCA Examples", " 9 Principal Component Analysis 9.1 Dimensionality Reduction The goal of dimensionality reduction is to extract low dimensional representations of high dimensional data that are useful for visualization, exploration, inference, or prediction. The low dimensional representations should capture key sources of variation in the data. Some methods for dimensionality reduction include: Principal component analysis Singular value decomposition Latent variable modeling Vector quantization Self-organizing maps Multidimensional scaling We will focus on what is likely the most commonly applied dimensionality reduction tool, principal components analysis. 9.2 Goal of PCA For a given set of variables, principal component analysis (PCA) finds (constrained) weighted sums of the variables to produce variables (called principal components) that capture consectuive maximum levels of variation in the data. Specifically, the first principal component is the weighted sum of the variables that results in a component with the highest variation. This component is then “removed” from the data, and the second principal component is obtained on the resulting residuals. This process is repeated until there is no variation left in the data. 9.3 Defining the First PC Suppose we have \\(m\\) variables, each with \\(n\\) observations: \\[ \\begin{aligned} {\\boldsymbol{x}}_1 &amp; = (x_{11}, x_{12}, \\ldots, x_{1n}) \\\\ {\\boldsymbol{x}}_2 &amp; = (x_{21}, x_{22}, \\ldots, x_{2n}) \\\\ \\ &amp; \\vdots \\ \\\\ {\\boldsymbol{x}}_m &amp; = (x_{m1}, x_{m2}, \\ldots, x_{mn}) \\end{aligned} \\] We can organize these variables into an \\(m \\times n\\) matrix \\({\\boldsymbol{X}}\\) where row \\(i\\) is \\({\\boldsymbol{x}}_i\\). Consider all possible weighted sums of these variables \\[\\tilde{\\pmb{x}} = \\sum_{i=1}^{m} u_i \\pmb{x}_i\\] where we constrain \\(\\sum_{i=1}^{m} u_i^2 = 1\\). We wish to identify the vector \\(\\pmb{u} = \\{u_i\\}_{i=1}^{m}\\) under this constraint that maximizes the sample variance of \\(\\tilde{\\pmb{x}}\\). However, note that if we first mean center each variable, replacing \\(x_{ij}\\) with \\[x_{ij}^* = x_{ij} - \\frac{1}{n} \\sum_{k=1}^n x_{ik},\\] then the sample variance of \\(\\tilde{\\pmb{x}} = \\sum_{i=1}^{m} u_i \\pmb{x}_i\\) is equal to that of \\(\\tilde{\\pmb{x}}^* = \\sum_{i=1}^{m} u_i \\pmb{x}^*_i\\). PCA is a method concerned with decompositon variance and covariance, so we don’t wish to involve the mean of each indivdual variable. Therefore, unless the true population mean of each variable is known (in which case it would be subtracted from its respective variable), we will formulate PCA in terms of mean centered variables, \\(\\pmb{x}^*_i = (x^*_{i1}, x^*_{i2}, \\ldots, x^*_{in})\\), which we can collect into \\(m \\times n\\) matrix \\({\\boldsymbol{X}}^*\\). We therefore consider all possible weighted sums of variables: \\[\\tilde{\\pmb{x}}^* = \\sum_{i=1}^{m} u_i \\pmb{x}^*_i.\\] The first principal component of \\({\\boldsymbol{X}}^*\\) (and \\({\\boldsymbol{X}}\\)) is \\(\\tilde{\\pmb{x}}^*\\) with maximum sample variance \\[ s^2_{\\tilde{{\\boldsymbol{x}}}^*} = \\frac{\\sum_{j=1}^n \\tilde{x}^{*2}_j}{n-1} \\] The \\(\\pmb{u} = \\{u_i\\}_{i=1}^{m}\\) yielding this first principal component is called its loadings. Note that \\[ s^2_{\\tilde{{\\boldsymbol{x}}}} = \\frac{\\sum_{j=1}^n \\left(\\tilde{x}_j - \\frac{1}{n} \\sum_{k=1}^n \\tilde{x}_k \\right)^2}{n-1} = \\frac{\\sum_{j=1}^n \\tilde{x}^{*2}_j}{n-1} = s^2_{\\tilde{{\\boldsymbol{x}}}^*} \\ , \\] so the loadings can be found from either \\({\\boldsymbol{X}}\\) or \\({\\boldsymbol{X}}^*\\). However, it the technically correct first PC is \\(\\tilde{{\\boldsymbol{x}}}^*\\) rather than \\(\\tilde{{\\boldsymbol{x}}}\\). This first PC is then removed from the data, and the procedure is repeated until all possible sample PCs are constructed. This is accomplished by calculating the product of \\({\\boldsymbol{u}}_{m \\times 1}\\) and \\(\\tilde{\\pmb{x}}^*_{1 \\times n}\\), and subtracting it from \\({\\boldsymbol{X}}^*\\): \\[ {\\boldsymbol{X}}^* - {\\boldsymbol{u}}\\tilde{\\pmb{x}}^* \\ . \\] 9.4 Calculating All PCs All of the PCs can be calculated simultaneously. First, we construct the \\(m \\times m\\) sample covariance matrix \\({\\boldsymbol{S}}\\) with \\((i,j)\\) entry \\[ s_{ij} = \\frac{\\sum_{k=1}^n (x_{ik} - \\bar{x}_{i\\cdot})(x_{jk} - \\bar{x}_{j\\cdot})}{n-1}. \\] The sample covariance can also be calculated by \\[ {\\boldsymbol{S}}= \\frac{1}{n-1} {\\boldsymbol{X}}^{*} {\\boldsymbol{X}}^{*T}. \\] It can be shown that \\[ s^2_{\\tilde{{\\boldsymbol{x}}}^*} = {\\boldsymbol{u}}^T {\\boldsymbol{S}}{\\boldsymbol{u}}, \\] so identifying \\({\\boldsymbol{u}}\\) that maximizes \\(s^2_{\\tilde{{\\boldsymbol{x}}}}\\) also maximizes \\({\\boldsymbol{u}}^T {\\boldsymbol{S}}{\\boldsymbol{u}}\\). Using a Lagrange multiplier, we wish to maximize \\[ {\\boldsymbol{u}}^T {\\boldsymbol{S}}{\\boldsymbol{u}}+ \\lambda({\\boldsymbol{u}}^T {\\boldsymbol{u}}- 1). \\] Differentiating with respect to \\({\\boldsymbol{u}}\\) and setting this to \\({\\boldsymbol{0}}\\), we get \\({\\boldsymbol{S}}{\\boldsymbol{u}}- \\lambda {\\boldsymbol{u}}= 0\\) or \\[ {\\boldsymbol{S}}{\\boldsymbol{u}}= \\lambda {\\boldsymbol{u}}. \\] For any such \\({\\boldsymbol{u}}\\) and \\(\\lambda\\) where this holds, note that \\[ s_{ij} = \\frac{\\sum_{k=1}^n (x_{ik} - \\bar{x}_{i\\cdot})(x_{jk} - \\bar{x}_{j\\cdot})}{n-1} = {\\boldsymbol{u}}^T {\\boldsymbol{S}}{\\boldsymbol{u}}= \\lambda \\] so the PC’s variance is \\(\\lambda\\). The eigendecompositon of a matrix identifies all such solutions to \\({\\boldsymbol{S}}{\\boldsymbol{u}}= \\lambda {\\boldsymbol{u}}.\\) Specifically, it calculates the decompositon \\[ {\\boldsymbol{S}}= {\\boldsymbol{U}}{\\boldsymbol{\\Lambda}}{\\boldsymbol{U}}^T \\] where \\({\\boldsymbol{U}}\\) is an \\(m \\times m\\) orthogonal matrix and \\({\\boldsymbol{\\Lambda}}\\) is a diagonal matrix with entries \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_m \\geq 0\\). The fact that \\({\\boldsymbol{U}}\\) is orthogonal means \\({\\boldsymbol{U}}{\\boldsymbol{U}}^T = {\\boldsymbol{U}}^T {\\boldsymbol{U}}= {\\boldsymbol{I}}\\). The following therefore hold: For each column \\(j\\) of \\({\\boldsymbol{U}}\\), say \\({\\boldsymbol{u}}_j\\), it follows that \\({\\boldsymbol{S}}{\\boldsymbol{u}}_j = \\lambda_j {\\boldsymbol{u}}_j\\) \\(\\| {\\boldsymbol{u}}_j \\|^2_2 = 1\\) and \\({\\boldsymbol{u}}_j^T {\\boldsymbol{u}}_k = {\\boldsymbol{0}}\\) for \\(\\lambda_j \\not= \\lambda_k\\) \\({\\operatorname{Var}}({\\boldsymbol{u}}_j^T {\\boldsymbol{X}}) = \\lambda_j\\) \\({\\operatorname{Var}}({\\boldsymbol{u}}_1^T {\\boldsymbol{X}}) \\geq {\\operatorname{Var}}({\\boldsymbol{u}}_2^T {\\boldsymbol{X}}) \\geq \\cdots \\geq {\\operatorname{Var}}({\\boldsymbol{u}}_m^T {\\boldsymbol{X}})\\) \\({\\boldsymbol{S}}= \\sum_{j=1}^m \\lambda_j {\\boldsymbol{u}}_j {\\boldsymbol{u}}_j^T\\) For \\(\\lambda_j \\not= \\lambda_k\\), \\[{\\operatorname{Cov}}({\\boldsymbol{u}}_j^T {\\boldsymbol{X}}, {\\boldsymbol{u}}_k^T {\\boldsymbol{X}}) = {\\boldsymbol{u}}_j^T {\\boldsymbol{S}}{\\boldsymbol{u}}_k = \\lambda_k {\\boldsymbol{u}}_j^T {\\boldsymbol{u}}_k = {\\boldsymbol{0}}\\] To calculate the actual principal components, let \\(x^*_{ij} = x_{ij} - \\bar{x}_{i\\cdot}\\) be the mean-centered variables. Let \\({\\boldsymbol{X}}^*\\) be the matrix composed of these mean-centered variables. Also, let \\({\\boldsymbol{u}}_j\\) be column \\(j\\) of \\({\\boldsymbol{U}}\\) from \\({\\boldsymbol{S}}= {\\boldsymbol{U}}{\\boldsymbol{\\Lambda}}{\\boldsymbol{U}}^T\\). Sample principal component \\(j\\) is then \\[ \\tilde{{\\boldsymbol{x}}}_j = {\\boldsymbol{u}}_j^T {\\boldsymbol{X}}^* = \\sum_{i=1}^m u_{ij} {\\boldsymbol{x}}^*_i \\] for \\(j = 1, 2, \\ldots, \\min(m, n-1)\\). For \\(j &gt; \\min(m, n-1)\\), we have \\(\\lambda_j = 0\\), so these principal components are not necessary to calculate. The loadings corresponding to PC \\(j\\) are \\({\\boldsymbol{u}}_j\\). Note that the convention is that mean of PC \\(j\\) is zero, i.e., that \\[ \\frac{1}{n} \\sum_{k=1}^n \\tilde{x}_{jk} = 0, \\] but as mentioned above \\(\\sum_{i=1}^m u_{ij} {\\boldsymbol{x}}^*_i\\) and the uncentered \\(\\sum_{i=1}^m u_{ij} {\\boldsymbol{x}}_i\\) have the same sample variance. It can be calculated that the variance of PC \\(j\\) is \\[ s^2_{\\tilde{{\\boldsymbol{x}}}_j} = \\frac{\\sum_{k=1}^n \\tilde{x}_{jk}^2}{n-1} = \\lambda_j. \\] The proportion of variance explained by PC \\(j\\) is \\[ \\operatorname{PVE}_j = \\frac{\\lambda_j}{\\sum_{k=1}^m \\lambda_k}. \\] 9.5 Singular Value Decomposition One way in which PCA is performed is to carry out a singular value decomposition (SVD) of the data matrix \\({\\boldsymbol{X}}\\). Let \\(q = \\min(m, n)\\). Recalling that \\({\\boldsymbol{X}}^*\\) is the row-wise mean centered \\({\\boldsymbol{X}}\\), we can take the SVD of \\({\\boldsymbol{X}}^*/\\sqrt{n-1}\\) to obtain \\[ \\frac{1}{\\sqrt{n-1}} {\\boldsymbol{X}}^* = {\\boldsymbol{U}}{\\boldsymbol{D}}{\\boldsymbol{V}}^T \\] where \\({\\boldsymbol{U}}_{m \\times q}\\), \\({\\boldsymbol{V}}_{n \\times q}\\), and diagonal \\({\\boldsymbol{D}}_{q \\times q}\\). Also, we have the orthogonality properties \\({\\boldsymbol{V}}^T {\\boldsymbol{V}}= {\\boldsymbol{U}}^T {\\boldsymbol{U}}= {\\boldsymbol{I}}_{q}\\). Finally, \\({\\boldsymbol{D}}\\) is composed of diagonal elements \\(d_1 \\geq d_2 \\geq \\cdots \\geq d_q \\geq 0\\) where \\(d_q = 0\\) if \\(q = n\\). Note that \\[ {\\boldsymbol{S}}= \\frac{1}{n-1} {\\boldsymbol{X}}^{*} {\\boldsymbol{X}}^{*T} = {\\boldsymbol{U}}{\\boldsymbol{D}}{\\boldsymbol{V}}^T \\left({\\boldsymbol{U}}{\\boldsymbol{D}}{\\boldsymbol{V}}^T\\right)^T = {\\boldsymbol{U}}{\\boldsymbol{D}}^2 {\\boldsymbol{U}}^T. \\] Therefore: The variance of PC \\(j\\) is \\(\\lambda_j = d_j^2\\) The loadings of PC \\(j\\) are contained in the columns of the left-hand matrix from the decomposition of \\({\\boldsymbol{S}}\\) or \\({\\boldsymbol{X}}^*\\) PC \\(j\\) is row \\(j\\) of \\({\\boldsymbol{D}}{\\boldsymbol{V}}^T\\) 9.6 A Simple PCA Function &gt; pca &lt;- function(x, space=c(&quot;rows&quot;, &quot;columns&quot;), + center=TRUE, scale=FALSE) { + space &lt;- match.arg(space) + if(space==&quot;columns&quot;) {x &lt;- t(x)} + x &lt;- t(scale(t(x), center=center, scale=scale)) + x &lt;- x/sqrt(nrow(x)-1) + s &lt;- svd(x) + loading &lt;- s$u + colnames(loading) &lt;- paste0(&quot;Loading&quot;, 1:ncol(loading)) + rownames(loading) &lt;- rownames(x) + pc &lt;- diag(s$d) %*% t(s$v) + rownames(pc) &lt;- paste0(&quot;PC&quot;, 1:nrow(pc)) + colnames(pc) &lt;- colnames(x) + pve &lt;- s$d^2 / sum(s$d^2) + if(space==&quot;columns&quot;) {pc &lt;- t(pc); loading &lt;- t(loading)} + return(list(pc=pc, loading=loading, pve=pve)) + } The input is as follows: x: a matrix of numerical values space: either &quot;rows&quot; or &quot;columns&quot;, denoting which dimension contains the variables center: if TRUE then the variables are mean centered before calculating PCs scale: if TRUE then the variables are std dev scaled before calculating PCs The output is a list with the following items: pc: a matrix of all possible PCs loading: the weights or “loadings” that determined each PC pve: the proportion of variation explained by each PC Note that the rows or columns of pc and loading have names to let you know on which dimension the values are organized. 9.7 The Ubiquitous PCA Example Here’s an example very frequently encountered to explain PCA, but it’s slightly complicated and conflates several ideas in PCA. I think it’s not a great example to motivate PCA, but it’s so common I want to carefully clarify what it’s displaying. &gt; set.seed(508) &gt; n &lt;- 70 &gt; z &lt;- sqrt(0.8) * rnorm(n) &gt; x1 &lt;- z + sqrt(0.2) * rnorm(n) &gt; x2 &lt;- z + sqrt(0.2) * rnorm(n) &gt; X &lt;- rbind(x1, x2) &gt; p &lt;- pca(x=X, space=&quot;rows&quot;) PCS is often explained by showing the following plot and stating, “The first PC finds the direction of maximal variance in the data…” The above figure was made with the following code: &gt; a1 &lt;- p$loading[1,1] * p$pc[1,] + mean(x1) &gt; a2 &lt;- p$loading[1,2] * p$pc[1,] + mean(x2) &gt; df &lt;- data.frame(x1=c(x1, a1), + x2=c(x2, a2), + legend=c(rep(&quot;data&quot;,n),rep(&quot;pc1_projection&quot;,n))) &gt; ggplot(df) + geom_point(aes(x=x1,y=x2,color=legend)) + + scale_color_manual(values=c(&quot;blue&quot;, &quot;red&quot;)) The red dots are therefore the projection of x1 and x2 onto the first PC, so they are neither the loadings nor the PC. This is rather complicated to understand before loadings and PCs are full understood. Note that there are several ways to calculate these projections. # all equivalent ways to get a1 p$loading[1,1] * p$pc[1,] outer(p$loading[,1], p$pc[1,])[1,] + mean(x1) lm(x1 ~ p$pc[1,])$fit # and # all equivalent ways to get a2 p$loading[2,2] * p$pc[2,] outer(p$loading[,1], p$pc[1,])[2,] + mean(x2) lm(x2 ~ p$pc[1,])$fit We haven’t seen the lm() function yet, but once we do this example will be useful to revisit to understand what is meant by “projection”. Here is PC1 vs PC2: &gt; data.frame(pc1=p$pc[1,], pc2=p$pc[2,]) %&gt;% + ggplot() + geom_point(aes(x=pc1,y=pc2)) + + theme(aspect.ratio=1) Here is PC1 vs x1: &gt; data.frame(pc1=p$pc[1,], x1=x1) %&gt;% + ggplot() + geom_point(aes(x=pc1,y=x1)) + + theme(aspect.ratio=1) Here is PC1 vs x2: &gt; data.frame(pc1=p$pc[1,], x2=x2) %&gt;% + ggplot() + geom_point(aes(x=pc1,y=x2)) + + theme(aspect.ratio=1) Here is PC1 vs z: &gt; data.frame(pc1=p$pc[1,], z=z) %&gt;% + ggplot() + geom_point(aes(x=pc1,y=z)) + + theme(aspect.ratio=1) 9.8 PC Biplots Sometimes it is informative to plot a PC versus another PC. This is called a PC biplot. It is possible that interesting subgroups or clusters of observations will emerge. 9.9 PCA Examples 9.9.1 Weather Data These daily temperature data (in tenths of degrees C) come from meteorogical observations for weather stations in the US for the year 2012 provided by NOAA (National Oceanic and Atmospheric Administration).: &gt; load(&quot;./data/weather_data.RData&quot;) &gt; dim(weather_data) [1] 2811 50 &gt; &gt; weather_data[1:5, 1:7] 11 16 18 19 27 30 31 AG000060611 138.0000 175.0000 173 164.0000 218 160 163.0000 AGM00060369 158.0000 162.0000 154 159.0000 165 125 171.0000 AGM00060425 272.7619 272.7619 152 163.0000 163 108 158.0000 AGM00060444 128.0000 102.0000 100 111.0000 125 33 125.0000 AGM00060468 105.0000 122.0000 97 263.5714 155 52 263.5714 This matrix contains temperature data on 50 days and 2811 stations that were randomly selected. First, we will convert temperatures to Fahrenheit: &gt; weather_data &lt;- 0.18*weather_data + 32 &gt; weather_data[1:5, 1:6] 11 16 18 19 27 30 AG000060611 56.84000 63.50000 63.14 61.52000 71.24 60.80 AGM00060369 60.44000 61.16000 59.72 60.62000 61.70 54.50 AGM00060425 81.09714 81.09714 59.36 61.34000 61.34 51.44 AGM00060444 55.04000 50.36000 50.00 51.98000 54.50 37.94 AGM00060468 50.90000 53.96000 49.46 79.44286 59.90 41.36 &gt; &gt; apply(weather_data, 1, median) %&gt;% + quantile(probs=seq(0,1,0.1)) 0% 10% 20% 30% 40% 50% 8.886744 49.010000 54.500000 58.460000 62.150000 65.930000 60% 70% 80% 90% 100% 69.679318 73.490000 77.990000 82.940000 140.000000 Let’s perform PCA on these data. &gt; mypca &lt;- pca(weather_data, space=&quot;rows&quot;) &gt; &gt; names(mypca) [1] &quot;pc&quot; &quot;loading&quot; &quot;pve&quot; &gt; dim(mypca$pc) [1] 50 50 &gt; dim(mypca$loading) [1] 2811 50 &gt; mypca$pc[1:3, 1:3] 11 16 18 PC1 19.5166741 25.441401 25.9023874 PC2 -2.6025225 -4.310673 0.9707207 PC3 -0.6681223 -1.240748 -3.7276658 &gt; mypca$loading[1:3, 1:3] Loading1 Loading2 Loading3 AG000060611 -0.015172744 0.013033849 -0.011273121 AGM00060369 -0.009439176 0.016884418 -0.004611284 AGM00060425 -0.015779138 0.007026312 -0.009907972 PC1 vs Time: &gt; day_of_the_year &lt;- as.numeric(colnames(weather_data)) &gt; data.frame(day=day_of_the_year, PC1=mypca$pc[1,]) %&gt;% + ggplot() + geom_point(aes(x=day, y=PC1), size=2) PC2 vs Time: &gt; data.frame(day=day_of_the_year, PC2=mypca$pc[2,]) %&gt;% + ggplot() + geom_point(aes(x=day, y=PC2), size=2) PC1 vs PC2 Biplot: This does not appear to be subgroups or clusters in the weather data set biplot of PC1 vs PC2. &gt; data.frame(PC1=mypca$pc[1,], PC2=mypca$pc[2,]) %&gt;% + ggplot() + geom_point(aes(x=PC1, y=PC2), size=2) Proportion of Variance Explained: &gt; data.frame(Component=1:length(mypca$pve), PVE=mypca$pve) %&gt;% + ggplot() + geom_point(aes(x=Component, y=PVE), size=2) We can multiple the loadings matrix by the PCs matrix to reproduce the data: &gt; # mean centered weather data &gt; weather_data_mc &lt;- weather_data - rowMeans(weather_data) &gt; &gt; # difference between the PC projections and the data &gt; # the small sum is just machine imprecision &gt; sum(abs(weather_data_mc/sqrt(nrow(weather_data_mc)-1) - + mypca$loading %*% mypca$pc)) [1] 1.329755e-10 The sum of squared weights – i.e., loadings – equals one for each component: &gt; sum(mypca$loading[,1]^2) [1] 1 &gt; &gt; apply(mypca$loading, 2, function(x) {sum(x^2)}) Loading1 Loading2 Loading3 Loading4 Loading5 Loading6 Loading7 1 1 1 1 1 1 1 Loading8 Loading9 Loading10 Loading11 Loading12 Loading13 Loading14 1 1 1 1 1 1 1 Loading15 Loading16 Loading17 Loading18 Loading19 Loading20 Loading21 1 1 1 1 1 1 1 Loading22 Loading23 Loading24 Loading25 Loading26 Loading27 Loading28 1 1 1 1 1 1 1 Loading29 Loading30 Loading31 Loading32 Loading33 Loading34 Loading35 1 1 1 1 1 1 1 Loading36 Loading37 Loading38 Loading39 Loading40 Loading41 Loading42 1 1 1 1 1 1 1 Loading43 Loading44 Loading45 Loading46 Loading47 Loading48 Loading49 1 1 1 1 1 1 1 Loading50 1 PCs by contruction have sample correlation equal to zero: &gt; cor(mypca$pc[1,], mypca$pc[2,]) [1] 3.135149e-17 &gt; cor(mypca$pc[1,], mypca$pc[3,]) [1] 2.273613e-16 &gt; cor(mypca$pc[1,], mypca$pc[12,]) [1] -1.231339e-16 &gt; cor(mypca$pc[5,], mypca$pc[27,]) [1] -2.099516e-17 &gt; # etc... I can transform the top PC back to the original units to display it at a scale that has a more direct interpretation. &gt; day_of_the_year &lt;- as.numeric(colnames(weather_data)) &gt; y &lt;- -mypca$pc[1,] + mean(weather_data) &gt; data.frame(day=day_of_the_year, max_temp=y) %&gt;% + ggplot() + geom_point(aes(x=day, y=max_temp)) 9.9.2 Yeast Gene Expression Yeast cells were synchronized so that they were on the same approximate cell cycle timing in Spellman et al. (1998). The goal was to understand how gene expression varies over the cell cycle from a genome-wide perspective. &gt; load(&quot;./data/spellman.RData&quot;) &gt; time [1] 0 30 60 90 120 150 180 210 240 270 330 360 390 &gt; dim(gene_expression) [1] 5981 13 &gt; gene_expression[1:6,1:5] 0 30 60 90 120 YAL001C 0.69542786 -0.4143538 3.2350520 1.6323737 -2.1091820 YAL002W -0.01210662 3.0465649 1.1062193 4.0591467 -0.1166399 YAL003W -2.78570526 -1.0156981 -2.1387564 1.9299681 0.7797033 YAL004W 0.55165887 0.6590093 0.5857847 0.3890409 -1.0009777 YAL005C -0.53191556 0.1577985 -1.2401448 0.8170350 -1.3520947 YAL007C -0.86693416 -1.1642322 -0.6359588 1.1179131 1.9587021 Proportion Variance Explained: &gt; p &lt;- pca(gene_expression, space=&quot;rows&quot;) &gt; ggplot(data.frame(pc=1:13, pve=p$pve)) + + geom_point(aes(x=pc,y=pve), size=2) PCs vs Time (with Smoothers): 9.9.3 HapMap Genotypes I curated a small data set that cleanly separates human subpopulations from the HapMap data. These include unrelated individuals from Yoruba people from Ibadan, Nigeria (YRI), Utah residents of northern and western European ancestry (CEU), Japanese individuals from Tokyo, Japan (JPT), and Han Chinese individuals from Beijing, China (CHB). &gt; hapmap &lt;- read.table(&quot;./data/hapmap_sample.txt&quot;) &gt; dim(hapmap) [1] 400 24 &gt; hapmap[1:6,1:6] NA18516 NA19138 NA19137 NA19223 NA19200 NA19131 rs2051075 0 1 2 1 1 1 rs765546 2 2 0 0 0 0 rs10019399 2 2 2 1 1 2 rs7055827 2 2 1 2 0 2 rs6943479 0 0 2 0 1 0 rs2095381 1 2 1 2 1 1 Proportion Variance Explained: &gt; p &lt;- pca(hapmap, space=&quot;rows&quot;) &gt; ggplot(data.frame(pc=(1:ncol(hapmap)), pve=p$pve)) + + geom_point(aes(x=pc,y=pve), size=2) PC1 vs PC2 Biplot: PC1 vs PC3 Biplot: PC2 vs PC3 Biplot: "],
["probability-and-statistics.html", "10 Probability and Statistics 10.1 Central Dogma of Inference 10.2 Data Analysis Without Probability", " 10 Probability and Statistics Probabilistic modeling and/or statistical inference are required when the goals include: Characterizing randomness or “noise” in the data Quantifying uncertainty in models we build or decisions we make from the data Predicting future observations or decisions in the face of uncertainty 10.1 Central Dogma of Inference Figure 10.1: Central Dogma of Statistical Inference 10.2 Data Analysis Without Probability It is possible to do data analysis without probability and formal statistical inference: Descriptive statistics can be reported without utilizing probability and statistical inference Exploratory data analysis and visualization tend to not involve probability or formal statistical inference Important problems in machine learning do not involve probability or statistical inference. "],
["probability-theory.html", "11 Probability Theory 11.1 Sample Space 11.2 Measure Theoretic Probabilty 11.3 Mathematical Probability 11.4 Union of Two Events 11.5 Conditional Probability 11.6 Independence 11.7 Bayes Theorem 11.8 Law of Total Probability", " 11 Probability Theory 11.1 Sample Space The sample space \\(\\Omega\\) is the set of all outcomes We are interested in calculating probabilities on relevant subsets of this space, called events: \\(A \\subseteq \\Omega\\) Examples — Two coin flips: \\(\\Omega =\\) {HH, HT, TH, TT} SNP genotypes: \\(\\Omega =\\) {AA, AT, TT} Amazon product rating: \\(\\Omega =\\) {1 star, 2 stars, …, 5 stars} Political survey: \\(\\Omega =\\) {agree, disagree} 11.2 Measure Theoretic Probabilty \\[(\\Omega, \\mathcal{F}, \\Pr)\\] \\(\\Omega\\) is the sample space \\(\\mathcal{F}\\) is the \\(\\sigma\\)-algebra of events where probability can be measured \\(\\Pr\\) is the probability measure 11.3 Mathematical Probability A proper mathematical formulation of a probability measure should include the following properties: The probability of any even \\(A\\) is such that \\(0 \\leq \\Pr(A) \\leq 1\\) If \\(\\Omega\\) is the sample space then \\(\\Pr(\\Omega)=1\\) Let \\(A^c\\) be all outcomes from \\(\\Omega\\) that are not in \\(A\\) (called the complement); then \\(\\Pr(A) + \\Pr(A^c) = 1\\) For any \\(n\\) events such that \\(A_i \\cap A_j = \\varnothing\\) for all \\(i \\not= j\\), then \\(\\Pr\\left( \\cup_{i=1}^n A_i \\right) = \\sum_{i=1}^n \\Pr(A_i)\\), where \\(\\varnothing\\) is the empty set 11.4 Union of Two Events The probability of two events are calculated by the following general relationship: \\[\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\] where we note that \\(\\Pr(A \\cap B)\\) gets counted twice in \\(\\Pr(A) + \\Pr(B)\\). 11.5 Conditional Probability An important calclation in probability and statistics is the conditional probability. We can consider the probability of an event \\(A\\), conditional on the fact that we are restricted to be within event \\(B\\). This is defined as: \\[\\Pr(A | B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}\\] 11.6 Independence Two events \\(A\\) and \\(B\\) by definition independent when: \\(\\Pr(A | B) = \\Pr(A)\\) \\(\\Pr(B | A) = \\Pr(B)\\) \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\) All three of these are equivalent. 11.7 Bayes Theorem A common approach in statistics is to obtain a conditional probability of two events through the opposite conditional probability and their marginal probability. This is called Bayes Theorem: \\[\\Pr(B | A) = \\frac{\\Pr(A | B)\\Pr(B)}{\\Pr(A)}\\] This forms the basis of Bayesian Inference but has more general use in carrying out probability calculations. 11.8 Law of Total Probability For events \\(A_1, \\ldots, A_n\\) such that \\(A_i \\cap A_j = \\varnothing\\) for all \\(i \\not= j\\) and \\(\\cup_{i=1}^n A_i = \\Omega\\), it follows that for any event \\(B\\): \\[\\Pr(B) = \\sum_{i=1}^n \\Pr(B | A_i) \\Pr(A_i).\\] "],
["random-variables.html", "12 Random Variables 12.1 Definition 12.2 Distributon of RV 12.3 Discrete Random Variables 12.4 Example: Discrete PMF 12.5 Example: Discrete CDF 12.6 Probabilities of Events Via Discrete CDF 12.7 Continuous Random Variables 12.8 Example: Continuous PDF 12.9 Example: Continuous CDF 12.10 Probabilities of Events Via Continuous CDF 12.11 Example: Continuous RV Event 12.12 Note on PMFs and PDFs 12.13 Note on CDFs 12.14 Sample Vs Population Statistics 12.15 Expected Value 12.16 Variance 12.17 Moment Generating Functions 12.18 Random Variables in R", " 12 Random Variables 12.1 Definition A random variable \\(X\\) is a function from \\(\\Omega\\) to the real numbers: \\[X: \\Omega \\rightarrow \\mathbb{R}\\] For any outcome in \\(\\Omega\\), the function \\(X(\\omega)\\) produces a real value. We will write the range of \\(X\\) as \\[\\mathcal{R} = \\{X(\\omega): \\omega \\in \\Omega\\}\\] where \\(\\mathcal{R} \\subseteq \\mathbb{R}\\). 12.2 Distributon of RV We define the probability distribution of a random variable through its probability mass function (pmf) for discrete rv’s or its probability density function (pdf) for continuous rv’s. We can also define the distribution through its cumulative distribution function (cdf). The pmf/pdf determines the cdf, and vice versa. 12.3 Discrete Random Variables A discrete rv \\(X\\) takes on a discrete set of values such as \\(\\{1, 2, \\ldots, n\\}\\) or \\(\\{0, 1, 2, 3, \\ldots \\}\\). Its distribution is characterized by its pmf \\[f(x) = \\Pr(X = x)\\] for \\(x \\in \\{X(\\omega): \\omega \\in \\Omega \\}\\) and \\(f(x) = 0\\) otherwise. Its cdf is \\[F(y) = \\Pr(X \\leq y) = \\sum_{x \\leq y} \\Pr(X = x)\\] for \\(y \\in \\mathbb{R}\\). 12.4 Example: Discrete PMF 12.5 Example: Discrete CDF 12.6 Probabilities of Events Via Discrete CDF Examples: Probability CDF PMF \\(\\Pr(X \\leq b)\\) \\(F(b)\\) \\(\\sum_{x \\leq b} f(x)\\) \\(\\Pr(X \\geq a)\\) \\(1-F(a-1)\\) \\(\\sum_{x \\geq a} f(x)\\) \\(\\Pr(X &gt; a)\\) \\(1-F(a)\\) \\(\\sum_{x &gt; a} f(x)\\) \\(\\Pr(a \\leq X \\leq b)\\) \\(F(b) - F(a-1)\\) \\(\\sum_{a \\leq x \\leq b} f(x)\\) \\(\\Pr(a &lt; X \\leq b)\\) \\(F(b) - F(a)\\) \\(\\sum_{a &lt; x \\leq b} f(x)\\) 12.7 Continuous Random Variables A continuous rv \\(X\\) takes on a continuous set of values such as \\([0, \\infty)\\) or \\(\\mathbb{R} = (-\\infty, \\infty)\\). The probability that \\(X\\) takes on any specific value is 0; but the probability it lies within an interval can be non-zero. Its pdf \\(f(x)\\) therefore gives an infinitesimal, local, relative probability. Its cdf is \\[F(y) = \\Pr(X \\leq y) = \\int_{-\\infty}^y f(x) dx\\] for \\(y \\in \\mathbb{R}\\). 12.8 Example: Continuous PDF 12.9 Example: Continuous CDF 12.10 Probabilities of Events Via Continuous CDF Examples: Probability CDF PDF \\(\\Pr(X \\leq b)\\) \\(F(b)\\) \\(\\int_{-\\infty}^{b} f(x) dx\\) \\(\\Pr(X \\geq a)\\) \\(1-F(a)\\) \\(\\int_{a}^{\\infty} f(x) dx\\) \\(\\Pr(X &gt; a)\\) \\(1-F(a)\\) \\(\\int_{a}^{\\infty} f(x) dx\\) \\(\\Pr(a \\leq X \\leq b)\\) \\(F(b) - F(a)\\) \\(\\int_{a}^{b} f(x) dx\\) \\(\\Pr(a &lt; X \\leq b)\\) \\(F(b) - F(a)\\) \\(\\int_{a}^{b} f(x) dx\\) 12.11 Example: Continuous RV Event 12.12 Note on PMFs and PDFs PMFs and PDFs are defined as \\(f(x)=0\\) outside of the range of \\(X\\), \\(\\mathcal{R} = \\{X(\\omega): \\omega \\in \\Omega\\}\\). That is: Also, they sum or integrate to 1: \\[\\sum_{x \\in \\mathcal{R}} f(x) = 1\\] \\[\\int_{x \\in \\mathcal{R}} f(x) dx = 1\\] Using measure theory, we can consider both types of rv’s in one framework, and we would write: \\[\\int_{-\\infty}^{\\infty} dF(x) = 1\\] 12.13 Note on CDFs Properties of all cdf’s, regardless of continuous or discrete underlying rv: They are right continuous with left limits \\(\\lim_{x \\rightarrow \\infty} F(x) = 1\\) \\(\\lim_{x \\rightarrow -\\infty} F(x) = 0\\) The right derivative of \\(F(x)\\) equals \\(f(x)\\) 12.14 Sample Vs Population Statistics We earlier discussed measures of center and spread for a set of data, such as the mean and the variance. Analogous measures exist for probability distributions. These are distinguished by calling those on data “sample” measures (e.g., sample mean) and those on probability distributions “population” measures (e.g., population mean). 12.15 Expected Value The expected value, also called the “population mean”, is a measure of center for a rv. It is calculated in a fashion analogous to the sample mean: \\[\\begin{align*} &amp; \\operatorname{E}[X] = \\sum_{x \\in \\mathcal{R}} x \\ f(x) &amp; \\mbox{(discrete)} \\\\ &amp; \\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x \\ f(x) \\ dx &amp; \\mbox{(continuous)} \\\\ &amp; \\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x \\ dF(x) &amp; \\mbox{(general)} \\end{align*}\\] 12.16 Variance The variance, also called the “population variance”, is a measure of spread for a rv. It is calculated in a fashion analogous to the sample variance: \\[{\\operatorname{Var}}(X) = {\\operatorname{E}}\\left[\\left(X-{\\operatorname{E}}[X]\\right)^2\\right] = {\\operatorname{E}}[X^2] - E[X]^2\\] \\[{\\rm SD}(X) = \\sqrt{{\\operatorname{Var}}(X)}\\] \\[{\\operatorname{Var}}(X) = \\sum_{x \\in \\mathcal{R}} \\left(x-{\\operatorname{E}}[X]\\right)^2 \\ f(x) \\ \\ \\ \\ \\mbox{(discrete)}\\] \\[{\\operatorname{Var}}(X) = \\int_{-\\infty}^{\\infty} \\left(x-{\\operatorname{E}}[X]\\right)^2 \\ f(x) \\ dx \\ \\ \\ \\ \\mbox{(continuous)}\\] 12.17 Moment Generating Functions The moment generating function (mgf) of a rv is defined to be \\[m(t) = \\operatorname{E}\\left[e^{tX}\\right]\\] whenever this expecation exists. Under certain conditions, the moments of a rv can then be obtained by: \\[\\operatorname{E} \\left[ X^k \\right] = \\frac{d^k}{dt^k}m(0).\\] 12.18 Random Variables in R The pmf/pdf, cdf, quantile function, and random number generator for many important random variables are built into R. They all follow the form, where &lt;name&gt; is replaced with the name used in R for each specific distribution: d&lt;name&gt;: pmf or pdf p&lt;name&gt;: cdf q&lt;name&gt;: quantile function or inverse cdf r&lt;name&gt;: random number generator To see a list of random variables, type ?Distributions in R. "],
["discrete-rvs.html", "13 Discrete RVs 13.1 Uniform (Discrete) 13.2 Uniform (Discrete) PMF 13.3 Uniform (Discrete) in R 13.4 Bernoulli 13.5 Binomial 13.6 Binomial PMF 13.7 Binomial in R 13.8 Poisson 13.9 Poisson PMF 13.10 Poisson in R", " 13 Discrete RVs 13.1 Uniform (Discrete) This simple rv distribution assigns equal probabilities to a finite set of values: \\[X \\sim \\mbox{Uniform}\\{1, 2, \\ldots, n\\}\\] \\[\\mathcal{R} = \\{1, 2, \\ldots, n\\}\\] \\[f(x; n) = 1/n \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = \\frac{n+1}{2}, \\ {\\operatorname{Var}}(X) = \\frac{n^2-1}{12}\\] 13.2 Uniform (Discrete) PMF 13.3 Uniform (Discrete) in R There is no family of functions built into R for this distribution since it is so simple. However, it is possible to generate random values via the sample function: &gt; n &lt;- 20L &gt; sample(x=1:n, size=10, replace=TRUE) [1] 9 15 6 8 13 19 19 1 20 14 &gt; &gt; x &lt;- sample(x=1:n, size=1e6, replace=TRUE) &gt; mean(x) - (n+1)/2 [1] -0.007369 &gt; var(x) - (n^2-1)/12 [1] 0.02429497 13.4 Bernoulli A single success/failure event, such as heads/tails when flipping a coin or survival/death. \\[X \\sim \\mbox{Bernoulli}(p)\\] \\[\\mathcal{R} = \\{0, 1\\}\\] \\[f(x; p) = p^x (1-p)^{1-x} \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = p, \\ {\\operatorname{Var}}(X) = p(1-p)\\] 13.5 Binomial An extension of the Bernoulli distribution to simultaneously considering \\(n\\) independent success/failure trials and counting the number of successes. \\[X \\sim \\mbox{Binomial}(n, p)\\] \\[\\mathcal{R} = \\{0, 1, 2, \\ldots, n\\}\\] \\[f(x; p) = {n \\choose x} p^x (1-p)^{n-x} \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = np, \\ {\\operatorname{Var}}(X) = np(1-p)\\] Note that \\({n \\choose x} = \\frac{n!}{x! (n-x)!}\\) is the number of unique ways to choose \\(x\\) items from \\(n\\) without respect to order. 13.6 Binomial PMF 13.7 Binomial in R &gt; str(dbinom) function (x, size, prob, log = FALSE) &gt; str(pbinom) function (q, size, prob, lower.tail = TRUE, log.p = FALSE) &gt; str(qbinom) function (p, size, prob, lower.tail = TRUE, log.p = FALSE) &gt; str(rbinom) function (n, size, prob) 13.8 Poisson Models the number of occurrences of something within a defined time/space period, where the occurrences are independent. Examples: the number of lightning strikes on campus in a given year; the number of emails received on a given day. \\[X \\sim \\mbox{Poisson}(\\lambda)\\] \\[\\mathcal{R} = \\{0, 1, 2, 3, \\ldots \\}\\] \\[f(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = \\lambda, \\ {\\operatorname{Var}}(X) = \\lambda\\] 13.9 Poisson PMF 13.10 Poisson in R &gt; str(dpois) function (x, lambda, log = FALSE) &gt; str(ppois) function (q, lambda, lower.tail = TRUE, log.p = FALSE) &gt; str(qpois) function (p, lambda, lower.tail = TRUE, log.p = FALSE) &gt; str(rpois) function (n, lambda) "],
["continuous-rvs.html", "14 Continuous RVs 14.1 Uniform (Continuous) 14.2 Uniform (Continuous) PDF 14.3 Uniform (Continuous) in R 14.4 Exponential 14.5 Exponential PDF 14.6 Exponential in R 14.7 Beta 14.8 Beta PDF 14.9 Beta in R 14.10 Normal 14.11 Normal PDF 14.12 Normal in R", " 14 Continuous RVs 14.1 Uniform (Continuous) Models the scenario where all values in the unit interval [0,1] are equally likely. \\[X \\sim \\mbox{Uniform}(0,1)\\] \\[\\mathcal{R} = [0,1]\\] \\[f(x) = 1 \\mbox{ for } x \\in \\mathcal{R}\\] \\[F(y) = y \\mbox{ for } y \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = 1/2, \\ {\\operatorname{Var}}(X) = 1/12\\] 14.2 Uniform (Continuous) PDF 14.3 Uniform (Continuous) in R &gt; str(dunif) function (x, min = 0, max = 1, log = FALSE) &gt; str(punif) function (q, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(qunif) function (p, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(runif) function (n, min = 0, max = 1) 14.4 Exponential Models a time to failure and has a “memoryless property”. \\[X \\sim \\mbox{Exponential}(\\lambda)\\] \\[\\mathcal{R} = [0, \\infty)\\] \\[f(x; \\lambda) = \\lambda e^{-\\lambda x} \\mbox{ for } x \\in \\mathcal{R}\\] \\[F(y; \\lambda) = 1 - e^{-\\lambda y} \\mbox{ for } y \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = \\frac{1}{\\lambda}, \\ {\\operatorname{Var}}(X) = \\frac{1}{\\lambda^2}\\] 14.5 Exponential PDF 14.6 Exponential in R &gt; str(dexp) function (x, rate = 1, log = FALSE) &gt; str(pexp) function (q, rate = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(qexp) function (p, rate = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(rexp) function (n, rate = 1) 14.7 Beta Yields values in \\((0,1)\\), so often used to generate random probabilities, such as the \\(p\\) in Bernoulli\\((p)\\). \\[X \\sim \\mbox{Beta}(\\alpha,\\beta) \\mbox{ where } \\alpha, \\beta &gt; 0\\] \\[\\mathcal{R} = (0,1)\\] \\[f(x; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta - 1} \\mbox{ for } x \\in \\mathcal{R}\\] where \\(\\Gamma(z) = \\int_{0}^{\\infty} x^{z-1} e^{-x} dx\\). \\[{\\operatorname{E}}[X] = \\frac{\\alpha}{\\alpha + \\beta}, \\ {\\operatorname{Var}}(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\] 14.8 Beta PDF 14.9 Beta in R &gt; str(dbeta) #shape1=alpha, shape2=beta function (x, shape1, shape2, ncp = 0, log = FALSE) &gt; str(pbeta) function (q, shape1, shape2, ncp = 0, lower.tail = TRUE, log.p = FALSE) &gt; str(qbeta) function (p, shape1, shape2, ncp = 0, lower.tail = TRUE, log.p = FALSE) &gt; str(rbeta) function (n, shape1, shape2, ncp = 0) 14.10 Normal Due to the Central Limit Theorem (covered later), this “bell curve” distribution is often observed in properly normalized real data. \\[X \\sim \\mbox{Normal}(\\mu, \\sigma^2)\\] \\[\\mathcal{R} = (-\\infty, \\infty)\\] \\[f(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}} \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = \\mu, \\ {\\operatorname{Var}}(X) = \\sigma^2\\] 14.11 Normal PDF 14.12 Normal in R &gt; str(dnorm) #notice it requires the STANDARD DEVIATION, not the variance function (x, mean = 0, sd = 1, log = FALSE) &gt; str(pnorm) function (q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(qnorm) function (p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(rnorm) function (n, mean = 0, sd = 1) "],
["joint-distributions.html", "15 Joint Distributions 15.1 Bivariate Random Variables 15.2 Events for Bivariate RVs 15.3 Marginal Distributions 15.4 Independent Random Variables 15.5 Conditional Distributions 15.6 Conditional Moments 15.7 Law of Total Variance 15.8 Covariance and Correlation 15.9 Multivariate Distributions 15.10 MV Expected Value 15.11 MV Variance-Covariance Matrix", " 15 Joint Distributions 15.1 Bivariate Random Variables For a pair of rv’s \\(X\\) and \\(Y\\) defined on the same probability space, we can define their joint pmf or pdf. For the discrete case, \\[\\begin{align*} f(x, y) &amp; = \\Pr(\\{\\omega: X(\\omega) = x\\} \\cap \\{\\omega: Y(\\omega) = y\\}) \\\\ \\ &amp; = \\Pr(X=x, Y=y). \\end{align*}\\] The joint pdf is defined analogously for continuous rv’s. 15.2 Events for Bivariate RVs Let \\(A_x \\times A_y \\subseteq \\mathbb{R} \\times \\mathbb{R}\\) be an event. Then \\(\\Pr(X \\in A_x, Y \\in A_y)\\) is calculated by: \\[\\begin{align*} &amp; \\sum_{x \\in A_x} \\sum_{y \\in A_y} f(x, y) &amp; \\mbox{(discrete)} \\\\ &amp; \\int_{x \\in A_x} \\int_{y \\in A_y} f(x, y) dy dx &amp; \\mbox{(continuous)} \\\\ &amp; \\int_{x \\in A_x} \\int_{y \\in A_y} dF_Y(y) dF_{X}(x) &amp; \\mbox{(general)} \\end{align*}\\] 15.3 Marginal Distributions We can calculate the marginal distribution of \\(X\\) (or \\(Y\\)) from their joint distribution: \\[f(x) = \\sum_{y \\in \\mathcal{R}_y} f(x, y)\\] \\[f(x) = \\int_{-\\infty}^{\\infty} f(x, y) dy\\] 15.4 Independent Random Variables Two rv’s are independent when their joint pmf or pdf factor: \\[f(x,y) = f(x) f(y)\\] This means, for example, in the continuous case, \\[\\begin{align*} \\Pr(X \\in A_x, Y \\in A_y) &amp; = \\int_{x \\in A_x} \\int_{y \\in A_y} f(x, y) dy dx \\\\ \\ &amp; = \\int_{x \\in A_x} \\int_{y \\in A_y} f(x) f(y) dy dx \\\\ \\ &amp; = \\Pr(X \\in A_x) \\Pr(Y \\in A_y) \\end{align*}\\] 15.5 Conditional Distributions We can define the conditional distribution of \\(X\\) given \\(Y\\) as follows. The conditional rv \\(X | Y \\sim F_{X|Y}\\) with conditional pmf or pdf for \\(X | Y=y\\) given by \\[ f(x | y) = \\frac{f(x, y)}{f(y)}. \\] 15.6 Conditional Moments The \\(k\\)th conditional moment (when it exists) is calculated by: \\[{\\operatorname{E}}\\left[X^k | Y=y\\right] = \\sum_{x \\in \\mathcal{R}_x} x^k f(x | y)\\] \\[{\\operatorname{E}}\\left[X^k | Y=y\\right] = \\int_{-\\infty}^{\\infty} x^k f(x | y) dx\\] Note that \\({\\operatorname{E}}\\left[X^k | Y\\right]\\) is a random variable that is a function of \\(Y\\) whose distribution is determined by that of \\(Y\\). 15.7 Law of Total Variance We can partition the variance of \\(X\\) according to the following conditional calculations on \\(Y\\): \\[{\\operatorname{Var}}(X) = {\\operatorname{Var}}({\\operatorname{E}}[X | Y]) + {\\operatorname{E}}[{\\operatorname{Var}}(X | Y)].\\] This is a useful result for partitioning variation in modeling fitting. 15.8 Covariance and Correlation The covariance, also called the “population covariance”, measures how two rv’s covary. It is calculated in a fashion analogous to the sample covariance: \\[{\\operatorname{Cov}}(X, Y) = \\operatorname{E} \\left[ (X - \\operatorname{E}[X]) (Y - \\operatorname{E}[Y]) \\right]\\] Note that \\({\\operatorname{Cov}}(X, X) = {\\operatorname{Var}}(X)\\). The population correlation is calculated analogously to the sample correlation: \\[\\operatorname{Cor}(X, Y) = \\frac{{\\operatorname{Cov}}(X, Y)}{\\operatorname{SD}(X)\\operatorname{SD}(Y)}\\] 15.9 Multivariate Distributions Let \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_n)^T\\) be a vector of \\(n\\) rv’s. We also let realized values be \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_n)^T\\). The joint pmf or pdf is written as \\[f(\\boldsymbol{x}) = f(x_1, x_2, \\ldots, x_n)\\] and if the rv’s are independent then \\[f(\\boldsymbol{x}) = \\prod_{i=1}^{n} f(x_i).\\] 15.10 MV Expected Value The expected value of \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_n)^T\\) is an \\(n\\)-vector: \\[{\\operatorname{E}}[\\boldsymbol{X}] = \\begin{bmatrix} {\\operatorname{E}}[X_1] \\\\ {\\operatorname{E}}[X_2] \\\\ \\vdots \\\\ {\\operatorname{E}}[X_n] \\end{bmatrix} \\] 15.11 MV Variance-Covariance Matrix The variance-covariance matrix of \\(\\boldsymbol{X}\\) is an \\(n \\times n\\) matrix with \\((i, j)\\) entry equal to \\({\\operatorname{Cov}}(X_i, X_j)\\). \\[{\\operatorname{Var}}(\\boldsymbol{X}) = \\begin{bmatrix} {\\operatorname{Var}}(X_1) &amp; {\\operatorname{Cov}}(X_1, X_2) &amp; \\cdots &amp; {\\operatorname{Cov}}(X_1, X_n) \\\\ {\\operatorname{Cov}}(X_2, X_1) &amp; {\\operatorname{Var}}(X_2) &amp; \\cdots &amp; \\vdots \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ {\\operatorname{Cov}}(X_n, X_1) &amp; \\cdots &amp; &amp; {\\operatorname{Var}}(X_n) \\end{bmatrix} \\] Often times \\(\\boldsymbol{\\Sigma}\\) is used as the matrix of population covariances in \\({\\operatorname{Var}}(\\boldsymbol{X})\\) so that \\(\\boldsymbol{\\Sigma}={\\operatorname{Var}}(\\boldsymbol{X})\\). "],
["multivariate-rvs.html", "16 Multivariate RVs 16.1 Multinomial 16.2 Multivariate Normal 16.3 Dirichlet 16.4 In R", " 16 Multivariate RVs 16.1 Multinomial Suppose \\(\\boldsymbol{X}\\) (an \\(m\\)-vector) is \\(\\mbox{Multinomial}_m(n, \\boldsymbol{p})\\), where \\(\\boldsymbol{p}\\) is an \\(m\\)-vector such that \\(\\sum_{i=1}^m p_i = 1\\). It has pmf \\[ f(\\boldsymbol{x}; \\boldsymbol{p}) = {n \\choose x_1 \\ x_2 \\ \\cdots \\ x_m} p_1^{x_1} p_2^{x_2} \\cdots p_m^{x_m} \\] where \\[{n \\choose x_1 \\ x_2 \\ \\cdots \\ x_m} = \\frac{n!}{x_1! x_2! \\cdots x_m!}\\] and \\(\\sum_{i=1}^m x_i = n\\). The Multinomial distribution is a generalization of the Binomial distribution. It models \\(n\\) independent outcomes where each outcome has probability \\(p_i\\) of category \\(i\\) occurring (for \\(i=1, 2, \\ldots, m\\)). The counts per category are contained in the \\(X_i\\) random variables that are constrained so that \\(\\sum_{i=1}^m X_i = n\\). It can be calculated that \\[{\\operatorname{E}}[X_i] = np_i, \\quad {\\operatorname{Var}}(X_i) = n p_i (1-p_i),\\] \\[{\\operatorname{Cov}}(X_i, X_j) = -n p_i p_j \\quad (i \\not= j).\\] 16.2 Multivariate Normal The \\(n\\)-vector \\(\\boldsymbol{X}\\) has Multivariate Normal distribution when \\(\\boldsymbol{X} \\sim \\mbox{MVN}_n(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) where \\(\\boldsymbol{\\mu}\\) is the \\(n\\)-vector of population means and \\(\\boldsymbol{\\Sigma}\\) is the \\(n \\times n\\) variance-covariance matrix. Its pdf is \\[ f(\\boldsymbol{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{2 \\pi |\\boldsymbol{\\Sigma}|}} \\exp -\\left\\{ -\\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}) \\right\\}. \\] Fun fact: \\(\\boldsymbol{\\Sigma}^{-1/2} (\\boldsymbol{X}-\\boldsymbol{\\mu}) \\sim \\mbox{MVN}_n(\\boldsymbol{0}, \\boldsymbol{I})\\). 16.3 Dirichlet The Dirichlet distribution models an \\(m\\)-vector \\(\\boldsymbol{X}\\) so that \\(0 \\leq X_i \\leq 1\\) and \\(\\sum_{i=1}^m X_i = 1\\). It is a generalization of the Beta distribution. The rv \\(\\boldsymbol{X} \\sim \\mbox{Dirichlet}_m(\\boldsymbol{\\alpha})\\), where \\(\\boldsymbol{\\alpha}\\) is an \\(m\\)-vector, has pdf \\[ f(\\boldsymbol{x}; \\boldsymbol{\\alpha}) = \\frac{\\Gamma\\left( \\sum_{i=1}^m \\alpha_i \\right)}{\\prod_{i=1}^m \\Gamma(\\alpha_i)} \\prod_{i=1}^m x_i^{\\alpha_i-1}. \\] It can be calculated that \\[{\\operatorname{E}}[X_i] = \\frac{\\alpha_i}{\\alpha_0}, {\\operatorname{Var}}(X_i) = \\frac{\\alpha_i (\\alpha_0 - \\alpha_i)}{\\alpha_0^2 (\\alpha_0 + 1)}, {\\operatorname{Cov}}(X_i, X_j) = \\frac{- \\alpha_i \\alpha_j}{\\alpha_0^2 (\\alpha_0 + 1)}\\] where \\(\\alpha_0 = \\sum_{k=1}^m \\alpha_k\\) and \\(i \\not= j\\) in \\({\\operatorname{Cov}}(X_i, X_j)\\). 16.4 In R For the Multinomial, base R contains the functions dmultinom and rmultinom. For the Multivariate Normal, there are several packages that work with this distribution. One choice is the package mvtnorm, which contains the functions dmvnorm and rmvnorm. For the Dirichlet, there are several packages that work with this distribution. One choice is the package MCMCpack, which contains the functions ddirichlet and rdirichlet. "],
["sums-of-random-variables.html", "17 Sums of Random Variables 17.1 Linear Transformation of a RV 17.2 Sums of Independent RVs 17.3 Sums of Dependent RVs 17.4 Means of Random Variables", " 17 Sums of Random Variables 17.1 Linear Transformation of a RV Suppose that \\(X\\) is a random variable and that \\(a\\) and \\(b\\) are constants. Then: \\[{\\operatorname{E}}\\left[a + bX \\right] = a + b {\\operatorname{E}}[X]\\] \\[{\\operatorname{Var}}\\left(a + bX \\right) = b^2 {\\operatorname{Var}}(X)\\] 17.2 Sums of Independent RVs If \\(X_1, X_2, \\ldots, X_n\\) are independent random variables, then: \\[{\\operatorname{E}}\\left[ \\sum_{i=1}^n X_i \\right] = \\sum_{i=1}^n {\\operatorname{E}}[X_i]\\] \\[{\\operatorname{Var}}\\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n {\\operatorname{Var}}(X_i)\\] 17.3 Sums of Dependent RVs If \\(X_1, X_2, \\ldots, X_n\\) are possibly dependent random variables, then: \\[{\\operatorname{E}}\\left[ \\sum_{i=1}^n X_i \\right] = \\sum_{i=1}^n {\\operatorname{E}}[X_i]\\] \\[{\\operatorname{Var}}\\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n {\\operatorname{Var}}(X_i) + \\sum_{i \\not= j} {\\operatorname{Cov}}(X_i, X_j)\\] Note that when \\(X_i\\) and \\(X_j\\) are independent (\\(i \\not= j\\)), then \\({\\operatorname{Cov}}(X_i, X_j) = 0\\). 17.4 Means of Random Variables Suppose \\(X_1, X_2, \\ldots, X_n\\) are independent and identically distributed (iid) random variables. Let \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) be their sample mean. Then: \\[{\\operatorname{E}}\\left[\\overline{X}_n \\right] = {\\operatorname{E}}[X_i]\\] \\[{\\operatorname{Var}}\\left(\\overline{X}_n \\right) = \\frac{1}{n}{\\operatorname{Var}}(X_i)\\] "],
["convergence-of-random-variables.html", "18 Convergence of Random Variables 18.1 Sequence of RVs 18.2 Convergence in Distribution 18.3 Convergence in Probability 18.4 Almost Sure Convergence 18.5 Strong Law of Large Numbers 18.6 Central Limit Theorem 18.7 Example: Calculations 18.8 Example: Plot", " 18 Convergence of Random Variables 18.1 Sequence of RVs Let \\(Z_1, Z_2, \\ldots\\) be an infinite sequence of rv’s. An important example is \\[Z_n = \\overline{X}_n = \\frac{\\sum_{i=1}^n X_i}{n}.\\] It is useful to be able to determine a limiting value or distribution of \\(\\{Z_i\\}\\). 18.2 Convergence in Distribution \\(\\{Z_i\\}\\) converges in distribution to \\(Z\\), written \\[Z_n \\stackrel{D}{\\longrightarrow} Z\\] if \\[F_{Z_n}(y) = \\Pr(Z_n \\leq y) \\rightarrow \\Pr(Z \\leq y) = F_{Z}(y)\\] as \\(n \\rightarrow \\infty\\) for all \\(y \\in \\mathbb{R}\\). 18.3 Convergence in Probability \\(\\{Z_i\\}\\) converges in probability to \\(Z\\), written \\[Z_n \\stackrel{P}{\\longrightarrow} Z\\] if \\[\\Pr(|Z_n - Z| \\leq \\epsilon) \\rightarrow 1\\] as \\(n \\rightarrow \\infty\\) for all \\(\\epsilon &gt; 0\\). Note that it may also be the case that \\(Z_n \\stackrel{P}{\\longrightarrow} \\theta\\) for a fixed, nonrandom value \\(\\theta\\). 18.4 Almost Sure Convergence \\(\\{Z_i\\}\\) converges almost surely (or “with probability 1”) to \\(Z\\), written \\[Z_n \\stackrel{a.s.}{\\longrightarrow} Z\\] if \\[\\Pr\\left(\\{\\omega: |Z_n(\\omega) - Z(\\omega)| \\stackrel{n \\rightarrow \\infty}{\\longrightarrow} 0 \\}\\right) = 1.\\] Note that it may also be the case that \\(Z_n \\stackrel{a.s.}{\\longrightarrow} \\theta\\) for a fixed, nonrandom value \\(\\theta\\). 18.5 Strong Law of Large Numbers Suppose \\(X_1, X_2, \\ldots, X_n\\) are iid rv’s with population mean \\({\\operatorname{E}}[X_i] = \\mu\\) where \\({\\operatorname{E}}[|X_i|] &lt; \\infty\\). Then \\[\\overline{X}_n \\stackrel{a.s.}{\\longrightarrow} \\mu.\\] 18.6 Central Limit Theorem Suppose \\(X_1, X_2, \\ldots, X_n\\) are iid rv’s with population mean \\({\\operatorname{E}}[X_i] = \\mu\\) and variance \\({\\operatorname{Var}}(X_i) = \\sigma^2\\). Then as \\(n \\rightarrow \\infty\\), \\[\\sqrt{n}(\\overline{X}_n - \\mu) \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0, \\sigma^2).\\] We can also get convergence to a \\(\\mbox{Normal}(0, 1)\\) by dividing by the standard deviation, \\(\\sigma\\): \\[\\frac{\\overline{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0, 1).\\] We write the second convergence result as above rather than \\[\\frac{\\sqrt{n} (\\overline{X}_n - \\mu)}{\\sigma} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0, 1)\\] because \\(\\sigma/\\sqrt{n}\\) is the “standard error” of \\(\\overline{X}_n\\) when \\(\\overline{X}_n\\) is treated as an estimator, so \\(\\sigma/\\sqrt{n}\\) is kept intact. Note that for fixed \\(n\\), \\[ {\\operatorname{E}}\\left[ \\frac{\\overline{X}_n - \\mu}{1/\\sqrt{n}} \\right] = 0 \\mbox{ and } {\\operatorname{Var}}\\left[ \\frac{\\overline{X}_n - \\mu}{1/\\sqrt{n}} \\right] = \\sigma^2, \\] \\[ {\\operatorname{E}}\\left[ \\frac{\\overline{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\right] = 0 \\mbox{ and } {\\operatorname{Var}}\\left[ \\frac{\\overline{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\right] = 1. \\] 18.7 Example: Calculations Let \\(X_1, X_2, \\ldots, X_{40}\\) be iid Poisson(\\(\\lambda\\)) with \\(\\lambda=6\\). We will form \\(\\sqrt{40}(\\overline{X} - 6)\\) over 10,000 realizations and compare their distribution to a Normal(0, 6) distribution. &gt; x &lt;- replicate(n=1e4, expr=rpois(n=40, lambda=6), + simplify=&quot;matrix&quot;) &gt; x_bar &lt;- apply(x, 2, mean) &gt; clt &lt;- sqrt(40)*(x_bar - 6) &gt; &gt; df &lt;- data.frame(clt=clt, x = seq(-18,18,length.out=1e4), + y = dnorm(seq(-18,18,length.out=1e4), + sd=sqrt(6))) 18.8 Example: Plot &gt; ggplot(data=df) + + geom_histogram(aes(x=clt, y=..density..), color=&quot;blue&quot;, + fill=&quot;lightgray&quot;, binwidth=0.75) + + geom_line(aes(x=x, y=y), size=1.5) "],
["population-principal-components-analysis.html", "19 Population Principal Components Analysis", " 19 Population Principal Components Analysis Suppose we have \\(m\\) random variables \\(X_1, X_2, \\ldots, X_m\\). We wish to identify a set of weights \\(w_1, w_2, \\ldots, w_m\\) that maximizes \\[ {\\operatorname{Var}}\\left(w_1 X_1 + w_2 X_2 + \\cdots + w_m X_m \\right). \\] However, this is unbounded, so we need to constrain the weights. It turns out that constraining the weights so that \\[ \\| {\\boldsymbol{w}}\\|_2^2 = \\sum_{i=1}^m w_i^2 = 1 \\] is both interpretable and mathematically tractable. Therefore we wish to maximize \\[ {\\operatorname{Var}}\\left(w_1 X_1 + w_2 X_2 + \\cdots + w_m X_m \\right) \\] subject to \\(\\| {\\boldsymbol{w}}\\|_2^2 = 1\\). Let \\({\\boldsymbol{\\Sigma}}\\) be the \\(m \\times m\\) population covariance matrix of the random variables \\(X_1, X_2, \\ldots, X_m\\). It follows that \\[ {\\operatorname{Var}}\\left(w_1 X_1 + w_2 X_2 + \\cdots + w_m X_m \\right) = {\\boldsymbol{w}}^T {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}. \\] Using a Lagrange multiplier, we wish to maximize \\[ {\\boldsymbol{w}}^T {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}+ \\lambda({\\boldsymbol{w}}^T {\\boldsymbol{w}}- 1). \\] Differentiating with respect to \\({\\boldsymbol{w}}\\) and setting to \\({\\boldsymbol{0}}\\), we get \\({\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}- \\lambda {\\boldsymbol{w}}= 0\\) or \\[ {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}= \\lambda {\\boldsymbol{w}}. \\] For any such \\({\\boldsymbol{w}}\\) and \\(\\lambda\\) where this holds, note that \\[ {\\operatorname{Var}}\\left(w_1 X_1 + w_2 X_2 + \\cdots + w_m X_m \\right) = {\\boldsymbol{w}}^T {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}= \\lambda \\] so the variance is \\(\\lambda\\). The eigendecompositon of a matrix identifies all such solutions to \\({\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}= \\lambda {\\boldsymbol{w}}\\). Specifically, it calculates the decompositon \\[ {\\boldsymbol{\\Sigma}}= {\\boldsymbol{W}}{\\boldsymbol{\\Lambda}}{\\boldsymbol{W}}^T \\] where \\({\\boldsymbol{W}}\\) is an \\(m \\times m\\) orthogonal matrix and \\({\\boldsymbol{\\Lambda}}\\) is a diagonal matrix with entries \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_m \\geq 0\\). The fact that \\({\\boldsymbol{W}}\\) is orthogonal means \\({\\boldsymbol{W}}{\\boldsymbol{W}}^T = {\\boldsymbol{W}}^T {\\boldsymbol{W}}= {\\boldsymbol{I}}\\). The following therefore hold: For each column \\(j\\) of \\({\\boldsymbol{W}}\\), say \\({\\boldsymbol{w}}_j\\), it follows that \\({\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}_j = \\lambda_j {\\boldsymbol{w}}_j\\) \\(\\| {\\boldsymbol{w}}_j \\|^2_2 = 1\\) and \\({\\boldsymbol{w}}_j^T {\\boldsymbol{w}}_k = {\\boldsymbol{0}}\\) for \\(\\lambda_j \\not= \\lambda_k\\) \\({\\operatorname{Var}}({\\boldsymbol{w}}_j^T {\\boldsymbol{X}}) = \\lambda_j\\) \\({\\operatorname{Var}}({\\boldsymbol{w}}_1^T {\\boldsymbol{X}}) \\geq {\\operatorname{Var}}({\\boldsymbol{w}}_2^T {\\boldsymbol{X}}) \\geq \\cdots \\geq {\\operatorname{Var}}({\\boldsymbol{w}}_m^T {\\boldsymbol{X}})\\) \\({\\boldsymbol{\\Sigma}}= \\sum_{j=1}^m \\lambda_j {\\boldsymbol{w}}_j {\\boldsymbol{w}}_j^T\\) For \\(\\lambda_j \\not= \\lambda_k\\), \\[{\\operatorname{Cov}}({\\boldsymbol{w}}_j^T {\\boldsymbol{X}}, {\\boldsymbol{w}}_k^T {\\boldsymbol{X}}) = {\\boldsymbol{w}}_j^T {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}_k = \\lambda_k {\\boldsymbol{w}}_j^T {\\boldsymbol{w}}_k = {\\boldsymbol{0}}\\] The \\(j\\)th population principal component (PC) of \\(X_1, X_2, \\ldots, X_m\\) is \\[ {\\boldsymbol{w}}_j^T {\\boldsymbol{X}}= w_{1j} X_1 + w_{2j} X_2 + \\cdots + w_{mj} X_m \\] where \\({\\boldsymbol{w}}_j = (w_{1j}, w_{2j}, \\ldots, w_{mj})^T\\) is column \\(j\\) of \\({\\boldsymbol{W}}\\) from the eigendecomposition \\[ {\\boldsymbol{\\Sigma}}= {\\boldsymbol{W}}{\\boldsymbol{\\Lambda}}{\\boldsymbol{W}}^T. \\] The column \\({\\boldsymbol{w}}_j\\) are called the loadings of the \\(j\\)th principal component. The variance explained by the \\(j\\)th PC is \\(\\lambda_j\\), which is diagonal element \\(j\\) of \\({\\boldsymbol{\\Lambda}}\\). "],
["from-probability-to-likelihood.html", "20 From Probability to Likelihood 20.1 Likelihood Function 20.2 Log-Likelihood Function 20.3 Sufficient Statistics 20.4 Factorization Theorem 20.5 Example: Normal 20.6 Likelihood Principle 20.7 Maximum Likelihood 20.8 Going Further", " 20 From Probability to Likelihood 20.1 Likelihood Function Suppose that we observe \\(x_1, x_2, \\ldots, x_n\\) according to the model \\(X_1, X_2, \\ldots, X_n \\sim F_{\\theta}\\). The joint pdf is \\(f(\\boldsymbol{x} ; \\theta)\\). We view the pdf as being a function of \\(\\boldsymbol{x}\\) for a fixed \\(\\theta\\). The likelihood function is obtained by reversing the arguments and viewing this as a function of \\(\\theta\\) for a fixed, observed \\(\\boldsymbol{x}\\): \\[L(\\theta ; \\boldsymbol{x}) = f(\\boldsymbol{x} ; \\theta).\\] 20.2 Log-Likelihood Function The log-likelihood function is \\[ \\ell(\\theta ; \\boldsymbol{x}) = \\log L(\\theta ; \\boldsymbol{x}).\\] When the data are iid, we have \\[ \\ell(\\theta ; \\boldsymbol{x}) = \\log \\prod_{i=1}^n f(x_i ; \\theta) = \\sum_{i=1}^n \\log f(x_i ; \\theta).\\] 20.3 Sufficient Statistics A statistic \\(T(\\boldsymbol{x})\\) is defined to be a function of the data. A sufficient statistic is a statistic where the distribution of data, conditional on this statistic, does not depend on \\(\\theta\\). That is, \\(\\boldsymbol{X} | T(\\boldsymbol{X})\\) does not depend on \\(\\theta\\). The interpretation is that the information in \\(\\boldsymbol{X}\\) about \\(\\theta\\) (the target of inference) is contained in \\(T(\\boldsymbol{X})\\). 20.4 Factorization Theorem The factorization theorem says that \\(T(\\boldsymbol{x})\\) is a sufficient statistic if and only if we can factor \\[f(\\boldsymbol{x} ; \\theta) = g(T(\\boldsymbol{x}), \\theta) h(\\boldsymbol{x}).\\] Therefore, if \\(T(\\boldsymbol{x})\\) is a sufficient statistic then \\[L(\\theta ; \\boldsymbol{x}) = g(T(\\boldsymbol{x}), \\theta) h(\\boldsymbol{x}) \\propto L(\\theta ; T(\\boldsymbol{x})).\\] This formalizes the idea that the information in \\(\\boldsymbol{X}\\) about \\(\\theta\\) (the target of inference) is contained in \\(T(\\boldsymbol{X})\\). 20.5 Example: Normal If \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu \\sigma^2)\\), then \\(\\overline{X}\\) is sufficient for \\(\\mu\\). As an exercise, show this via the factorization theorem. Hint: \\(\\sum_{i=1}^n (x_i - \\mu)^2 = \\sum_{i=1}^n (x_i - \\overline{x})^2 + n(\\overline{x} - \\mu)^2\\). 20.6 Likelihood Principle If \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) are two data sets so that \\[L(\\theta ; \\boldsymbol{x}) \\propto L(\\theta ; \\boldsymbol{y}),\\] \\[\\mbox{i.e., } L(\\theta ; \\boldsymbol{x}) = c(\\boldsymbol{x}, \\boldsymbol{y}) L(\\theta ; \\boldsymbol{y}),\\] then inferenece \\(\\theta\\) should be the same for \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\). 20.7 Maximum Likelihood A common starting point for inference is to calculate the maximum likelihood estimate. This is the value of \\(\\theta\\) that maximizes \\(L(\\theta ; \\boldsymbol{x})\\) for an observe data set \\(\\boldsymbol{x}\\). \\[\\begin{align*} \\hat{\\theta}_{{\\rm MLE}} &amp; = \\operatorname{argmax}_{\\theta} L(\\theta ; \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_{\\theta} \\ell (\\theta ; \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_{\\theta} L (\\theta ; T(\\boldsymbol{x})) \\end{align*}\\] where the last equality holds for sufficient statistics \\(T(\\boldsymbol{x})\\). 20.8 Going Further If this interests you, be sure to read about: Minimal sufficient statistics Complete sufficient statistics Ancillary statistics Basu’s theorem "],
["exponential-family-distributions.html", "21 Exponential Family Distributions 21.1 Rationale 21.2 Definition 21.3 Example: Bernoulli 21.4 Example: Normal 21.5 Natural Single Parameter EFD 21.6 Calculating Moments 21.7 Example: Normal 21.8 Maximum Likelihood 21.9 Table of Common EFDs", " 21 Exponential Family Distributions 21.1 Rationale Exponential family distributions (EFDs) provide a generalized parameterization and form of a very large class of distributions used in inference. For example, Binomia, Poisson, Exponential, Normal, Multinomial, MVN, and Dirichlet are all EFDs. The generalized form provides generally applicable formulas for moments, estimators, etc. EFDs also facilitate developing general algorithms for model fitting. 21.2 Definition If \\(X\\) follows an EFD then it has pdf of the form \\[f(x ; \\boldsymbol{\\theta}) = h(x) \\exp \\left\\{ \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(x) - A(\\boldsymbol{\\eta}) \\right\\} \\] where \\(\\boldsymbol{\\theta}\\) is a vector of parameters, \\(\\{T_k(x)\\}\\) are sufficient statistics, \\(A(\\boldsymbol{\\eta})\\) is the cumulant generating function. The functions \\(\\eta_k(\\boldsymbol{\\theta})\\) for \\(k=1, \\ldots, d\\) map the usual parameters to the “natural parameters”. \\(\\{T_k(x)\\}\\) are sufficient statistics for \\(\\{\\eta_k\\}\\) due to the factorization theorem. \\(A(\\boldsymbol{\\eta})\\) is sometimes called the “log normalizer” because \\[A(\\boldsymbol{\\eta}) = \\log \\int h(x) \\exp \\left\\{ \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(x) \\right\\}.\\] 21.3 Example: Bernoulli \\[\\begin{align*} f(x ; p) &amp; = p^x (1-p)^{1-x} \\\\ &amp; = \\exp \\left\\{ x \\log(p) + (1-x) \\log(1-p) \\right\\} \\\\ &amp; = \\exp \\left\\{ x \\log\\left( \\frac{p}{1-p} \\right) + \\log(1-p) \\right\\} \\end{align*}\\] \\(\\eta(p) = \\log\\left( \\frac{p}{1-p} \\right)\\) \\(T(x) = x\\) \\(A(\\eta) = \\log\\left(1 + e^\\eta\\right)\\) 21.4 Example: Normal \\[\\begin{align*} f(x ; \\mu, \\sigma^2) &amp; = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left\\{-\\frac{(x-\\mu)^2}{2 \\sigma^2}\\right\\} \\\\ &amp; = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left\\{\\frac{\\mu}{\\sigma^2} x - \\frac{1}{2 \\sigma^2} x^2 - \\log(\\sigma) - \\frac{\\mu^2}{2 \\sigma^2}\\right\\} \\end{align*}\\] \\(\\boldsymbol{\\eta}(\\mu, \\sigma^2) = \\left(\\frac{\\mu}{\\sigma^2}, - \\frac{1}{2 \\sigma^2} \\right)^T\\) \\(\\boldsymbol{T}(x) = \\left(x, x^2\\right)^T\\) \\(A(\\boldsymbol{\\eta}) = \\log(\\sigma) + \\frac{\\mu^2}{2 \\sigma^2} = -\\frac{1}{2} \\log(-2 \\eta_2) - \\frac{\\eta_1^2}{4\\eta_2}\\) 21.5 Natural Single Parameter EFD A natural single parameter EFD simplifies to the scenario where \\(d=1\\) and \\(T(x) = x\\): \\[f(x ; \\eta) = h(x) \\exp \\left\\{ \\eta x - A(\\eta) \\right\\} \\] 21.6 Calculating Moments \\[ \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}) = {\\operatorname{E}}[T_k(X)] \\] \\[ \\frac{\\partial^2}{\\partial \\eta_k^2} A(\\boldsymbol{\\eta}) = {\\operatorname{Var}}[T_k(X)] \\] 21.7 Example: Normal For \\(X \\sim \\mbox{Normal}(\\mu, \\sigma^2)\\), \\[{\\operatorname{E}}[X] = \\frac{\\partial}{\\partial \\eta_1} A(\\boldsymbol{\\eta}) = -\\frac{\\eta_1}{2 \\eta_2} = \\mu,\\] \\[{\\operatorname{Var}}(X) = \\frac{\\partial^2}{\\partial \\eta_1^2} A(\\boldsymbol{\\eta}) = -\\frac{1}{2 \\eta_2} = \\sigma^2.\\] 21.8 Maximum Likelihood Suppose \\(X_1, X_2, \\ldots, X_n\\) are iid from some EFD. Then, \\[ \\ell(\\boldsymbol{\\eta} ; \\boldsymbol{x}) = \\sum_{i=1}^n \\left[ \\log h(x_i) + \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(x_i) - A(\\boldsymbol{\\eta}) \\right] \\] \\[ \\frac{\\partial}{\\partial \\eta_k} \\ell(\\boldsymbol{\\eta} ; \\boldsymbol{x}) = \\sum_{i=1}^n T_k(x_i) - n \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}) \\] Setting the second equation to 0, it follows that the MLE of \\(\\eta_k\\) is the solution to \\[ \\frac{1}{n} \\sum_{i=1}^n T_k(x_i) = \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}). \\] 21.9 Table of Common EFDs "],
["statistical-inference.html", "22 Statistical Inference 22.1 Data Collection as a Probability 22.2 Example: Simple Random Sample 22.3 Example: Randomized Controlled Trial 22.4 Parameters and Statistics 22.5 Sampling Distribution 22.6 Central Dogma of Inference 22.7 Example: Fair Coin?", " 22 Statistical Inference 22.1 Data Collection as a Probability Suppose data are collected in such a way that it is randomly observed according to a probability distribution If that probability distribution can be parameterized, then it is possible that the parameters describe key characteristics of the population of interest Statistical inference reverse engineers this process to estimate the unknown values of the parameters and express a measure of uncertainty about these estimates 22.2 Example: Simple Random Sample Individuals are uniformly and independently randomly sampled from a population. The measurements taken on these individuals are then modeled as random variables, specifically random realizations from the complete population of values. Simple random samples form the basis of modern surveys. 22.3 Example: Randomized Controlled Trial Individuals under study are randomly assigned to one of two or more available treatments. This induces randomization directly into the study and breaks the relationship between the treatments and other variables that may be influencing the response of interest. This is the gold standard study design in clinical trials to assess the evidence that a new drug works on a given disease. 22.4 Parameters and Statistics A parameter is a number that describes a population A parameter is often a fixed number We usually do not know its value A statistic is a number calculated from a sample of data A statistic is used to estimate a parameter 22.5 Sampling Distribution The sampling distribution of a statistic is the probability disribution of the statistic under repeated realizations of the data from the assumed data generating probability distribution. The sampling distribution is how we connect an observed statistic to the population. 22.6 Central Dogma of Inference 22.7 Example: Fair Coin? Suppose I claim that a specific coin is fair, i.e., that it lands on heads or tails with equal probability. I flip it 20 times and it lands on heads 16 times. My data is \\(x=16\\) heads out of \\(n=20\\) flips. My data generation model is \\(X \\sim \\mbox{Binomial}(20, p)\\). I form the statistic \\(\\hat{p} = 16/20\\) as an estimate of \\(p\\). Let’s simulate 10,000 times what my estimate would look like if \\(p=0.5\\) and I repeated the 20 coin flips over and over. &gt; x &lt;- replicate(n=1e4, expr=rbinom(1, size=20, prob=0.5)) &gt; sim_p_hat &lt;- x/20 &gt; my_p_hat &lt;- 16/20 What can I do with this information? "],
["inference-goals-and-strategies.html", "23 Inference Goals and Strategies 23.1 Basic Idea 23.2 Normal Example 23.3 Point Estimate of \\(\\mu\\) 23.4 Sampling Distribution of \\(\\hat{\\mu}\\) 23.5 Pivotal Statistic", " 23 Inference Goals and Strategies 23.1 Basic Idea Data are collected in such a way that there exists a reasonable probability model for this process that involves parameters informative about the population. Common Goals: Form point estimates the parameters Quantify uncertainty on the estimates Test hypotheses on the parameters 23.2 Normal Example Suppose a simple random sample of \\(n\\) data points is collected so that the following model of the data is reasonable: \\(X_1, X_2, \\ldots, X_n\\) are iid Normal(\\(\\mu\\), \\(\\sigma^2\\)). The goal is to do inference on \\(\\mu\\), the population mean. For simplicity, assume that \\(\\sigma^2\\) is known (e.g., \\(\\sigma^2 = 1\\)). 23.3 Point Estimate of \\(\\mu\\) There are a number of ways to form an estimate of \\(\\mu\\), but one that has several justifications is the sample mean: \\[\\hat{\\mu} = \\overline{x} = \\frac{1}{n}\\sum_{i=1}^n x_i,\\] where \\(x_1, x_2, \\ldots, x_n\\) are the observed data points. 23.4 Sampling Distribution of \\(\\hat{\\mu}\\) If we were to repeat this study over and over, how would \\(\\hat{\\mu}\\) behave? \\[\\hat{\\mu} = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\] \\[\\overline{X} \\sim \\mbox{Normal}(\\mu, \\sigma^2/n)\\] How do we use this to quantify uncertainty and test hypotheses? 23.5 Pivotal Statistic One very useful strategy is to work backwards from a pivotal statistic, which is a statistic that does not depend on any unknown paramaters. Example: \\[\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1)\\] Note that in general for a rv \\(Y\\) it is the case that \\((Y - \\operatorname{E}[Y])/\\sqrt{\\operatorname{Var}(Y)}\\) has population mean 0 and variance 1. "],
["confidence-intervals.html", "24 Confidence Intervals 24.1 Goal 24.2 Formulation 24.3 Interpretation 24.4 A Normal CI 24.5 A Simulation 24.6 Normal\\((0,1)\\) Percentiles 24.7 Commonly Used Percentiles 24.8 \\((1-\\alpha)\\)-Level CIs 24.9 One-Sided CIs", " 24 Confidence Intervals 24.1 Goal Once we have a point estimate of a parameter, we would like a measure of its uncertainty. Given that we are working within a probabilistic framework, the natural language of uncertainty is through probability statements. We interpret this measure of uncertainty in terms of hypothetical repetitions of the sampling scheme we used to collect the original data set. 24.2 Formulation Confidence intervals take the form \\[(\\hat{\\mu} - C_{\\ell}, \\hat{\\mu} + C_{u})\\] where \\[{\\rm Pr}(\\mu - C_{\\ell} \\leq \\hat{\\mu} \\leq \\mu + C_{u})\\] forms the “level” or coverage probability of the interval. 24.3 Interpretation If we repeat the study many times, then the CI \\((\\hat{\\mu} - C_{\\ell}, \\hat{\\mu} + C_{u})\\) will contain the true value \\(\\mu\\) with a long run frequency equal to \\({\\rm Pr}(\\mu - C_{\\ell} \\leq \\hat{\\mu} \\leq \\mu + C_{u})\\). A CI calculated on an observed data set is not intepreted as: “There is probability \\({\\rm Pr}(\\mu - C_{\\ell} \\leq \\hat{\\mu} \\leq \\mu + C_{u})\\) that \\(\\mu\\) is in our calculated \\((\\hat{\\mu} - C_{\\ell}, \\hat{\\mu} + C_{u})\\).” Why not? 24.4 A Normal CI If \\(Z \\sim\\) Normal(0,1), then \\({\\rm Pr}(-1.96 \\leq Z \\leq 1.96) = 0.95.\\) \\[\\begin{eqnarray} 0.95 &amp; = &amp; {\\rm Pr} \\left(-1.96 \\leq \\frac{\\hat{\\mu} - \\mu}{\\sigma/\\sqrt{n}} \\leq 1.96 \\right) \\\\ \\ &amp; = &amp; {\\rm Pr} \\left(-1.96 \\frac{\\sigma}{\\sqrt{n}} \\leq \\hat{\\mu} - \\mu \\leq 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\ \\ &amp; = &amp; {\\rm Pr} \\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} \\leq \\hat{\\mu} \\leq \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right) \\end{eqnarray}\\] Therefore, \\(\\left(\\hat{\\mu} - 1.96\\frac{\\sigma}{\\sqrt{n}}, \\hat{\\mu} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\) forms a 95% confidence interval of \\(\\mu\\). 24.5 A Simulation &gt; mu &lt;- 5 &gt; n &lt;- 20 &gt; x &lt;- replicate(10000, rnorm(n=n, mean=mu)) # 10000 studies &gt; m &lt;- apply(x, 2, mean) # the estimate for each study &gt; ci &lt;- cbind(m - 1.96/sqrt(n), m + 1.96/sqrt(n)) &gt; head(ci) [,1] [,2] [1,] 4.797848 5.674386 [2,] 4.599996 5.476534 [3,] 4.472930 5.349468 [4,] 4.778946 5.655485 [5,] 4.778710 5.655248 [6,] 4.425023 5.301561 &gt; cover &lt;- (mu &gt; ci[,1]) &amp; (mu &lt; ci[,2]) &gt; mean(cover) [1] 0.9512 24.6 Normal\\((0,1)\\) Percentiles Above we constructed a 95% CI. How do we construct (1-\\(\\alpha\\))-level CIs? Let \\(z_{\\alpha}\\) be the \\(\\alpha\\) percentile of the Normal(0,1) distribution. If \\(Z \\sim\\) Normal(0,1), then \\[\\begin{eqnarray*} 1-\\alpha &amp; = &amp; {\\rm Pr}(z_{\\alpha/2} \\leq Z \\leq z_{1-\\alpha/2}) \\\\ \\ &amp; = &amp; {\\rm Pr}(-|z_{\\alpha/2}| \\leq Z \\leq |z_{\\alpha/2}|) \\end{eqnarray*}\\] 24.7 Commonly Used Percentiles &gt; # alpha/2 upper and lower percentiles for alpha=0.05 &gt; qnorm(0.025) [1] -1.959964 &gt; qnorm(0.975) [1] 1.959964 &gt; # alpha/2 upper and lower percentiles for alpha=0.10 &gt; qnorm(0.05) [1] -1.644854 &gt; qnorm(0.95) [1] 1.644854 24.8 \\((1-\\alpha)\\)-Level CIs If \\(Z \\sim\\) Normal(0,1), then \\({\\rm Pr}(-|z_{\\alpha/2}| \\leq Z \\leq |z_{\\alpha/2}|) = 1-\\alpha.\\) Repeating the steps from the 95% CI case, we get the following is a \\((1-\\alpha)\\)-Level CI for \\(\\mu\\): \\[\\left(\\hat{\\mu} - |z_{\\alpha/2}| \\frac{\\sigma}{\\sqrt{n}}, \\hat{\\mu} + |z_{\\alpha/2}| \\frac{\\sigma}{\\sqrt{n}}\\right)\\] 24.9 One-Sided CIs The CIs we have considered so far are “two-sided”. Sometimes we are also interested in “one-sided” CIs. If \\(Z \\sim\\) Normal(0,1), then \\(1-\\alpha = {\\rm Pr}(Z \\geq -|z_{\\alpha}|)\\) and \\(1-\\alpha = {\\rm Pr}(Z \\leq |z_{\\alpha}|).\\) We can use this fact along with the earlier derivations to show that the following are valid CIs: \\[(1-\\alpha)\\mbox{-level upper: } \\left(-\\infty, \\hat{\\mu} + |z_{\\alpha}| \\frac{\\sigma}{\\sqrt{n}}\\right)\\] \\[(1-\\alpha)\\mbox{-level lower: } \\left(\\hat{\\mu} - |z_{\\alpha}| \\frac{\\sigma}{\\sqrt{n}}, \\infty\\right)\\] "],
["hypothesis-tests.html", "25 Hypothesis Tests 25.1 Example: HT on Fairness of a Coin 25.2 A Caveat 25.3 Definition 25.4 Return to Normal Example 25.5 HTs on Parameter Values 25.6 Two-Sided vs. One-Sided HT 25.7 Test Statistic 25.8 Null Distribution (Two-Sided) 25.9 Null Distribution (One-Sided) 25.10 P-values 25.11 Calling a Test “Significant” 25.12 Types of Errors 25.13 Error Rates", " 25 Hypothesis Tests 25.1 Example: HT on Fairness of a Coin Suppose I claim that a specific coin is fair, i.e., that it lands on heads or tails with equal probability. I flip it 20 times and it lands on heads 16 times. My data is \\(x=16\\) heads out of \\(n=20\\) flips. My data generation model is \\(X \\sim \\mbox{Binomial}(20, p)\\). I form the statistic \\(\\hat{p} = 16/20\\) as an estimate of \\(p\\). More formally, I want to test the hypothesis: \\(H_0: p=0.5\\) vs. \\(H_1: p \\not=0.5\\) under the model \\(X \\sim \\mbox{Binomial}(20, p)\\) based on the test statistic \\(\\hat{p} = X/n\\). Let’s simulate 10,000 times what my estimate would look like if \\(p=0.5\\) and I repeated the 20 coin flips over and over. &gt; x &lt;- replicate(n=1e4, expr=rbinom(1, size=20, prob=0.5)) &gt; sim_p_hat &lt;- x/20 &gt; my_p_hat &lt;- 16/20 The vector sim_p_hat contains 10,000 draws from the null distribution, i.e., the distribution of my test statstic \\(\\hat{p} = X/n\\) when \\(H_0: p=0.5\\) is true. The deviation of the test statistic from the null hypothesis can be measured by \\(|\\hat{p} - 0.5|\\). Let’s compare our observed deviation \\(|16/20 - 0.5|\\) to the 10,000 simulated null data sets. Specifically, let’s calculate the frequency by which these 10,000 cases are as or more extreme than the observed test statistic. &gt; sum(abs(sim_p_hat-0.5) &gt;= abs(my_p_hat-0.5))/1e4 [1] 0.0055 This quantity is called the p-value of the hypothesis test. 25.2 A Caveat This example is a simplification of a more general framework for testing statistical hypotheses. Given the intuition provided by the example, let’s now formalize these ideas. 25.3 Definition A hypothesis test or significance test is a formal procedure for comparing observed data with a hypothesis whose truth we want to assess The results of a test are expressed in terms of a probability that measures how well the data and the hypothesis agree The null hypothesis (\\(H_0\\)) is the statement being tested, typically the status quo The alternative hypothesis (\\(H_1\\)) is the complement of the null, and it is often the “interesting” state 25.4 Return to Normal Example Let’s return to our Normal example in order to demonstrate the framework. Suppose a simple random sample of \\(n\\) data points is collected so that the following model of the data is reasonable: \\(X_1, X_2, \\ldots, X_n\\) are iid Normal(\\(\\mu\\), \\(\\sigma^2\\)). The goal is to do test a hypothesis on \\(\\mu\\), the population mean. For simplicity, assume that \\(\\sigma^2\\) is known (e.g., \\(\\sigma^2 = 1\\)). 25.5 HTs on Parameter Values Hypothesis tests are usually formulated in terms of values of parameters. For example: \\[H_0: \\mu = 5\\] \\[H_1: \\mu \\not= 5\\] Note that the choice of 5 here is arbitrary, for illustrative purposes only. In a typical real world problem, the values that define the hypotheses will be clear from the context. 25.6 Two-Sided vs. One-Sided HT Hypothesis tests can be two-sided or one-sided: \\[H_0: \\mu = 5 \\mbox{ vs. } H_1: \\mu \\not= 5 \\mbox{ (two-sided)}\\] \\[H_0: \\mu \\leq 5 \\mbox{ vs. } H_1: \\mu &gt; 5 \\mbox{ (one-sided)}\\] \\[H_0: \\mu \\geq 5 \\mbox{ vs. } H_1: \\mu &lt; 5 \\mbox{ (one-sided)}\\] 25.7 Test Statistic A test statistic is designed to quantify the evidence against the null hypothesis in favor of the alternative. They are usually defined (and justified using math theory) so that the larger the test statistic is, the more evidence there is. For the Normal example and the two-sided hypothesis \\((H_0: \\mu = 5 \\mbox{ vs. } H_1: \\mu \\not= 5)\\), here is our test statistic: \\[ |z| = \\frac{\\left|\\overline{x} - 5\\right|}{\\sigma/\\sqrt{n}} \\] What would the test statistic be for the one-sided hypothesis tests? 25.8 Null Distribution (Two-Sided) The null distribution is the sampling distribution of the test statistic when \\(H_0\\) is true. We saw earlier that \\(\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1).\\) When \\(H_0\\) is true, then \\(\\mu=5\\). So when \\(H_0\\) is true it follows that \\[Z = \\frac{\\overline{X} - 5}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1)\\] and then probabiliy calculations on \\(|Z|\\) are straightforward. Note that \\(Z\\) is pivotal when \\(H_0\\) is true! 25.9 Null Distribution (One-Sided) When performing a one-sided hypothesis test, such as \\(H_0: \\mu \\leq 5 \\mbox{ vs. } H_1: \\mu &gt; 5\\), the null distribution is typically calculated under the “least favorable” value, which is the boundary value. In this example it would be \\(\\mu=5\\) and we would again utilize the null distribution \\[Z = \\frac{\\overline{X} - 5}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1).\\] 25.10 P-values The p-value is defined to be the probability that a test statistic from the null distribution is as or more extreme than the observed statistic. In our Normal example on the two-sided hypothesis test, the p-value is \\[{\\rm Pr}(|Z^*| \\geq |z|)\\] where \\(Z^* \\sim \\mbox{Normal}(0,1)\\) and \\(|z|\\) is the value of the test statistic calculated on the data (so it is a fixed number once we observe the data). 25.11 Calling a Test “Significant” A hypothesis test is called statistically significant — meaning we reject \\(H_0\\) in favor of \\(H_1\\) — if its p-value is sufficiently small. Commonly used cut-offs are 0.01 or 0.05, although these are not always appropriate and they are historical artifacts. Applying a specific p-value cut-off to determine significance determines an error rate, which we define next. 25.12 Types of Errors There are two types of errors that can be committed when performing a hypothesis test. A Type I error or false positive is when a hypothesis test is called signficant and the null hypothesis is actually true. A Type II error or false negative is when a hypothesis test is not called signficant and the alternative hypothesis is actually true. 25.13 Error Rates The Type I error rate or false positive rate is the probability of this type of error given that \\(H_0\\) is true. If a hypothesis test is called significant when p-value \\(\\leq \\alpha\\) then it has a Type I error rate equal to \\(\\alpha\\). The Type II error rate or false negative rate is the probability of this type of error given that \\(H_1\\) is true. The power of a hypothesis test is \\(1 -\\) Type II error rate. Hypothesis tests are usually derived with a goal to control the Type I error rate while maximizing the power. "],
["maximum-likelihood-estimation.html", "26 Maximum Likelihood Estimation 26.1 The Normal Example 26.2 MLE \\(\\rightarrow\\) Normal Pivotal Statistics 26.3 Likelihood Function 26.4 Log-Likelihood Function 26.5 Calculating MLEs 26.6 Properties 26.7 Assumptions and Notation 26.8 Consistency 26.9 Equivariance 26.10 Fisher Information 26.11 Standard Error 26.12 Asymptotic Normal 26.13 Asymptotic Pivotal Statistic 26.14 Wald Test 26.15 Confidence Intervals 26.16 Optimality 26.17 Delta Method 26.18 Delta Method Example 26.19 Multiparameter Fisher Info Matrix 26.20 Multiparameter Asymptotic MVN", " 26 Maximum Likelihood Estimation 26.1 The Normal Example We formulated both confidence intervals and hypothesis tests under the following idealized scenario: Suppose a simple random sample of \\(n\\) data points is collected so that the following model of the data is reasonable: \\(X_1, X_2, \\ldots, X_n\\) are iid Normal(\\(\\mu\\), \\(\\sigma^2\\)). The goal is to do inference on \\(\\mu\\), the population mean. For simplicity, assume that \\(\\sigma^2\\) is known (e.g., \\(\\sigma^2 = 1\\)). There is a good reason why we did this. 26.2 MLE \\(\\rightarrow\\) Normal Pivotal Statistics The random variable distributions we introduced last week have maximum likelihood estimators (MLEs) that can be standardized to yield a pivotal statistic with a Normal(0,1) distribution based on MLE theory. For example, if \\(X \\sim \\mbox{Binomial}(n,p)\\) then \\(\\hat{p}=X/n\\) is the MLE. For large \\(n\\) it approximately holds that \\[ \\frac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\sim \\mbox{Normal}(0,1). \\] 26.3 Likelihood Function Suppose that we observe \\(x_1, x_2, \\ldots, x_n\\) according to the model \\(X_1, X_2, \\ldots, X_n \\sim F_{\\theta}\\). The joint pdf is \\(f(\\boldsymbol{x} ; \\theta)\\). We view the pdf as being a function of \\(\\boldsymbol{x}\\) for a fixed \\(\\theta\\). The likelihood function is obtained by reversing the arguments and viewing this as a function of \\(\\theta\\) for a fixed, observed \\(\\boldsymbol{x}\\): \\[L(\\theta ; \\boldsymbol{x}) = f(\\boldsymbol{x} ; \\theta).\\] 26.4 Log-Likelihood Function The log-likelihood function is \\[ \\ell(\\theta ; \\boldsymbol{x}) = \\log L(\\theta ; \\boldsymbol{x}).\\] When the data are iid, we have \\[ \\ell(\\theta ; \\boldsymbol{x}) = \\log \\prod_{i=1}^n f(x_i ; \\theta) = \\sum_{i=1}^n \\log f(x_i ; \\theta).\\] 26.5 Calculating MLEs The maximum likelihood estimate is the value of \\(\\theta\\) that maximizes \\(L(\\theta ; \\boldsymbol{x})\\) for an observe data set \\(\\boldsymbol{x}\\). \\[\\begin{align*} \\hat{\\theta}_{{\\rm MLE}} &amp; = \\operatorname{argmax}_{\\theta} L(\\theta ; \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_{\\theta} \\ell (\\theta ; \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_{\\theta} L (\\theta ; T(\\boldsymbol{x})) \\end{align*}\\] where the last equality holds for sufficient statistics \\(T(\\boldsymbol{x})\\). The MLE can usually be calculated analytically or numerically. 26.6 Properties When “certain regularity assumptions” are true, the following properties hold for MLEs. Consistent Equivariant Asymptotically Normal Asymptotically Efficient (or Optimal) Approximate Bayes Estimator 26.7 Assumptions and Notation We will assume that \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_{\\theta}\\) and let \\(\\hat{\\theta}_n\\) be the MLE of \\(\\theta\\) based on the \\(n\\) observations. The only exception is for the Binomial distribution where we will assume that \\(X \\sim \\mbox{Binomial}(n, p)\\), which is the sum of \\(n\\) iid \\(\\mbox{Bernoulli}(p)\\) rv’s. We will assume that the “certain regularity assumptions” are true in the following results. 26.8 Consistency An estimator is consistent if it converges in probability to the true parameter value. MLEs are consistent so that as \\(n \\rightarrow \\infty\\), \\[\\hat{\\theta}_n \\stackrel{P}{\\rightarrow} \\theta,\\] where \\(\\theta\\) is the true value. 26.9 Equivariance If \\(\\hat{\\theta}_n\\) is the MLE of \\(\\theta\\), then \\(g\\left(\\hat{\\theta}_n\\right)\\) is the MLE of \\(g(\\theta)\\). Example: For the Normal\\((\\mu, \\sigma^2)\\) the MLE of \\(\\mu\\) is \\(\\overline{X}\\). Therefore, the MLE of \\(e^\\mu\\) is \\(e^{\\overline{X}}\\). 26.10 Fisher Information The Fisher Information of \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_{\\theta}\\) is: \\[\\begin{align*} I_n(\\theta) &amp; = \\operatorname{Var}\\left( \\frac{d}{d\\theta} \\log f(\\boldsymbol{X}; \\theta) \\right) = \\sum_{i=1}^n \\operatorname{Var}\\left( \\frac{d}{d\\theta} \\log f(X_i; \\theta) \\right) \\\\ &amp; = - \\operatorname{E}\\left( \\frac{d^2}{d\\theta^2} \\log f(\\boldsymbol{X}; \\theta) \\right) = - \\sum_{i=1}^n \\operatorname{E}\\left( \\frac{d^2}{d\\theta^2} \\log f(X_i; \\theta) \\right) \\end{align*}\\] 26.11 Standard Error In general, the standard error of the standard deviation of sampling distribution of an estimate or statistic. For MLEs, the standard error is \\(\\sqrt{{\\operatorname{Var}}\\left(\\hat{\\theta}_n\\right)}\\). It has the approximation \\[\\operatorname{se}\\left(\\hat{\\theta}_n\\right) \\approx \\frac{1}{\\sqrt{I_n(\\theta)}}\\] and the standard error estimate is \\[\\hat{\\operatorname{se}}\\left(\\hat{\\theta}_n\\right) = \\frac{1}{\\sqrt{I_n\\left(\\hat{\\theta}_n\\right)}}.\\] 26.12 Asymptotic Normal MLEs converge in distribution to the Normal distribution. Specifically, as \\(n \\rightarrow \\infty\\), \\[\\frac{\\hat{\\theta}_n - \\theta}{{\\operatorname{se}}\\left(\\hat{\\theta}_n\\right)} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1)\\] and \\[\\frac{\\hat{\\theta}_n - \\theta}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1).\\] 26.13 Asymptotic Pivotal Statistic By the previous result, we now have an approximate (asymptotic) pivotal statistic: \\[Z = \\frac{\\hat{\\theta}_n - \\theta}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1).\\] This allows us to construct approximate confidence intervals and hypothesis test as in the idealized \\(\\mbox{Normal}(\\mu, \\sigma^2)\\) (with \\(\\sigma^2\\) known) scenario from the previous sections. 26.14 Wald Test Consider the hypothesis test, \\(H_0: \\theta=\\theta_0\\) vs \\(H_1: \\theta \\not= \\theta_0\\). We form test statistic \\[z = \\frac{\\hat{\\theta}_n - \\theta_0}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)},\\] which has approximate p-value \\[\\mbox{p-value} = {\\rm Pr}(|Z^*| \\geq |z|),\\] where \\(Z^*\\) is a Normal\\((0,1)\\) random variable. 26.15 Confidence Intervals Using this MLE theory, we can form approximate \\((1-\\alpha)\\) level confidence intervals as follows. Two-sided: \\[\\left(\\hat{\\theta}_n - |z_{\\alpha/2}| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right), \\hat{\\theta}_n + |z_{\\alpha/2}| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)\\right)\\] Upper: \\[\\left(-\\infty, \\hat{\\theta}_n + |z_{\\alpha}| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)\\right)\\] Lower: \\[\\left(\\hat{\\theta}_n - |z_{\\alpha}| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right), \\infty\\right)\\] 26.16 Optimality The MLE is such that \\[ \\sqrt{n} \\left( \\hat{\\theta}_n - \\theta \\right) \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0, \\tau^2)\\] for some \\(\\tau^2\\). Suppose that \\(\\tilde{\\theta}_n\\) is any other estimate so that \\[ \\sqrt{n} \\left( \\tilde{\\theta}_n - \\theta \\right) \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0, \\gamma^2).\\] It follows that \\[\\frac{\\tau^2}{\\gamma^2} \\leq 1.\\] 26.17 Delta Method Suppose that \\(g()\\) is a differentiable function and \\(g&#39;(\\theta) \\not= 0\\). Note that for some \\(t\\) in a neighborhood of \\(\\theta\\), a first-order Taylor expansion tells us that \\(g(t) \\approx g&#39;(\\theta) (t - \\theta)\\). From this we know that \\[{\\operatorname{Var}}\\left(g(\\hat{\\theta}_n) \\right) \\approx g&#39;(\\theta)^2 {\\operatorname{Var}}(\\hat{\\theta}_n)\\] The delta method shows that \\(\\hat{{\\operatorname{se}}}\\left(g(\\hat{\\theta}_n)\\right) = |g&#39;(\\hat{\\theta}_n)| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)\\) and \\[\\frac{g(\\hat{\\theta}_n) - g(\\theta)}{|g&#39;(\\hat{\\theta}_n)| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1).\\] 26.18 Delta Method Example Suppose \\(X \\sim \\mbox{Binomial}(n,p)\\) which has MLE, \\(\\hat{p} = X/n\\). By the equivariance property, the MLE of the per-trial variance \\(p(1-p)\\) is \\(\\hat{p}(1-\\hat{p})\\). It can be calculated that \\(\\hat{{\\operatorname{se}}}(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n}\\). Let \\(g(p) = p(1-p)\\). Then \\(g&#39;(p) = 1-2p\\). By the delta method, \\[\\hat{{\\operatorname{se}}}\\left( \\hat{p}(1-\\hat{p}) \\right) = \\left| (1-2\\hat{p}) \\right| \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}.\\] 26.19 Multiparameter Fisher Info Matrix Suppose that \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_{\\boldsymbol{\\theta}}\\) where \\(\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\ldots, \\theta_d)^T\\) has MLE \\(\\hat{\\boldsymbol{\\theta}}_n\\). The Fisher Information Matrix \\(I_n(\\boldsymbol{\\theta})\\) is the \\(d \\times d\\) matrix with \\((i, j)\\) entry \\[ -\\sum_{k=1}^n \\operatorname{E}\\left( \\frac{\\partial^2}{\\partial\\theta_i \\partial\\theta_j} \\log f(X_k; \\boldsymbol{\\theta}) \\right).\\] 26.20 Multiparameter Asymptotic MVN Under appropriate regularity conditions, as \\(n \\rightarrow \\infty\\), \\[ \\left( \\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta} \\right) \\stackrel{D}{\\longrightarrow} \\mbox{MVN}_d \\left( \\boldsymbol{0}, I_n(\\boldsymbol{\\theta})^{-1} \\right) \\mbox{ and } \\] \\[ \\left( \\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta} \\right)^T I_n(\\hat{\\boldsymbol{\\theta}}_n) \\left( \\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta} \\right) \\stackrel{D}{\\longrightarrow} \\mbox{MVN}_d \\left( \\boldsymbol{0}, \\boldsymbol{I} \\right). \\] "],
["mle-examples-one-sample.html", "27 MLE Examples: One Sample 27.1 Exponential Family Distributions 27.2 Summary of MLE Statistics 27.3 Notes 27.4 Binomial 27.5 Normal 27.6 Poisson 27.7 One-Sided CIs and HTs", " 27 MLE Examples: One Sample 27.1 Exponential Family Distributions Suppose \\(X_1, X_2, \\ldots, X_n\\) are iid from some EFD. Then, \\[ \\frac{\\partial}{\\partial \\eta_k} \\ell(\\boldsymbol{\\eta} ; \\boldsymbol{x}) = \\sum_{i=1}^n T_k(x_i) - n \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}) \\] Setting the second equation to 0, it follows that the MLE of \\(\\eta_k\\) is the solution to \\[ \\frac{1}{n} \\sum_{i=1}^n T_k(x_i) = \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}). \\] where note that \\(\\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}) = {\\operatorname{E}}[T_k(X)]\\). 27.2 Summary of MLE Statistics In all of these scenarios, \\(Z\\) converges in distribution to Normal\\((0,1)\\) for large \\(n\\). Distribution MLE Std Err \\(Z\\) Statistic Binomial\\((n,p)\\) \\(\\hat{p} = X/n\\) \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) \\(\\frac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}}\\) Normal\\((\\mu, \\sigma^2)\\) \\(\\hat{\\mu} = \\overline{X}\\) \\(\\frac{\\hat{\\sigma}}{\\sqrt{n}}\\) \\(\\frac{\\hat{\\mu} - \\mu}{\\hat{\\sigma}/\\sqrt{n}}\\) Poisson\\((\\lambda)\\) \\(\\hat{\\lambda} = \\overline{X}\\) \\(\\sqrt{\\frac{\\hat{\\lambda}}{n}}\\) \\(\\frac{\\hat{\\lambda} - \\lambda}{\\sqrt{\\hat{\\lambda}/n}}\\) 27.3 Notes For the Normal and Poisson distributions, our model is \\(X_1, X_2, \\ldots, X_n\\) iid from each respective distribution For the Binomial distribution, our model is \\(X \\sim \\mbox{Binomial}(n, p)\\) In the Normal model, \\(\\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n}}\\) is the MLE of \\(\\sigma\\) The above formulas were given in terms of the random variable probability models; on observed data the same formulas are used except we observed data lower case letters, e.g., replace \\(\\overline{X}\\) with \\(\\overline{x}\\) 27.4 Binomial Approximate \\((1-\\alpha)\\)-level two-sided CI: \\[\\left(\\hat{p} - |z_{\\alpha/2}| \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + |z_{\\alpha/2}| \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right)\\] Hypothesis test, \\(H_0: p=p_0\\) vs \\(H_1: p \\not= p_0\\): \\[z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\mbox{ and } \\mbox{p-value} = {\\rm Pr}(|Z^*| \\geq |z|)\\] where \\(Z^*\\) is a Normal\\((0,1)\\) random variable. 27.5 Normal Approximate \\((1-\\alpha)\\)-level two-sided CI: \\[\\left(\\hat{\\mu} - |z_{\\alpha/2}| \\frac{\\hat{\\sigma}}{\\sqrt{n}}, \\hat{\\mu} + |z_{\\alpha/2}| \\frac{\\hat{\\sigma}}{\\sqrt{n}} \\right)\\] Hypothesis test, \\(H_0: \\mu=\\mu_0\\) vs \\(H_1: \\mu \\not= \\mu_0\\): \\[z = \\frac{\\hat{\\mu} - \\mu_0}{\\hat{\\sigma}/\\sqrt{n}} \\mbox{ and } \\mbox{p-value} = {\\rm Pr}(|Z^*| \\geq |z|)\\] where \\(Z^*\\) is a Normal\\((0,1)\\) random variable. 27.6 Poisson Approximate \\((1-\\alpha)\\)-level two-sided CI: \\[\\left(\\hat{\\lambda} - |z_{\\alpha/2}| \\sqrt{\\frac{\\hat{\\lambda}}{n}}, \\hat{\\lambda} + |z_{\\alpha/2}| \\sqrt{\\frac{\\hat{\\lambda}}{n}} \\right)\\] Hypothesis test, \\(H_0: \\lambda=\\lambda_0\\) vs \\(H_1: \\lambda \\not= \\lambda_0\\): \\[z = \\frac{\\hat{\\lambda} - \\lambda_0}{\\sqrt{\\frac{\\hat{\\lambda}}{n}}} \\mbox{ and } \\mbox{p-value} = {\\rm Pr}(|Z^*| \\geq |z|)\\] where \\(Z^*\\) is a Normal\\((0,1)\\) random variable. 27.7 One-Sided CIs and HTs The one-sided versions of these approximate confidence intervals and hypothesis tests work analogously. The procedures shown for the \\(\\mbox{Normal}(\\mu, \\sigma^2)\\) case with known \\(\\sigma^2\\) from last week are utilzied with the appropriate subsitutions as in the above examples. "],
["mle-examples-two-samples.html", "28 MLE Examples: Two-Samples 28.1 Comparing Two Populations 28.2 Two RVs 28.3 Two Sample Means 28.4 Two MLEs 28.5 Poisson 28.6 Normal (Unequal Variances) 28.7 Normal (Equal Variances) 28.8 Binomial 28.9 Example: Binomial CI 28.10 Example: Binomial HT", " 28 MLE Examples: Two-Samples 28.1 Comparing Two Populations So far we have concentrated on analyzing \\(n\\) observations from a single population. However, suppose that we want to do inference to compare two populations? The framework we have described so far is easily extended to accommodate this. 28.2 Two RVs If \\(X\\) and \\(Y\\) are independent rv’s then: \\[{\\rm E}[X - Y] = {\\rm E}[X] - {\\rm E}[Y]\\] \\[{\\rm Var}(X-Y) = {\\rm Var}(X) + {\\rm Var}(Y)\\] 28.3 Two Sample Means Let \\(X_1, X_2, \\ldots, X_{n_1}\\) be iid rv’s with population mean \\(\\mu_1\\) and population variance \\(\\sigma^2_1\\). Let \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) be iid rv’s with population mean \\(\\mu_2\\) and population variance \\(\\sigma^2_2\\). Assume that the two sets of rv’s are independent. Then when the CLT applies to each set of rv’s, as \\(\\min(n_1, n_2) \\rightarrow \\infty\\), it follows that \\[ \\frac{\\overline{X} - \\overline{Y} - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1)\\] 28.4 Two MLEs Suppose \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_{\\theta}\\) and \\(Y_1, Y_2, \\ldots, Y_m \\stackrel{{\\rm iid}}{\\sim} F_{\\gamma}\\) with MLEs \\(\\hat{\\theta}_n\\) and \\(\\hat{\\gamma}_m\\), respectively. Then as \\(\\min(n, m) \\rightarrow \\infty\\), \\[\\frac{\\hat{\\theta}_n - \\hat{\\gamma}_m - (\\theta - \\gamma)}{\\sqrt{\\hat{{\\operatorname{se}}}(\\hat{\\theta}_n)^2 + \\hat{{\\operatorname{se}}}(\\hat{\\gamma}_m)^2}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1).\\] 28.5 Poisson Let \\(X_1, X_2, \\ldots, X_{n_1}\\) be iid \\(\\mbox{Poisson}(\\lambda_1)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) be iid \\(\\mbox{Poisson}(\\lambda_2)\\). We have \\(\\hat{\\lambda}_1 = \\overline{X}\\) and \\(\\hat{\\lambda}_2 = \\overline{Y}\\). As \\(\\min(n_1, n_2) \\rightarrow \\infty\\), \\[ \\frac{\\hat{\\lambda}_1 - \\hat{\\lambda}_2 - (\\lambda_1 - \\lambda_2)}{\\sqrt{\\frac{\\hat{\\lambda}_1}{n_1} + \\frac{\\hat{\\lambda}_2}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1). \\] 28.6 Normal (Unequal Variances) Let \\(X_1, X_2, \\ldots, X_{n_1}\\) be iid \\(\\mbox{Normal}(\\mu_1, \\sigma^2_1)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) be iid \\(\\mbox{Normal}(\\mu_2, \\sigma^2_2)\\). We have \\(\\hat{\\mu}_1 = \\overline{X}\\) and \\(\\hat{\\mu}_2 = \\overline{Y}\\). As \\(\\min(n_1, n_2) \\rightarrow \\infty\\), \\[ \\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\hat{\\sigma}^2_1}{n_1} + \\frac{\\hat{\\sigma}^2_2}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1). \\] 28.7 Normal (Equal Variances) Let \\(X_1, X_2, \\ldots, X_{n_1}\\) be iid \\(\\mbox{Normal}(\\mu_1, \\sigma^2)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) be iid \\(\\mbox{Normal}(\\mu_2, \\sigma^2)\\). We have \\(\\hat{\\mu}_1 = \\overline{X}\\) and \\(\\hat{\\mu}_2 = \\overline{Y}\\). As \\(\\min(n_1, n_2) \\rightarrow \\infty\\), \\[ \\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\hat{\\sigma}^2}{n_1} + \\frac{\\hat{\\sigma}^2}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1) \\] where \\[ \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n_1}(X_i - \\overline{X})^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline{Y})^2}{n_1 + n_2} \\] 28.8 Binomial Let \\(X \\sim \\mbox{Binomial}(n_1, p_1)\\) and \\(Y \\sim \\mbox{Binomial}(n_2, p_2)\\). We have \\(\\hat{p}_1 = X/n_1\\) and \\(\\hat{p}_2 = Y/n_2\\). As \\(\\min(n_1, n_2) \\rightarrow \\infty\\), \\[ \\frac{\\hat{p}_1 - \\hat{p}_2 - (p_1 - p_2)}{\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1). \\] 28.9 Example: Binomial CI A 95% CI for the difference \\(p_1 - p_2\\) can be obtained by unfolding the above pivotal statistic: \\[\\left((\\hat{p}_1 - \\hat{p}_2) - 1.96 \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\right.,\\] \\[\\left. (\\hat{p}_1 - \\hat{p}_2) + 1.96 \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\right)\\] 28.10 Example: Binomial HT Suppose we wish to test \\(H_0: p_1 = p_2\\) vs \\(H_1: p_1 \\not= p_2\\). First form the \\(z\\)-statistic: \\[ z = \\frac{\\hat{p}_1 - \\hat{p}_2 }{\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}}. \\] Now, calculate the p-value: \\[ {\\rm Pr}(|Z^*| \\geq |z|) \\] where \\(Z^*\\) is a Normal(0,1) random variable. "],
["the-t-distribution.html", "29 The t Distribution 29.1 Normal, Unknown Variance 29.2 Aside: Chi-Square Distribution 29.3 Theoretical Basis of the t 29.4 When Is t Utilized? 29.5 t vs Normal 29.6 t Percentiles 29.7 Confidence Intervals 29.8 Hypothesis Tests 29.9 Two-Sample t-Distribution 29.10 Two-Sample t-Distribution 29.11 Two-Sample t-Distributions", " 29 The t Distribution 29.1 Normal, Unknown Variance Suppose a sample of \\(n\\) data points is modeled by \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu, \\sigma^2)\\) where \\(\\sigma^2\\) is unknown. Recall that \\(S = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}}\\) is the sample standard deviation. The statistic \\[\\frac{\\overline{X} - \\mu}{S/\\sqrt{n}}\\] has a \\(t_{n-1}\\) distribution, a t-distribution with \\(n-1\\) degrees of freedom. 29.2 Aside: Chi-Square Distribution Suppose \\(Z_1, Z_2, \\ldots, Z_v \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(0, 1)\\). Then \\(Z_1^2 + Z_2^2 + \\cdots + Z_v^2\\) has a \\(\\chi^2_v\\) distribution, where \\(v\\) is the degrees of freedom. This \\(\\chi^2_v\\) rv has a pdf, expected value equal to \\(v\\), and variance equal to \\(2v\\). Also, \\[\\frac{(n-1) S^2}{\\sigma^2} \\sim \\chi^2_{n-1}.\\] 29.3 Theoretical Basis of the t Suppose that \\(Z \\sim \\mbox{Normal}(0,1)\\), \\(X \\sim \\chi^2_v\\), and \\(Z\\) and \\(X\\) are independent. Then \\(\\frac{Z}{\\sqrt{X/v}}\\) has a \\(t_v\\) distribution. Since \\(\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1)\\) and \\(\\overline{X}\\) and \\(S^2\\) are independent (shown later), it follows that the following has a \\(t_{n-1}\\) distribution: \\[\\frac{\\overline{X} - \\mu}{S/\\sqrt{n}}.\\] 29.4 When Is t Utilized? The t distribution and its corresponding CI’s and HT’s are utilized when the data are Normal (or approximately Normal) and \\(n\\) is small Small typically means that \\(n &lt; 30\\) In this case the inference based on the t distribution will be more accurate When \\(n \\geq 30\\), there is very little difference between using \\(t\\)-statistics and \\(z\\)-statistics 29.5 t vs Normal 29.6 t Percentiles We calculated percentiles of the Normal(0,1) distribution (e.g., \\(z_\\alpha\\)). We can do the analogous calculation with the t distribution. Let \\(t_\\alpha\\) be the \\(\\alpha\\) percentile of the t distribution. Examples: &gt; qt(0.025, df=4) # alpha = 0.025 [1] -2.776445 &gt; qt(0.05, df=4) [1] -2.131847 &gt; qt(0.95, df=4) [1] 2.131847 &gt; qt(0.975, df=4) [1] 2.776445 29.7 Confidence Intervals Here is a \\((1-\\alpha)\\)-level CI for \\(\\mu\\) using this distribution: \\[ \\left(\\hat{\\mu} - |t_{\\alpha/2}| \\frac{s}{\\sqrt{n}}, \\hat{\\mu} + |t_{\\alpha/2}| \\frac{s}{\\sqrt{n}} \\right), \\] where as before \\(\\hat{\\mu} = \\overline{x}\\). This produces a wider CI than the \\(z\\) statistic analogue. 29.8 Hypothesis Tests Suppose we want to test \\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\not= \\mu_0\\) where \\(\\mu_0\\) is a known, given number. The t-statistic is \\[ t = \\frac{\\hat{\\mu} - \\mu_0}{\\frac{s}{\\sqrt{n}}} \\] with p-value \\[ {\\rm Pr}(|T^*| \\geq |t|) \\] where \\(T^* \\sim t_{n-1}\\). 29.9 Two-Sample t-Distribution Let \\(X_1, X_2, \\ldots, X_{n_1} \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu_1, \\sigma^2_1)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2} \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu_2, \\sigma^2_2)\\) have unequal variances. We have \\(\\hat{\\mu}_1 = \\overline{X}\\) and \\(\\hat{\\mu}_2 = \\overline{Y}\\). The unequal variance two-sample t-statistic is \\[ t = \\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{S^2_1}{n_1} + \\frac{S^2_2}{n_2}}}. \\] 29.10 Two-Sample t-Distribution Let \\(X_1, X_2, \\ldots, X_{n_1} \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu_1, \\sigma^2)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2} \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu_2, \\sigma^2)\\) have equal variance. We have \\(\\hat{\\mu}_1 = \\overline{X}\\) and \\(\\hat{\\mu}_2 = \\overline{Y}\\). The equal variance two-sample t-statistic is \\[ t = \\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{S^2}{n_1} + \\frac{S^2}{n_2}}}. \\] where \\[ S^2 = \\frac{\\sum_{i=1}^{n_1}(X_i - \\overline{X})^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline{Y})^2}{n_1 + n_2 - 2}. \\] 29.11 Two-Sample t-Distributions When the two populations have equal variances, the pivotal t-statistic follows a \\(t_{n_1 + n_2 -2}\\) distribution. When there are unequal variances, the pivotal t-statistic follows a t distribution where the degrees of freedom comes from an approximation using the Welch–Satterthwaite equation (which R calculates). "],
["inference-in-r.html", "30 Inference in R 30.1 BSDA Package 30.2 Example: Poisson 30.3 Direct Calculations 30.4 Commonly Used Functions 30.5 About These Functions 30.6 Normal Data: “Davis” Data Set 30.7 Height vs Weight 30.8 An Error? 30.9 Updated Height vs Weight 30.10 Density Plots of Height 30.11 Density Plots of Weight 30.12 t.test() Function 30.13 Two-Sided Test of Male Height 30.14 Output of t.test() 30.15 Tidying the Output 30.16 Two-Sided Test of Female Height 30.17 Difference of Two Means 30.18 Test with Equal Variances 30.19 Paired Sample Test (v. 1) 30.20 Paired Sample Test (v. 2) 30.21 The Coin Flip Example 30.22 binom.test() 30.23 alternative = &quot;greater&quot; 30.24 alternative = &quot;less&quot; 30.25 prop.test() 30.26 An Observation 30.27 Wording of Surveys 30.28 The Data 30.29 Inference on the Difference 30.30 90% Confidence Interval 30.31 Poisson Data: poisson.test() 30.32 Example: RNA-Seq 30.33 \\(H_1: \\lambda_1 \\not= \\lambda_2\\) 30.34 \\(H_1: \\lambda_1 &lt; \\lambda_2\\) 30.35 \\(H_1: \\lambda_1 &gt; \\lambda_2\\) 30.36 Question", " 30 Inference in R 30.1 BSDA Package &gt; install.packages(&quot;BSDA&quot;) &gt; library(BSDA) &gt; str(z.test) function (x, y = NULL, alternative = &quot;two.sided&quot;, mu = 0, sigma.x = NULL, sigma.y = NULL, conf.level = 0.95) 30.2 Example: Poisson Apply z.test(): &gt; set.seed(210) &gt; n &lt;- 40 &gt; lam &lt;- 14 &gt; x &lt;- rpois(n=n, lambda=lam) &gt; lam.hat &lt;- mean(x) &gt; stddev &lt;- sqrt(lam.hat) &gt; z.test(x=x, sigma.x=stddev, mu=lam) One-sample z-Test data: x z = 0.41885, p-value = 0.6753 alternative hypothesis: true mean is not equal to 14 95 percent confidence interval: 13.08016 15.41984 sample estimates: mean of x 14.25 30.3 Direct Calculations Confidence interval: &gt; lam.hat &lt;- mean(x) &gt; lam.hat [1] 14.25 &gt; stderr &lt;- sqrt(lam.hat)/sqrt(n) &gt; lam.hat - abs(qnorm(0.025)) * stderr # lower bound [1] 13.08016 &gt; lam.hat + abs(qnorm(0.025)) * stderr # upper bound [1] 15.41984 Hypothesis test: &gt; z &lt;- (lam.hat - lam)/stderr &gt; z # test statistic [1] 0.4188539 &gt; 2 * pnorm(-abs(z)) # two-sided p-value [1] 0.6753229 30.4 Commonly Used Functions R has the following functions for doing inference on some of the distributions we have considered. Normal: t.test() Binomial: binomial.test() or prop.test() Poisson: poisson.test() These perform one-sample and two-sample hypothesis testing and confidence interval construction for both the one-sided and two-sided cases. 30.5 About These Functions We covered a convenient, unified MLE framework that allows us to better understand how confidence intervals and hypothesis testing are performed However, this framework requires large sample sizes and is not necessarily the best method to apply in all circumstances The above R functions are versatile functions for analyzing Normal, Binomial, and Poisson distributed data (or approximations thereof) that use much broader theory and methods than we have covered However, the arguments these functions take and the ouput of the functions are in line with the framework that we have covered 30.6 Normal Data: “Davis” Data Set &gt; library(&quot;car&quot;) &gt; data(&quot;Davis&quot;) &gt; htwt &lt;- tbl_df(Davis) &gt; htwt # A tibble: 200 x 5 sex weight height repwt repht &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 M 77 182 77 180 2 F 58 161 51 159 3 F 53 161 54 158 4 M 68 177 70 175 5 F 59 157 59 155 6 M 76 170 76 165 7 M 76 167 77 165 8 M 69 186 73 180 9 M 71 178 71 175 10 M 65 171 64 170 # … with 190 more rows 30.7 Height vs Weight &gt; ggplot(htwt) + + geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) + + scale_colour_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 30.8 An Error? &gt; which(htwt$height &lt; 100) [1] 12 &gt; htwt[12,] # A tibble: 1 x 5 sex weight height repwt repht &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 F 166 57 56 163 &gt; htwt[12,c(2,3)] &lt;- htwt[12,c(3,2)] 30.9 Updated Height vs Weight &gt; ggplot(htwt) + + geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) + + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 30.10 Density Plots of Height &gt; ggplot(htwt) + + geom_density(aes(x=height, color=sex), size=1.5) + + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 30.11 Density Plots of Weight &gt; ggplot(htwt) + + geom_density(aes(x=weight, color=sex), size=1.5) + + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 30.12 t.test() Function From the help file… Usage t.test(x, ...) ## Default S3 method: t.test(x, y = NULL, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...) ## S3 method for class &#39;formula&#39; t.test(formula, data, subset, na.action, ...) 30.13 Two-Sided Test of Male Height &gt; m_ht &lt;- htwt %&gt;% filter(sex==&quot;M&quot;) %&gt;% select(height) &gt; testresult &lt;- t.test(x = m_ht$height, mu=177) &gt; class(testresult) [1] &quot;htest&quot; &gt; is.list(testresult) [1] TRUE 30.14 Output of t.test() &gt; names(testresult) [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;conf.int&quot; &quot;estimate&quot; [6] &quot;null.value&quot; &quot;stderr&quot; &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; &gt; testresult One Sample t-test data: m_ht$height t = 1.473, df = 87, p-value = 0.1443 alternative hypothesis: true mean is not equal to 177 95 percent confidence interval: 176.6467 179.3760 sample estimates: mean of x 178.0114 30.15 Tidying the Output &gt; library(broom) &gt; tidy(testresult) # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 178. 1.47 0.144 87 177. 179. One S… # … with 1 more variable: alternative &lt;chr&gt; 30.16 Two-Sided Test of Female Height &gt; f_ht &lt;- htwt %&gt;% filter(sex==&quot;F&quot;) %&gt;% select(height) &gt; t.test(x = f_ht$height, mu = 164) One Sample t-test data: f_ht$height t = 1.3358, df = 111, p-value = 0.1844 alternative hypothesis: true mean is not equal to 164 95 percent confidence interval: 163.6547 165.7739 sample estimates: mean of x 164.7143 30.17 Difference of Two Means &gt; t.test(x = m_ht$height, y = f_ht$height) Welch Two Sample t-test data: m_ht$height and f_ht$height t = 15.28, df = 174.29, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 11.57949 15.01467 sample estimates: mean of x mean of y 178.0114 164.7143 30.18 Test with Equal Variances &gt; htwt %&gt;% group_by(sex) %&gt;% summarize(sd(height)) # A tibble: 2 x 2 sex `sd(height)` &lt;fct&gt; &lt;dbl&gt; 1 F 5.66 2 M 6.44 &gt; t.test(x = m_ht$height, y = f_ht$height, var.equal = TRUE) Two Sample t-test data: m_ht$height and f_ht$height t = 15.519, df = 198, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 11.60735 14.98680 sample estimates: mean of x mean of y 178.0114 164.7143 30.19 Paired Sample Test (v. 1) First take the difference between the paired observations. Then apply the one-sample t-test. &gt; htwt &lt;- htwt %&gt;% mutate(diffwt = (weight - repwt), + diffht = (height - repht)) &gt; t.test(x = htwt$diffwt) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.00546 0.0319 0.975 182 -0.332 0.343 One S… # … with 1 more variable: alternative &lt;chr&gt; &gt; t.test(x = htwt$diffht) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 2.08 13.5 2.64e-29 182 1.77 2.38 One S… # … with 1 more variable: alternative &lt;chr&gt; 30.20 Paired Sample Test (v. 2) Enter each sample into the t.test() function, but use the paired=TRUE argument. This is operationally equivalent to the previous version. &gt; t.test(x=htwt$weight, y=htwt$repwt, paired=TRUE) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.00546 0.0319 0.975 182 -0.332 0.343 Paire… # … with 1 more variable: alternative &lt;chr&gt; &gt; t.test(x=htwt$height, y=htwt$repht, paired=TRUE) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 2.08 13.5 2.64e-29 182 1.77 2.38 Paire… # … with 1 more variable: alternative &lt;chr&gt; &gt; htwt %&gt;% select(height, repht) %&gt;% na.omit() %&gt;% + summarize(mean(height), mean(repht)) # A tibble: 1 x 2 `mean(height)` `mean(repht)` &lt;dbl&gt; &lt;dbl&gt; 1 171. 168. 30.21 The Coin Flip Example I flip it 20 times and it lands on heads 16 times. My data is \\(x=16\\) heads out of \\(n=20\\) flips. My data generation model is \\(X \\sim \\mbox{Binomial}(20, p)\\). I form the statistic \\(\\hat{p} = 16/20\\) as an estimate of \\(p\\). Let’s do hypothesis testing and confidence interval construction on these data. 30.22 binom.test() &gt; str(binom.test) function (x, n, p = 0.5, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95) &gt; binom.test(x=16, n=20, p = 0.5) Exact binomial test data: 16 and 20 number of successes = 16, number of trials = 20, p-value = 0.01182 alternative hypothesis: true probability of success is not equal to 0.5 95 percent confidence interval: 0.563386 0.942666 sample estimates: probability of success 0.8 30.23 alternative = &quot;greater&quot; Tests \\(H_0: p \\leq 0.5\\) vs. \\(H_1: p &gt; 0.5\\). &gt; binom.test(x=16, n=20, p = 0.5, alternative=&quot;greater&quot;) Exact binomial test data: 16 and 20 number of successes = 16, number of trials = 20, p-value = 0.005909 alternative hypothesis: true probability of success is greater than 0.5 95 percent confidence interval: 0.5989719 1.0000000 sample estimates: probability of success 0.8 30.24 alternative = &quot;less&quot; Tests \\(H_0: p \\geq 0.5\\) vs. \\(H_1: p &lt; 0.5\\). &gt; binom.test(x=16, n=20, p = 0.5, alternative=&quot;less&quot;) Exact binomial test data: 16 and 20 number of successes = 16, number of trials = 20, p-value = 0.9987 alternative hypothesis: true probability of success is less than 0.5 95 percent confidence interval: 0.0000000 0.9286461 sample estimates: probability of success 0.8 30.25 prop.test() This is a “large \\(n\\)” inference method that is very similar to our \\(z\\)-statistic approach. &gt; str(prop.test) function (x, n, p = NULL, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95, correct = TRUE) &gt; prop.test(x=16, n=20, p=0.5) 1-sample proportions test with continuity correction data: 16 out of 20, null probability 0.5 X-squared = 6.05, df = 1, p-value = 0.01391 alternative hypothesis: true p is not equal to 0.5 95 percent confidence interval: 0.5573138 0.9338938 sample estimates: p 0.8 30.26 An Observation &gt; p &lt;- binom.test(x=16, n=20, p = 0.5)$p.value &gt; binom.test(x=16, n=20, p = 0.5, conf.level=(1-p)) Exact binomial test data: 16 and 20 number of successes = 16, number of trials = 20, p-value = 0.01182 alternative hypothesis: true probability of success is not equal to 0.5 98.81821 percent confidence interval: 0.5000000 0.9625097 sample estimates: probability of success 0.8 Exercise: Figure out what happened here. 30.27 Wording of Surveys The way a question is phrased can influence a person’s response. For example, Pew Research Center conducted a survey with the following question: “As you may know, by 2014 nearly all Americans will be required to have health insurance. [People who do not buy insurance will pay a penalty] while [People who cannot afford it will receive financial help from the government]. Do you approve or disapprove of this policy?” For each randomly sampled respondent, the statements in brackets were randomized: either they were kept in the order given above, or the two statements were reversed. Credit: This example comes from Open Intro Statistics, Exercise 6.10. 30.28 The Data The following table shows the results of this experiment. 2nd Statement Sample Size Approve Law Disapprove Law Other “people who cannot afford it will receive financial help from the government” 771 47 49 3 “people who do not buy it will pay a penalty” 732 34 63 3 30.29 Inference on the Difference Create and interpret a 90% confidence interval of the difference in approval. Also perform a hyppthesis test that the approval rates are equal. &gt; x &lt;- round(c(0.47*771, 0.34*732)) &gt; n &lt;- round(c(771*0.97, 732*0.97)) &gt; prop.test(x=x, n=n, conf.level=0.90) 2-sample test for equality of proportions with continuity correction data: x out of n X-squared = 26.023, df = 1, p-value = 3.374e-07 alternative hypothesis: two.sided 90 percent confidence interval: 0.08979649 0.17670950 sample estimates: prop 1 prop 2 0.4839572 0.3507042 30.30 90% Confidence Interval Let’s use MLE theory to construct of a two-sided 90% CI. &gt; p1.hat &lt;- 0.47 &gt; n1 &lt;- 771 &gt; p2.hat &lt;- 0.34 &gt; n2 &lt;- 732 &gt; stderr &lt;- sqrt(p1.hat*(1-p1.hat)/n1 + p2.hat*(1-p2.hat)/n2) &gt; &gt; # the 90% CI &gt; (p1.hat - p2.hat) + c(-1,1)*abs(qnorm(0.05))*stderr [1] 0.08872616 0.17127384 30.31 Poisson Data: poisson.test() &gt; str(poisson.test) function (x, T = 1, r = 1, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95) From the help: Arguments x number of events. A vector of length one or two. T time base for event count. A vector of length one or two. r hypothesized rate or rate ratio alternative indicates the alternative hypothesis and must be one of &quot;two.sided&quot;, &quot;greater&quot; or &quot;less&quot;. You can specify just the initial letter. conf.level confidence level for the returned confidence interval. 30.32 Example: RNA-Seq RNA-Seq gene expression was measured for p53 lung tissue in 12 healthy individuals and 14 individuals with lung cancer. The counts were given as follows. Healthy: 82 64 66 88 65 81 85 87 60 79 80 72 Cancer: 59 50 60 60 78 69 70 67 72 66 66 68 54 62 It is hypothesized that p53 expression is higher in healthy individuals. Test this hypothesis, and form a 99% CI. 30.33 \\(H_1: \\lambda_1 \\not= \\lambda_2\\) &gt; healthy &lt;- c(82, 64, 66, 88, 65, 81, 85, 87, 60, 79, 80, 72) &gt; cancer &lt;- c(59, 50, 60, 60, 78, 69, 70, 67, 72, 66, 66, 68, + 54, 62) &gt; poisson.test(x=c(sum(healthy), sum(cancer)), T=c(12,14), + conf.level=0.99) Comparison of Poisson rates data: c(sum(healthy), sum(cancer)) time base: c(12, 14) count1 = 909, expected count1 = 835.38, p-value = 0.0005739 alternative hypothesis: true rate ratio is not equal to 1 99 percent confidence interval: 1.041626 1.330051 sample estimates: rate ratio 1.177026 30.34 \\(H_1: \\lambda_1 &lt; \\lambda_2\\) &gt; poisson.test(x=c(sum(healthy), sum(cancer)), T=c(12,14), + alternative=&quot;less&quot;, conf.level=0.99) Comparison of Poisson rates data: c(sum(healthy), sum(cancer)) time base: c(12, 14) count1 = 909, expected count1 = 835.38, p-value = 0.9998 alternative hypothesis: true rate ratio is less than 1 99 percent confidence interval: 0.000000 1.314529 sample estimates: rate ratio 1.177026 30.35 \\(H_1: \\lambda_1 &gt; \\lambda_2\\) &gt; poisson.test(x=c(sum(healthy), sum(cancer)), T=c(12,14), + alternative=&quot;greater&quot;, conf.level=0.99) Comparison of Poisson rates data: c(sum(healthy), sum(cancer)) time base: c(12, 14) count1 = 909, expected count1 = 835.38, p-value = 0.0002881 alternative hypothesis: true rate ratio is greater than 1 99 percent confidence interval: 1.053921 Inf sample estimates: rate ratio 1.177026 30.36 Question Which analysis is the more informative and scientifically correct one, and why? "],
["likelihood-ratio-tests.html", "31 Likelihood Ratio Tests 31.1 General Set-up 31.2 Significance Regions 31.3 P-values 31.4 Example: Wald Test 31.5 Neyman-Pearson Lemma 31.6 Simple vs. Composite Hypotheses 31.7 General Hypothesis Tests 31.8 Composite \\(H_0\\) 31.9 Generalized LRT 31.10 Null Distribution of Gen. LRT 31.11 Example: Poisson 31.12 Example: Normal", " 31 Likelihood Ratio Tests 31.1 General Set-up Most hypothesis testing procedures can be formulated so that a test statistic, \\(S(\\boldsymbol{x})\\) is applied to the data \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_n)^T\\) so that: \\(S(\\boldsymbol{x}) \\geq 0\\) The larger \\(S(\\boldsymbol{x})\\) is, the more significant the test is (i.e., the more evidence against the null in favor of the alternative) The p-value is \\(p(\\boldsymbol{x}) = \\Pr(S(\\boldsymbol{X^*}) \\geq S(\\boldsymbol{x}))\\) where \\(S(\\boldsymbol{X^*})\\) is distributed according to the null distribution 31.2 Significance Regions A level \\(\\alpha\\) test is a signficance rule (i.e., a rule for calling a test statistically significant) that results in a false positive rate (i.e., Type I error rate) of \\(\\alpha\\). Under our set-up, significance regions take the form: \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: S(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\},\\] where \\(c_{1-\\alpha}\\) is the \\((1-\\alpha)\\) percentile of \\(S(\\boldsymbol{X}^*)\\) so that \\(\\Pr(S(\\boldsymbol{X}^*) \\geq c_{1-\\alpha}) = \\alpha\\). We restrict \\(0 &lt; \\alpha &lt; 1\\). Note that if \\(\\alpha&#39; \\leq \\alpha\\) then \\(\\Gamma_{\\alpha&#39;} \\subseteq \\Gamma_\\alpha\\). 31.3 P-values A p-value can be defined in terms of significance regions: \\[p(\\boldsymbol{x}) = \\min \\left\\{\\alpha: \\boldsymbol{x} \\in \\Gamma_\\alpha \\right\\}\\] 31.4 Example: Wald Test Consider the hypothesis test, \\(H_0: \\theta=\\theta_0\\) vs \\(H_1: \\theta \\not= \\theta_0\\). Let \\(\\hat{\\theta}_n(\\boldsymbol{x})\\) be the MLE of \\(\\theta\\). We have \\[S(\\boldsymbol{x}) = \\frac{\\left| \\hat{\\theta}_n(\\boldsymbol{x}) - \\theta_0 \\right|}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n(\\boldsymbol{x})\\right)},\\] \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: S(\\boldsymbol{x}) \\geq |z_{\\alpha/2}| \\right\\},\\] where \\(z_{\\alpha/2}\\) is the \\(\\alpha/2\\) percentile of the Normal\\((0,1)\\) distribution. 31.5 Neyman-Pearson Lemma Suppose we are testing \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta = \\theta_1\\) where in practice \\(\\theta_0\\) and \\(\\theta_1\\) are known, fixed quantities. The most powerful test has statistic and significance regions: \\[ S(\\boldsymbol{x}) = \\frac{f(\\boldsymbol{x}; \\theta_1)}{f(\\boldsymbol{x}; \\theta_0)} = \\frac{L(\\theta_1; \\boldsymbol{x})}{L(\\theta_0; \\boldsymbol{x})} \\] \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: S(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\},\\] where \\(c_{1-\\alpha}\\) is the \\((1-\\alpha)\\) percentile of \\(S(\\boldsymbol{X}^*)\\) so that \\(\\operatorname{Pr}_{\\theta_0}(S(\\boldsymbol{X}^*) \\geq c_{1-\\alpha}) = \\alpha\\). 31.6 Simple vs. Composite Hypotheses A simple hypothesis is defined in terms of a single value, e.g., \\(H_0: \\mu=0\\) \\(H_0: p = p_0\\) where \\(p_0\\) is a placehold for a known, fixed number in practice \\(H_1: \\lambda=5\\) A composite hypothesis is defined by multiple values, e.g., \\(H_0: \\mu \\leq 0\\) vs \\(H_1: \\mu &gt; 0\\) \\(H_0: p_1 = p_2\\), where \\(p_1\\) and \\(p_2\\) are two unknown parameters corresponding to two populations \\(H_1: \\mu \\not= 0\\) 31.7 General Hypothesis Tests Let \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_\\theta\\) where \\(\\theta \\in \\Theta\\). Let \\(\\Theta_0, \\Theta_1 \\subseteq \\Theta\\) so that \\(\\Theta_0 \\cap \\Theta_1 = \\varnothing\\) and \\(\\Theta_0 \\cup \\Theta_1 = \\Theta\\). The hypothesis test is: \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\) If \\(\\Theta_0\\) or \\(\\Theta_1\\) contain more than one value then the corresponding hypothesis is composite. 31.8 Composite \\(H_0\\) The significance regions indexed by their level \\(\\alpha\\) are determined so that: \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: S(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\},\\] where \\(c_{1-\\alpha}\\) is such that \\[\\max_{\\theta \\in \\Theta_0} \\Pr(S(\\boldsymbol{X^*}) \\geq c_{1-\\alpha}) = \\alpha.\\] In this case, \\[\\begin{align*} p(\\boldsymbol{x}) &amp; = \\min \\left\\{\\alpha: \\boldsymbol{x} \\in \\Gamma_\\alpha \\right\\} \\\\ &amp; = \\max_{\\theta \\in \\Theta_0} \\operatorname{Pr}_\\theta (S(\\boldsymbol{X}^*) \\geq S(\\boldsymbol{x})) \\end{align*}\\] 31.9 Generalized LRT Let \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_\\theta\\) where \\(\\theta \\in \\Theta\\) and we are testing \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\). The generalized LRT utilizes test statistic and significance regions: \\[\\lambda(\\boldsymbol{x}) = \\frac{\\max_{\\theta \\in \\Theta} L(\\theta; \\boldsymbol{x})}{\\max_{\\theta \\in \\Theta_0} L(\\theta; \\boldsymbol{x})} = \\frac{L\\left(\\hat{\\theta}; \\boldsymbol{x}\\right)}{L\\left(\\hat{\\theta}_0; \\boldsymbol{x}\\right)} \\] \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: \\lambda(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\}\\] 31.10 Null Distribution of Gen. LRT The null distribution of \\(\\lambda(\\boldsymbol{x})\\) under “certain regularity assumptions” can be shown to be such that, as \\(n \\rightarrow \\infty\\), \\[ 2 \\log \\lambda(\\boldsymbol{x}) \\stackrel{D}{\\longrightarrow} \\chi^2_v \\] where \\(v = \\operatorname{dim}(\\Theta) - \\operatorname{dim}(\\Theta_0)\\). The significance regions can be more easily written as \\(\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: 2 \\log \\lambda(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\}\\) where \\(c_{1-\\alpha}\\) is the \\(1-\\alpha\\) percentile of the \\(\\chi^2_v\\) distribution. 31.11 Example: Poisson Let \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} \\mbox{Poisson}(\\theta)\\) where \\(\\theta &gt; 0\\) and we are testing \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\not= \\theta_0\\). The unconstrained MLE is \\(\\hat{\\theta} = \\overline{x}\\). The generalized LRT statistic \\[ 2 \\log \\lambda(\\boldsymbol{x}) = 2 \\log \\frac{e^{-n \\hat{\\theta}} \\hat{\\theta}^{\\sum x_i} }{e^{-n \\theta_0} \\theta_0^{\\sum x_i} } = 2 n \\left[ (\\theta_0 - \\hat{\\theta}) - \\hat{\\theta} \\log(\\theta_0 / \\hat{\\theta}) \\right] \\] which has an asymptotic \\(\\chi^2_1\\) null distribution. 31.12 Example: Normal Let \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu, \\sigma^2)\\) and we are testing \\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\not= \\mu_0\\). The generalized LRT can be applied for multidimensional parameter spaces \\(\\boldsymbol{\\Theta}\\) as well. The statistic, which has asymptotic null distribution \\(\\chi^2_1\\), is \\[ 2 \\log \\lambda(\\boldsymbol{x}) = 2 \\log \\left( \\frac{\\hat{\\sigma}^2_0}{\\hat{\\sigma}^2} \\right)^{n/2} \\] where \\[ \\hat{\\sigma}^2_0 = \\frac{\\sum_{i=1}^n (x_i - \\mu_0)^2}{n}, \\quad \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}. \\] "],
["likelihood-function-2.html", "32 Likelihood Function 32.1 Same MLE, Different \\(L(\\theta | \\boldsymbol{x})\\) 32.2 Weighted Likelihood Estimate 32.3 Conditional Expected Value 32.4 Standard Errror", " 32 Likelihood Function 32.1 Same MLE, Different \\(L(\\theta | \\boldsymbol{x})\\) 32.2 Weighted Likelihood Estimate Instead of employing estimator \\(\\hat{\\theta}_{{\\rm MLE}} = \\operatorname{argmax}_\\theta L(\\theta ; \\boldsymbol{x})\\), consider instead an arbitrary weight function, \\(g(\\theta)\\). We could take a weighted average of the likelihood function, assuming all of the integrals below exist. \\[ \\tilde{\\theta} = \\frac{\\int \\theta g(\\theta) L(\\theta ; \\boldsymbol{x}) d\\theta}{\\int g(\\theta) L(\\theta ; \\boldsymbol{x}) d\\theta} \\] 32.3 Conditional Expected Value If we set \\[ h(\\theta | \\boldsymbol{x}) = \\frac{g(\\theta) L(\\theta ; \\boldsymbol{x})}{\\int g(\\theta^*) L(\\theta^* ; \\boldsymbol{x}) d\\theta^*} \\] then \\(h(\\theta | \\boldsymbol{x})\\) is a probability density function and \\[ \\tilde{\\theta} = {\\operatorname{E}}_{h(\\theta | \\boldsymbol{x})}[\\theta]. \\] 32.4 Standard Errror Consider the model, \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{\\theta}\\). Since \\(\\tilde{\\theta} = {\\operatorname{E}}_{h(\\theta | \\boldsymbol{x})}[\\theta]\\) is a function of the data \\(\\boldsymbol{x}\\), it follows that in most circumstances it should be possible to obtain an approximation to its standard error, \\(\\sqrt{{\\operatorname{Var}}(\\tilde{\\theta})}\\) and an estimate of the standard error. This allows for frequentist inference of estimates based on a weighted integral of the likelihood function. "],
["bayesian-inference.html", "33 Bayesian Inference 33.1 Frequentist Probability 33.2 Bayesian Probability 33.3 The Framework 33.4 An Example 33.5 Calculations 33.6 In Practice 33.7 Goal 33.8 Advantages 33.9 Computation", " 33 Bayesian Inference 33.1 Frequentist Probability The inference framework we have covered so far uses a frequentist intepretation of probability. We made statements such as, “If we repeat this study over and over, the long run frequency is such that…” 33.2 Bayesian Probability Traditional Bayesian inference is based on a different interpretation of probability, that probability is a measure of subjective belief. We will call this “subjective Bayesian statistics.” 33.3 The Framework A prior probability distribution is introduced for an unknown parameter, which is a probability distribution on the unknown parameter that captures one’s subjective belief about its possible values. The posterior probability distributuon of the parameter is then calculated using Bayes theorem once data are observed. Analogs of confidence intervals and hypothesis tests can then be obtained through the posterior distribution. 33.4 An Example Prior: \\(P \\sim \\mbox{Uniform}(0,1)\\) Data generating distribution: \\(X|P=p \\sim \\mbox{Binomial}(n,p)\\) Posterior pdf (via Bayes Theorem): \\[\\begin{align*} f(p | X=x) &amp; = \\frac{\\Pr(X=x | P=p) f(p)}{\\Pr(X=x)} \\\\ &amp; = \\frac{\\Pr(X=x | P=p) f(p)}{\\int \\Pr(X=x | P=p^*) f(p^*) dp^*} \\end{align*}\\] 33.5 Calculations In the previous example, it is possible to analytically calculate the posterior distribution. (In the example, it is a Beta distribution with parameters that involve \\(x\\).) However, this is often impossible. Bayesian inference often involves complicated and intensive calculations to numerically approximate the posterior probability distribution. 33.6 In Practice Although the Bayesian inference framework has its roots in the subjective view of probability, in modern times this philosophical aspect is often ignored or unimportant. When subjectivism is ignored, is this really Bayesian statistics, or is it frequentist statistics that includes a probability model on the unknown parameter(s) that employes Bayes Theorem? Bayesian inference is often used because it provides a flexible and sometimes superior model for real world problems. But the interpretation and evaluation are often tacitly frequentist. There are very few pure subjective Bayesians working in the natural sciences or in technology industries. 33.7 Goal Suppose we model \\((X_1, X_2, \\ldots, X_n) | \\theta \\ {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\ F_{\\theta}\\) with prior distribution \\(\\theta \\sim F_{\\tau}\\) where it should be noted that \\(\\theta\\) also depends on (possibly unknown or subjective) parameter(s) \\(\\tau\\). The ultimate goal is to determine the posterior distribution of \\(\\theta | \\boldsymbol{X}\\) through Bayes theorem: \\[ f(\\theta | \\boldsymbol{X}) = \\frac{f(\\boldsymbol{X} | \\theta) f(\\theta)}{f(\\boldsymbol{X})} = \\frac{f(\\boldsymbol{X} | \\theta) f(\\theta)}{\\int f(\\boldsymbol{X} | \\theta^*) f(\\theta^*) d\\theta^*}. \\] If there is a true fixed value of \\(\\theta\\), then a well-behaved model should be so that \\(f(\\theta | \\boldsymbol{X})\\) concentrates around this fixed value as \\(n \\rightarrow \\infty\\). 33.8 Advantages Statements on measures of uncertainty and inference are easier to make Often superior numerical stability to the estimates Data across studies or multiple samples easier to combine (e.g., how to combine frequentist p-values?) High-dimensional inference works especially well in a Bayesian framework 33.9 Computation Bayesian inference can be particularly computationally intensive. The challenge is usually in calculating the denominator of the right hand side of Bayes thereom, \\(f(\\boldsymbol{X})\\): \\[ f(\\theta | \\boldsymbol{X}) = \\frac{f(\\boldsymbol{X} | \\theta) f(\\theta)}{f(\\boldsymbol{X})} \\] Markov chain Monte Carlo methods and variational inference methods are particularly popular for dealing with the numerical challenges of obtain good estimates of the posterior distribution. "],
["estimation.html", "34 Estimation 34.1 Assumptions 34.2 Posterior Distribution 34.3 Posterior Expectation 34.4 Posterior Interval 34.5 Maximum A Posteriori Probability 34.6 Loss Functions 34.7 Bayes Risk 34.8 Bayes Estimators", " 34 Estimation 34.1 Assumptions We will assume that \\((X_1, X_2, \\ldots, X_n) | \\theta {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{\\theta}\\) with prior distribution \\(\\theta \\sim F_{\\tau}\\) unless stated otherwise. Shorthand for the former is \\(\\boldsymbol{X} | \\theta {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{\\theta}\\). We will write the pdf or pmf of \\(X\\) as \\(f(x | \\theta)\\) as opposed to \\(f(x ; \\theta)\\) because in the Bayesian framework this actually represents conditional probability. We will write the pdf or pmf of \\(\\theta\\) as \\(f(\\theta)\\) or \\(f(\\theta ; \\tau)\\) or \\(f(\\theta | \\tau)\\). Always remember that prior distributions require paramater values, even if we don’t explicitly write them. 34.2 Posterior Distribution The posterior distribution of \\(\\theta | \\boldsymbol{X}\\) is obtained through Bayes theorem: \\[\\begin{align*} f(\\theta | \\boldsymbol{x}) &amp; = \\frac{f(\\boldsymbol{x} | \\theta) f(\\theta)}{f(\\boldsymbol{x})} = \\frac{f(\\boldsymbol{x} | \\theta) f(\\theta)}{\\int f(\\boldsymbol{x} | \\theta^*) f(\\theta^*) d\\theta^*} \\\\ &amp; \\propto L(\\theta ; \\boldsymbol{x}) f(\\theta) \\end{align*}\\] 34.3 Posterior Expectation A very common point estimate of \\(\\theta\\) in Bayesian inference is the posterior expected value: \\[\\begin{align*} \\operatorname{E}[\\theta | \\boldsymbol{x}] &amp; = \\int \\theta f(\\theta | \\boldsymbol{x}) d\\theta \\\\ &amp; = \\frac{\\int \\theta L(\\theta ; \\boldsymbol{x}) f(\\theta) d\\theta}{\\int L(\\theta ; \\boldsymbol{x}) f(\\theta) d\\theta} \\end{align*}\\] 34.4 Posterior Interval The Bayesian analog of the frequentist confidence interval is the \\(1-\\alpha\\) posterior interval, where \\(C_{\\ell}\\) and \\(C_{u}\\) are determined so that: \\[ 1-\\alpha = \\Pr(C_\\ell \\leq \\theta \\leq C_u | \\boldsymbol{x}) \\] 34.5 Maximum A Posteriori Probability The maximum a posteriori probability (MAP) is the value (or values) of \\(\\theta\\) that maximize the posterior pdf or pmf: \\[\\begin{align*} \\hat{\\theta}_{\\text{MAP}} &amp; = \\operatorname{argmax}_\\theta \\Pr(\\theta | \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_\\theta L(\\theta ; \\boldsymbol{x}) f(\\theta) \\end{align*}\\] This is a frequentist-esque use of the Bayesian framework. 34.6 Loss Functions Let \\(\\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right)\\) be a loss function for a given estimator \\(\\tilde{\\theta}\\). Examples are \\[ \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) = \\left(\\theta - \\tilde{\\theta}\\right)^2 \\mbox{ or } \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) = \\left|\\theta - \\tilde{\\theta}\\right|. \\] Note that, where the expected value is over \\(f(\\boldsymbol{x}; \\theta)\\): \\[\\begin{align*} \\operatorname{E}\\left[\\left(\\theta - \\tilde{\\theta}\\right)^2\\right] &amp; = \\left(\\operatorname{E}\\left[\\tilde{\\theta}\\right] - \\theta\\right)^2 + \\operatorname{Var}\\left(\\tilde{\\theta}\\right) \\\\ &amp; = \\mbox{bias}^2 + \\mbox{variance} \\end{align*}\\] 34.7 Bayes Risk The Bayes risk, \\(R\\left(\\theta, \\tilde{\\theta}\\right)\\), is the expected loss with respect to the posterior: \\[ {\\operatorname{E}}\\left[ \\left. \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) \\right| \\boldsymbol{x} \\right] = \\int \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) f(\\theta | \\boldsymbol{x}) d\\theta \\] 34.8 Bayes Estimators The Bayes estimator minimizes the Bayes risk. The posterior expectation \\({\\operatorname{E}}\\left[ \\left. \\theta \\right| \\boldsymbol{x} \\right]\\) minimizes the Bayes risk of \\(\\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) = \\left(\\theta - \\tilde{\\theta}\\right)^2\\). The median of \\(f(\\theta | \\boldsymbol{x})\\), calculated by \\(F^{-1}_{\\theta | \\boldsymbol{x}}(1/2)\\), minimizes the Bayes risk of \\(\\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) = \\left|\\theta - \\tilde{\\theta}\\right|\\). "],
["classification.html", "35 Classification 35.1 Assumptions 35.2 Prior Probability on H 35.3 Posterior Probability 35.4 Loss Function 35.5 Bayes Risk 35.6 Bayes Rule", " 35 Classification 35.1 Assumptions Let \\((X_1, X_2, \\ldots, X_n) | \\theta {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_\\theta\\) where \\(\\theta \\in \\Theta\\) and \\(\\theta \\sim F_{\\tau}\\). Let \\(\\Theta_0, \\Theta_1 \\subseteq \\Theta\\) so that \\(\\Theta_0 \\cap \\Theta_1 = \\varnothing\\) and \\(\\Theta_0 \\cup \\Theta_1 = \\Theta\\). Given observed data \\(\\boldsymbol{x}\\), we wish to classify whether \\(\\theta \\in \\Theta_0\\) or \\(\\theta \\in \\Theta_1\\). This is the Bayesian analog of hypothesis testing. 35.2 Prior Probability on H Let \\(H\\) be a rv such that \\(H=0\\) when \\(\\theta \\in \\Theta_0\\) and \\(H=1\\) when \\(\\theta \\in \\Theta_1\\). From the prior distribution on \\(\\theta\\), we can calculate \\[ \\Pr(H=0) = \\int_{\\theta \\in \\Theta_0} f(\\theta) d\\theta \\] and \\(\\Pr(H=1) = 1-\\Pr(H=0)\\). 35.3 Posterior Probability Using Bayes theorem, we can also calculate \\[\\begin{align*} \\Pr(H=0 | \\boldsymbol{x}) &amp; = \\frac{f(\\boldsymbol{x} | H=0) \\Pr(H=0)}{f(\\boldsymbol{x})} \\\\ &amp; = \\frac{\\int_{\\theta \\in \\Theta_0} f(\\boldsymbol{x} | \\theta) f(\\theta) d\\theta}{\\int_{\\theta \\in \\Theta} f(\\boldsymbol{x} | \\theta) f(\\theta) d\\theta} \\end{align*}\\] where note that \\(\\Pr(H=1 | \\boldsymbol{x}) = 1-\\Pr(H=0 | \\boldsymbol{x})\\). 35.4 Loss Function Let \\(\\mathcal{L}\\left(\\tilde{H}, H\\right)\\) be such that \\[\\begin{align*} \\mathcal{L}\\left(\\tilde{H}=1, H=0 \\right) &amp; = c_{I}\\\\ \\mathcal{L}\\left(\\tilde{H}=0, H=1 \\right) &amp; = c_{II} \\end{align*}\\] for some \\(c_{I}, c_{II} &gt; 0\\). 35.5 Bayes Risk The Bayes risk, \\(R\\left(\\tilde{H}, H\\right)\\), is \\[\\begin{align*} \\operatorname{E}\\left[ \\left. \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) \\right| \\boldsymbol{x} \\right] &amp; = c_{I} \\Pr(\\tilde{H}=1, H=0) + c_{II} \\Pr(\\tilde{H}=0, H=1) \\\\ &amp; = c_{I} \\Pr(\\tilde{H}=1 | H=0) \\Pr(H=0) \\\\ &amp; \\quad\\quad + c_{II} \\Pr(\\tilde{H}=0 | H=1) \\Pr(H=1) \\end{align*}\\] Notice how this balances what frequentists call Type I error and Type II error. 35.6 Bayes Rule The estimate \\(\\tilde{H}\\) that minimizes \\(R\\left(\\tilde{H}, H\\right)\\) is \\[\\tilde{H}=1 \\mbox{ when } \\Pr(H=1 | \\boldsymbol{x}) \\geq \\frac{c_{I}}{c_{I} + c_{II}}\\] and \\(\\tilde{H}=0\\) otherwise. "],
["priors.html", "36 Priors 36.1 Conjugate Priors 36.2 Example: Beta-Bernoulli 36.3 Example: Normal-Normal 36.4 Example: Dirichlet-Multinomial 36.5 Example: Gamma-Poisson 36.6 Jeffreys Prior 36.7 Examples: Jeffreys Priors 36.8 Improper Prior", " 36 Priors 36.1 Conjugate Priors A conjugate prior is a prior distribution for a data generating distribution so that the posterior distribution is of the same type as the prior. Conjugate priors are useful for obtaining stratightforward calculations of the posterior. There is a systematic method for calculating conjugate priors for exponential family distributions. 36.2 Example: Beta-Bernoulli Suppose \\(\\boldsymbol{X} | \\mu {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Bernoulli}(p)\\) and suppose that \\(p \\sim \\mbox{Beta}(\\alpha, \\beta)\\). \\[\\begin{align*} f(p | \\boldsymbol{x}) &amp; \\propto L(p ; \\boldsymbol{x}) f(p) \\\\ &amp; = p^{\\sum x_i} (1-p)^{\\sum (1-x_i)} p^{\\alpha - 1} (1-p)^{\\beta-1} \\\\ &amp; = p^{\\alpha - 1 + \\sum x_i} (1-p)^{\\beta - 1 + \\sum (1-x_i)} \\\\ &amp; \\propto \\mbox{Beta}(\\alpha + \\sum x_i, \\beta + \\sum (1-x_i)) \\end{align*}\\] Therefore, \\[ {\\operatorname{E}}[p | \\boldsymbol{x}] = \\frac{\\alpha + \\sum x_i}{\\alpha + \\beta + n}. \\] 36.3 Example: Normal-Normal Suppose \\(\\boldsymbol{X} | \\mu {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known, and suppose that \\(\\mu \\sim \\mbox{Normal}(a, b^2)\\). Then it can be shown that \\(\\mu | \\boldsymbol{x} \\sim \\mbox{Normal}({\\operatorname{E}}[\\mu | \\boldsymbol{x}], {\\operatorname{Var}}(\\mu | \\boldsymbol{x}))\\) where \\[ {\\operatorname{E}}[\\mu | \\boldsymbol{x}] = \\frac{b^2}{\\frac{\\sigma^2}{n} + b^2} \\overline{x} + \\frac{\\frac{\\sigma^2}{n}}{\\frac{\\sigma^2}{n} + b^2} a \\] \\[ {\\operatorname{Var}}(\\mu | \\boldsymbol{x}) = \\frac{b^2 \\frac{\\sigma^2}{n}}{\\frac{\\sigma^2}{n} + b^2} \\] 36.4 Example: Dirichlet-Multinomial This is a problem on Homework 3! 36.5 Example: Gamma-Poisson This is a problem on Homework 3! 36.6 Jeffreys Prior If we do inference based on prior \\(\\theta \\sim F_{\\tau}\\) to obtain \\(f(\\theta | \\boldsymbol{x}) \\propto L(\\theta; \\boldsymbol{x}) f(\\theta)\\), it follows that this inference may not be invariant to transformations of \\(\\theta\\), such as \\(\\eta = g(\\theta)\\). If we utilize a Jeffreys prior, which means it is such that \\[f(\\theta) \\propto \\sqrt{I(\\theta)}\\] then the prior will be invariant to transformations of \\(\\theta\\). We would want to show that \\(f(\\theta) \\propto \\sqrt{I(\\theta)}\\) implies \\(f(\\eta) \\propto \\sqrt{I(\\eta)}\\). 36.7 Examples: Jeffreys Priors Normal\\((\\mu, \\sigma^2)\\), \\(\\sigma^2\\) known: \\(f(\\mu) \\propto 1\\) Normal\\((\\mu, \\sigma^2)\\), \\(\\mu\\) known: \\(f(\\sigma) \\propto \\frac{1}{\\sigma}\\) Poisson\\((\\lambda)\\): \\(f(\\lambda) \\propto \\frac{1}{\\sqrt{\\lambda}}\\) Bernoulli\\((p)\\): \\(f(p) \\propto \\frac{1}{\\sqrt{p(1-p)}}\\) 36.8 Improper Prior An improper prior is a prior such that \\(\\int f(\\theta) d\\theta = \\infty\\). Nevertheless, sometimes it still may be the case that \\(f(\\theta | \\boldsymbol{x}) \\propto L(\\theta; \\boldsymbol{x}) f(\\theta)\\) yields a probability distribution. Take for example the case where \\(\\boldsymbol{X} | \\mu {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known, and suppose that \\(f(\\mu) \\propto 1\\). Then \\(\\int f(\\theta) d\\theta = \\infty\\), but \\[ f(\\theta | \\boldsymbol{x}) \\propto L(\\theta; \\boldsymbol{x}) f(\\theta) \\sim \\mbox{Normal}\\left(\\overline{x}, \\sigma^2/n\\right)\\] which is a proper probability distribution. "],
["theory.html", "37 Theory 37.1 de Finetti’s Theorem 37.2 Admissibility", " 37 Theory 37.1 de Finetti’s Theorem Let \\(X_1, X_2, \\ldots\\) be an infinite exchangeable sequence of Bernoulli rv’s. There exists a random variable \\(P \\in [0, 1]\\) such that: \\(X_1|P, X_2|P, \\ldots\\) are conditionally independent \\(X_1, X_2, \\ldots | P=p \\stackrel{{\\rm iid}}{\\sim} \\mbox{Bernoulli}(p)\\) This theorem is often used to justify the assumption of exchangeability, which is weaker than iid, with a prior distribution on the parameter(s). 37.2 Admissibility An estimator \\(\\tilde{\\theta}\\) is admissible with respect to risk function \\(R(\\cdot, \\theta)\\) if there is exists no other estimator \\(\\hat{\\theta}\\) such that \\(R(\\hat{\\theta}, \\theta) &lt; R(\\tilde{\\theta}, \\theta)\\) for all \\(\\theta \\in \\Theta\\). There’s a theoretical result that says all admissible estimators are Bayes estimates. "],
["empirical-bayes.html", "38 Empirical Bayes 38.1 Rationale 38.2 Approach 38.3 Example: Normal", " 38 Empirical Bayes 38.1 Rationale Under the scenario that \\(\\boldsymbol{X} | \\theta {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{\\theta}\\) with prior distribution \\(\\theta \\sim F_{\\tau}\\), we have to determine values for \\(\\tau\\). The empirical Bayes approach uses the observed data to estimate the prior parameter(s), \\(\\tau\\). This is especially useful for high-dimensional data when many parameters are simultaneously drawn from a prior with multiple observations drawn per parameter realization. 38.2 Approach The usual approach is to first integrate out the parameter to obtain \\[ f(\\boldsymbol{x} ; \\tau) = \\int f(\\boldsymbol{x} | \\theta) f(\\theta ; \\tau) d\\theta. \\] An estimation method (such as MLE) is then applied to estimate \\(\\tau\\). Then inference proceeds as usual under the assumption that \\(\\theta \\sim f(\\theta ; \\hat{\\tau})\\). 38.3 Example: Normal Suppose that \\(X_i | \\mu_i \\sim \\mbox{Normal}(\\mu_i, 1)\\) for \\(i=1, 2, \\ldots, n\\) where these rv’s are independent. Also suppose that \\(\\mu_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(a, b^2)\\). \\[ f(x_i ; a, b) = \\int f(x_i | \\mu_i) f(\\mu_i; a, b) d\\mu_i \\sim \\mbox{Normal}(a, 1+b^2). \\] \\[ \\implies \\hat{a} = \\overline{x}, \\ 1+\\hat{b}^2 = \\frac{\\sum_{k=1}^n (x_k - \\overline{x})^2}{n} \\] \\[\\begin{align*} \\operatorname{E}[\\mu_i | x_i] &amp; = \\frac{1}{1+b^2}a + \\frac{b^2}{1+b^2}x_i \\implies \\\\ &amp; \\\\ \\hat{\\operatorname{E}}[\\mu_i | x_i] &amp; = \\frac{1}{1+\\hat{b}^2}\\hat{a} + \\frac{\\hat{b}^2}{1+\\hat{b}^2}x_i \\\\ &amp; = \\frac{n}{\\sum_{k=1}^n (x_k - \\overline{x})^2} \\overline{x} + \\left(1-\\frac{n}{\\sum_{k=1}^n (x_k - \\overline{x})^2}\\right) x_i \\end{align*}\\] "],
["why-numerical-methods-for-likelihood.html", "39 Why Numerical Methods for Likelihood 39.1 Challenges 39.2 Approaches", " 39 Why Numerical Methods for Likelihood 39.1 Challenges Frequentist model: \\[X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}\\] Bayesian model: \\[X_1, X_2, \\ldots, X_n | {\\boldsymbol{\\theta}}{\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}} \\mbox{ and } {\\boldsymbol{\\theta}}\\sim F_{\\boldsymbol{\\tau}}\\] Sometimes it’s not possible to find formulas for \\(\\hat{{\\boldsymbol{\\theta}}}_{\\text{MLE}}\\), \\(\\hat{{\\boldsymbol{\\theta}}}_{\\text{MAP}}\\), \\({\\operatorname{E}}[{\\boldsymbol{\\theta}}| {\\boldsymbol{x}}]\\), or \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\). We have to use numerical methods instead. 39.2 Approaches We will discuss the following numerical approaches to likelihood based inference: Expectation-maximization (EM) algorithm Variational inference Markov chain Monte Carlo (MCMC) Metropolis sampling Metropolis-Hastings sampling Gibbs sampling "],
["latent-variable-models.html", "40 Latent Variable Models 40.1 Definition 40.2 Empirical Bayes Revisited 40.3 Normal Mixture Model 40.4 Bernoulli Mixture Model", " 40 Latent Variable Models 40.1 Definition Latent variables (or hidden variables) are random variables that are present in the model, but unobserved. We will denote latent variables by \\(Z\\), and we will assume \\[(X_1, Z_1), (X_2, Z_2), \\ldots, (X_n, Z_n) {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}.\\] A realized value of \\(Z\\) is \\(z\\), \\({\\boldsymbol{Z}}= (Z_1, Z_2, \\ldots, Z_n)^T\\), etc. The EM algorithm and variational inference involve latent variables. Bayesian models are a special case of latent variable models: the unobserved random parameters are latent variables. 40.2 Empirical Bayes Revisited In the earlier EB example, we supposed that \\(X_i | \\mu_i \\sim \\mbox{Normal}(\\mu_i, 1)\\) for \\(i=1, 2, \\ldots, n\\) where these rv’s are independent, and also that \\(\\mu_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(a, b^2)\\). The unobserved parameters \\(\\mu_1, \\mu_2, \\ldots, \\mu_n\\) are latent variables. In this case, \\({\\boldsymbol{\\theta}}= (a, b^2)\\). 40.3 Normal Mixture Model Suppose \\({X_1, X_2, \\ldots, X_n}{\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}\\) where \\({\\boldsymbol{\\theta}}= (\\pi_1, \\ldots, \\pi_K, \\mu_1, \\ldots, \\mu_K, \\sigma^2_1, \\ldots, \\sigma^2_K)\\) with pdf \\[ f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n \\sum_{k=1}^K \\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}} \\exp \\left\\{ -\\frac{(x_i - \\mu_k)^2}{2 \\sigma^2_k} \\right\\}. \\] The MLEs of the unknown paramaters cannot be found analytically. This is a mixture common model to work with in applications, so we need to be able to estimate the parameters. There is a latent variable model that produces the same maerginal distribution and likelihood function. Let \\({\\boldsymbol{Z}}_1, {\\boldsymbol{Z}}_2, \\ldots, {\\boldsymbol{Z}}_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Multinomial}_K(1, {\\boldsymbol{\\pi}})\\) where \\({\\boldsymbol{\\pi}}= (\\pi_1, \\ldots, \\pi_K)\\). Note that \\(Z_{ik} \\in \\{0, 1\\}\\) and \\(\\sum_{k=1}^K Z_{ik} = 1\\). Let \\([X_i | Z_{ik} = 1] \\sim \\mbox{Normal}(\\mu_k, \\sigma^2_k)\\), where \\(\\{X_i | {\\boldsymbol{Z}}_i\\}_{i=1}^{n}\\) are jointly independent. The joint pdf is \\[ f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n \\prod_{k=1}^K \\left[ \\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}} \\exp \\left\\{ -\\frac{(x_i - \\mu_k)^2}{2 \\sigma^2_k} \\right\\} \\right]^{z_{ik}}. \\] Note that \\[ f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n f(x_i, {\\boldsymbol{z}}_i; {\\boldsymbol{\\theta}}). \\] It can be verified that \\(f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\) is the marginal distribution of this latent variable model: \\[ f(x_i ; {\\boldsymbol{\\theta}}) = \\sum_{{\\boldsymbol{z}}_i} f(x_i, {\\boldsymbol{z}}_i; {\\boldsymbol{\\theta}}) = \\sum_{k=1}^K \\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}} \\exp \\left\\{ -\\frac{(x_i - \\mu_k)^2}{2 \\sigma^2_k} \\right\\}. \\] 40.4 Bernoulli Mixture Model Suppose \\({X_1, X_2, \\ldots, X_n}{\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}\\) where \\({\\boldsymbol{\\theta}}= (\\pi_1, \\ldots, \\pi_K, p_1, \\ldots, p_K)\\) with pdf \\[ f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n \\sum_{k=1}^K \\pi_k p_k^{x_i} (1-p_k)^{1-x_i}. \\] As in the Normal mixture model, the MLEs of the unknown paramaters cannot be found analytically. As before, there is a latent variable model that produces the same maerginal distribution and likelihood function. Let \\({\\boldsymbol{Z}}_1, {\\boldsymbol{Z}}_2, \\ldots, {\\boldsymbol{Z}}_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Multinomial}_K(1, {\\boldsymbol{\\pi}})\\) where \\({\\boldsymbol{\\pi}}= (\\pi_1, \\ldots, \\pi_K)\\). Note that \\(Z_{ik} \\in \\{0, 1\\}\\) and \\(\\sum_{k=1}^K Z_{ik} = 1\\). Let \\([X_i | Z_{ik} = 1] \\sim \\mbox{Bernoulli}(p_k)\\), where \\(\\{X_i | {\\boldsymbol{Z}}_i\\}_{i=1}^{n}\\) are jointly independent. The joint pdf is \\[ f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n \\prod_{k=1}^K \\left[ p_k^{x_i} (1-p_k)^{1-x_i} \\right]^{z_{ik}}. \\] "],
["em-algorithm.html", "41 EM Algorithm 41.1 Rationale 41.2 Requirement 41.3 The Algorithm 41.4 \\(Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\) 41.5 EM for MAP", " 41 EM Algorithm 41.1 Rationale For any likelihood function, \\(L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) = f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\), there is an abundance of optimization methods that can be used to find the MLE or MAP. However: Optimization methods can be messy to implement There may be probabilistic structure that we can use to simplify the optimization process and also provide theoretical guarantees on its convergence Optimization isn’t necessarily the only goal, but one may also be interested in point estimates of the latent variable values 41.2 Requirement The expectation-maximization (EM) algorithm allows us to calculate MLEs and MAPs when certain geometric properties are satisfied in the probabilistic model. In order for the EM algorithm to be a practical approach, then we should have a latent variable model \\(f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}})\\) that is used to do inference on \\(f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\) or \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\). Note: Sometimes \\(({\\boldsymbol{x}}, {\\boldsymbol{z}})\\) is called the complete data and \\({\\boldsymbol{x}}\\) is called the observed data when we are using the EM as a method for dealing with missing data. 41.3 The Algorithm Choose initial value \\({\\boldsymbol{\\theta}}^{(0)}\\) Calculate \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}}, {\\boldsymbol{\\theta}}^{(t)})\\) Calculate \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right]\\] Set \\[{\\boldsymbol{\\theta}}^{(t+1)} = {\\text{argmax}}_{{\\boldsymbol{\\theta}}} Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\] Iterate until convergence and set \\(\\widehat{{\\boldsymbol{\\theta}}} = {\\boldsymbol{\\theta}}^{(\\infty)}\\) 41.4 \\(Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\) Continuous \\({\\boldsymbol{Z}}\\): \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = \\int \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}) d{\\boldsymbol{z}}\\] Discrete \\({\\boldsymbol{Z}}\\): \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = \\sum_{{\\boldsymbol{z}}} \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)})\\] 41.5 EM for MAP If we wish to calculate the MAP we replace \\(Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\) with \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right] + \\log f({\\boldsymbol{\\theta}})\\] where \\(f({\\boldsymbol{\\theta}})\\) is the prior distribution on \\({\\boldsymbol{\\theta}}\\). "],
["em-examples.html", "42 EM Examples 42.1 Normal Mixture Model 42.2 E-Step 42.3 M-Step 42.4 Caveat 42.5 Yeast Gene Expression 42.6 Initialize Values 42.7 Run EM Algorithm 42.8 Fitted Mixture Distribution 42.9 Bernoulli Mixture Model 42.10 Other Applications of EM", " 42 EM Examples 42.1 Normal Mixture Model Returning to the Normal mixture model introduced earlier, we first calculate \\[ \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) = \\sum_{i=1}^n \\sum_{k=1}^K z_{ik} \\log \\pi_k + z_{ik} \\log \\phi(x_i; \\mu_k, \\sigma^2_k) \\] where \\[ \\phi(x_i; \\mu_k, \\sigma^2_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}} \\exp \\left\\{ -\\frac{(x_i - \\mu_k)^2}{2 \\sigma^2_k} \\right\\}. \\] In caculating \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right]\\] we only need to know \\({\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}[Z_{ik} | {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}]\\), which turns out to be \\[ {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}[Z_{ik} | {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}] = \\frac{\\pi_k \\phi(x_i; \\mu_k, \\sigma^2_k)}{\\sum_{j=1}^K \\pi_j \\phi(x_i; \\mu_j, \\sigma^2_j)}. \\] Note that we take \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right]\\] so the parameter in \\(\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}})\\) is a free \\({\\boldsymbol{\\theta}}\\), but the paramaters used to take the conditional expectation of \\({\\boldsymbol{Z}}\\) are fixed at \\({\\boldsymbol{\\theta}}^{(t)}\\). Let’s define \\[ \\hat{z}_{ik}^{(t)} = {\\operatorname{E}}\\left[z_{ik} | {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}\\right] = \\frac{\\pi^{(t)}_k \\phi(x_i; \\mu^{(t)}_k, \\sigma^{2, (t)}_k)}{\\sum_{j=1}^K \\pi^{(t)}_j \\phi(x_i; \\mu^{(t)}_j, \\sigma^{2, (t)}_j)}. \\] 42.2 E-Step We calculate \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right]\\] \\[ = \\sum_{i=1}^n \\sum_{k=1}^K \\hat{z}_{ik}^{(t)} \\log \\pi_k + \\hat{z}_{ik}^{(t)} \\log \\phi(x_i; \\mu_k, \\sigma^2_k)\\] At this point the parameters making up \\(\\hat{z}_{ik}^{(t)}\\) are fixed at \\({\\boldsymbol{\\theta}}^{(t)}\\). 42.3 M-Step We now caculate \\({\\boldsymbol{\\theta}}^{(t+1)} = {\\text{argmax}}_{{\\boldsymbol{\\theta}}} Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}\\), which yields: \\[ \\pi_k^{(t+1)} = \\frac{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)}}{n} \\] \\[ \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)} x_i}{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)}} \\] \\[ \\sigma_k^{2, (t+1)} = \\frac{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)} \\left(x_i - \\mu_k^{(t+1)} \\right)^2}{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)}} \\] Note: You need to use a Lagrange multiplier to obtain \\(\\{\\pi_k^{(t+1)}\\}_{k=1}^{K}\\). 42.4 Caveat If we assign one and only one data point to mixture component \\(k\\), meaning \\(\\mu_k^{(t)} = x_i\\) and \\(\\hat{z}_{ik}^{(t)}=1\\) for some \\(k\\) and \\(i\\), then as \\(\\sigma^{2, (t)}_k \\rightarrow 0\\), the likelihood goes to \\(\\infty\\). Therefore, when implementing the EM algorithm for this particular Normal mixture model, we have to be careful to bound all \\(\\sigma^{2, (t)}_k\\) away from zero and avoid this scenario. 42.5 Yeast Gene Expression Measured ratios of the nuclear to cytoplasmic fluorescence for a protein-GFP construct that is hypothesized as being nuclear in mitotic cells and largely cytoplasmic in mating cells. 42.6 Initialize Values &gt; set.seed(508) &gt; B &lt;- 100 &gt; p &lt;- rep(0,B) &gt; mu1 &lt;- rep(0,B) &gt; mu2 &lt;- rep(0,B) &gt; s1 &lt;- rep(0,B) &gt; s2 &lt;- rep(0,B) &gt; p[1] &lt;- runif(1, min=0.1, max=0.9) &gt; mu.start &lt;- sample(x, size=2, replace=FALSE) &gt; mu1[1] &lt;- min(mu.start) &gt; mu2[1] &lt;- max(mu.start) &gt; s1[1] &lt;- var(sort(x)[1:60]) &gt; s2[1] &lt;- var(sort(x)[61:120]) &gt; z &lt;- rep(0,120) 42.7 Run EM Algorithm &gt; for(i in 2:B) { + z &lt;- (p[i-1]*dnorm(x, mean=mu2[i-1], sd=sqrt(s2[i-1])))/ + (p[i-1]*dnorm(x, mean=mu2[i-1], sd=sqrt(s2[i-1])) + + (1-p[i-1])*dnorm(x, mean=mu1[i-1], sd=sqrt(s1[i-1]))) + mu1[i] &lt;- sum((1-z)*x)/sum(1-z) + mu2[i] &lt;- sum(z*x)/sum(z) + s1[i] &lt;- sum((1-z)*(x-mu1[i])^2)/sum(1-z) + s2[i] &lt;- sum(z*(x-mu2[i])^2)/sum(z) + p[i] &lt;- sum(z)/length(z) + } &gt; &gt; tail(cbind(mu1, s1, mu2, s2, p), n=3) mu1 s1 mu2 s2 p [98,] 2.455325 0.3637967 6.7952 6.058291 0.5340015 [99,] 2.455325 0.3637967 6.7952 6.058291 0.5340015 [100,] 2.455325 0.3637967 6.7952 6.058291 0.5340015 42.8 Fitted Mixture Distribution 42.9 Bernoulli Mixture Model As an exercise, derive the EM algorithm of the Bernoilli mixture model introduced earlier. Hint: Replace \\(\\phi(x_i; \\mu_k, \\sigma^2_k)\\) with the appropriate Bernoilli pmf. 42.10 Other Applications of EM Dealing with missing data Multiple imputation of missing data Truncated observations Bayesian hyperparameter estimation Hidden Markov models "],
["theory-of-em.html", "43 Theory of EM 43.1 Decomposition 43.2 Kullback-Leibler Divergence 43.3 Lower Bound 43.4 EM Increases Likelihood", " 43 Theory of EM 43.1 Decomposition Let \\(q({\\boldsymbol{z}})\\) be a probability distribution on the latent variables, \\({\\boldsymbol{z}}\\). Consider the following decomposition: \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) + {\\text{KL}}(q({\\boldsymbol{z}}) \\|f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})) \\] where \\[ \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) = \\int q({\\boldsymbol{z}}) \\log\\left(\\frac{f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}})}{q({\\boldsymbol{z}})}\\right) d{\\boldsymbol{z}}\\] \\[ {\\text{KL}}(q({\\boldsymbol{z}}) \\| f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})) = - \\int q({\\boldsymbol{z}}) \\log\\left(\\frac{f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})}{q({\\boldsymbol{z}})}\\right) d{\\boldsymbol{z}}\\] 43.2 Kullback-Leibler Divergence The KL divergence provides an asymmetric measure of the difference between two probability distributions. The KL divergence is such that \\({\\text{KL}}(q \\| f) \\geq 0\\) where \\({\\text{KL}}(q \\| f) = 0\\) if and only if \\(q=f\\). This property is known as Gibbs inequality. 43.3 Lower Bound Note that \\(\\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}})\\) provides a lower bound on the likelihood function: \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) \\geq \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) \\] If we set \\(q({\\boldsymbol{z}}) = f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)})\\), then for a fixed \\({\\boldsymbol{\\theta}}^{(t)}\\) and as a function of \\({\\boldsymbol{\\theta}}\\), \\[ \\begin{aligned} \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) &amp; \\propto \\int f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}) \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) d{\\boldsymbol{z}}\\\\ &amp; = Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) \\end{aligned} \\] 43.4 EM Increases Likelihood Since \\({\\boldsymbol{\\theta}}^{(t+1)} = {\\text{argmax}}_{{\\boldsymbol{\\theta}}} Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\), it follows that \\[Q({\\boldsymbol{\\theta}}^{(t+1)}, {\\boldsymbol{\\theta}}^{(t)}) \\geq Q({\\boldsymbol{\\theta}}^{(t)}, {\\boldsymbol{\\theta}}^{(t)}).\\] Also, by the properties of KL divergence stated above, we have \\[ {\\text{KL}}(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t+1)}) \\| f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)})) \\geq {\\text{KL}}(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}) \\| f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)})). \\] Putting these together we have \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t+1)}) \\geq \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}). \\] "],
["variational-inference.html", "44 Variational Inference 44.1 Rationale 44.2 Optimization Goal 44.3 Mean Field Approximation 44.4 Optimal \\(q_k({\\boldsymbol{z}}_k)\\) 44.5 Remarks", " 44 Variational Inference 44.1 Rationale Performing the EM algorithm required us to be able to compute \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\) and also optimize \\(Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\). Sometimes this is not possible. Variational inference takes advantage of the decomposition \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) + {\\text{KL}}(q({\\boldsymbol{z}}) \\|f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})) \\] and instead considers other forms of \\(q({\\boldsymbol{z}})\\) to identify a more tractable optimization. 44.2 Optimization Goal Since \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) + {\\text{KL}}(q({\\boldsymbol{z}}) \\|f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})) \\] it follows that the closer \\(q({\\boldsymbol{z}})\\) is to \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\), the term \\(\\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}})\\) grows larger while \\({\\text{KL}}(q({\\boldsymbol{z}}) \\|f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}))\\) becomes smaller. The goal is typically to identify a restricted form of \\(q({\\boldsymbol{z}})\\) that maximizes \\(\\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}})\\), which serves as an approximation to the posterior distribution \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\). 44.3 Mean Field Approximation A mean field approximation implies we restrict \\(q({\\boldsymbol{z}})\\) to be \\[ q({\\boldsymbol{z}}) = \\prod_{k=1}^K q_k({\\boldsymbol{z}}_k) \\] for some partition \\({\\boldsymbol{z}}= ({\\boldsymbol{z}}_1, {\\boldsymbol{z}}_2, \\ldots, {\\boldsymbol{z}}_K)\\). This partition is very context specific and is usually driven by the original model and what is tractable. 44.4 Optimal \\(q_k({\\boldsymbol{z}}_k)\\) Under the above restriction, it can be shown that the \\(\\{q_k({\\boldsymbol{z}}_k)\\}\\) that maximize \\(\\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}})\\) have the form: \\[ q_k({\\boldsymbol{z}}_k) \\propto \\exp \\left\\{ \\int \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) \\prod_{j \\not= k} q_j({\\boldsymbol{z}}_j)d{\\boldsymbol{z}}_j \\right\\}. \\] These pdf’s or pmf’s can be calculated iteratively by cycling over \\(k=1, 2, \\ldots, K\\) after intializing them appropriately. Note that convergence is guaranteed. 44.5 Remarks If \\({\\boldsymbol{\\theta}}\\) is also random, then it can be included in \\({\\boldsymbol{z}}\\). The estimated \\(\\hat{f}({\\boldsymbol{z}}| {\\boldsymbol{x}})\\) is typically concentrated around the high density region of the true \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}})\\), so it is useful for calculations such as the MAP, but it is not guaranteed to be a good overall estimate of \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}})\\). Variational inference is typically faster than MCMC (covered next). Given this is an optimization procedure, care can be taken to speed up convergence and avoid unintended local maxima. "],
["markov-chain-monte-carlo.html", "45 Markov Chain Monte Carlo 45.1 Motivation 45.2 Note 45.3 Big Picture 45.4 Metropolis-Hastings Algorithm 45.5 Metropolis Algorithm 45.6 Utilizing MCMC Output 45.7 Remarks 45.8 Full Conditionals 45.9 Gibbs Sampling 45.10 Gibbs and MH 45.11 Latent Variables 45.12 Theory 45.13 Software", " 45 Markov Chain Monte Carlo 45.1 Motivation When performing Bayesian inferece, it is often (but not always) possible to calculate \\[f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}}) \\propto L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}})\\] but it is typically much more difficult to calculate \\[f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}}) = \\frac{L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}})}{f({\\boldsymbol{x}})}.\\] Markov chain Monte Carlo is a method for simulating data approximately from \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\) with knowledge of only \\(L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}})\\). 45.2 Note MCMC can be used to approximately simulate data from any distribution that is only proportionally characterized, but it is probably most well know for doing so in the context of Bayesian infererence. We will explain MCMC in the context of Bayesian inference. 45.3 Big Picture We draw a Markov chain of \\({\\boldsymbol{\\theta}}\\) values so that, in some asymptotic sense, these are equivalent to iid draws from \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\). The draws are done competitively so that the next draw of a realization of \\({\\boldsymbol{\\theta}}\\) depends on the current value. The Markov chain is set up so that it only depends on \\(L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}})\\). A lot of practical decisions need to be made by the user, so utilize MCMC carefully. 45.4 Metropolis-Hastings Algorithm Initialize \\({\\boldsymbol{\\theta}}^{(0)}\\) Generate \\({\\boldsymbol{\\theta}}^{*} \\sim q({\\boldsymbol{\\theta}}| {\\boldsymbol{\\theta}}^{(b)})\\) for some pdf or pmf \\(q(\\cdot | \\cdot)\\) With probablity \\[A({\\boldsymbol{\\theta}}^{*}, {\\boldsymbol{\\theta}}^{(b)}) = \\min\\left( 1, \\frac{L({\\boldsymbol{\\theta}}^{*}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}}^{*}) q({\\boldsymbol{\\theta}}^{(b)} | {\\boldsymbol{\\theta}}^{*})}{L({\\boldsymbol{\\theta}}^{(b)}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}}^{(b)}) q({\\boldsymbol{\\theta}}^{*} | {\\boldsymbol{\\theta}}^{(b)})} \\right)\\] set \\({\\boldsymbol{\\theta}}^{(b+1)} = {\\boldsymbol{\\theta}}^{*}\\). Otherise, set \\({\\boldsymbol{\\theta}}^{(b+1)} = {\\boldsymbol{\\theta}}^{(b)}\\) Continue for \\(b = 1, 2, \\ldots, B\\) iterations and carefully select which \\({\\boldsymbol{\\theta}}^{(b)}\\) are utilized to approximate iid observations from \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\) 45.5 Metropolis Algorithm The Metropolis algorithm restricts \\(q(\\cdot, \\cdot)\\) to be symmetric so that \\(q({\\boldsymbol{\\theta}}^{(b)} | {\\boldsymbol{\\theta}}^{*}) = q({\\boldsymbol{\\theta}}^{*} | {\\boldsymbol{\\theta}}^{(b)})\\) and \\[ A({\\boldsymbol{\\theta}}^{*}, {\\boldsymbol{\\theta}}^{(b)}) = \\min\\left( 1, \\frac{L({\\boldsymbol{\\theta}}^{*}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}}^{*})}{L({\\boldsymbol{\\theta}}^{(b)}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}}^{(b)})} \\right). \\] 45.6 Utilizing MCMC Output Two common uses of the output from MCMC are as follows: \\({\\operatorname{E}}[f({\\boldsymbol{\\theta}}) | {\\boldsymbol{x}}]\\) is approximated by \\[ \\hat{{\\operatorname{E}}}[f({\\boldsymbol{\\theta}}) | {\\boldsymbol{x}}] = \\frac{1}{B} \\sum_{b=1}^B f\\left({\\boldsymbol{\\theta}}^{(b)}\\right). \\] Some subsequence \\({\\boldsymbol{\\theta}}^{(b_1)}, {\\boldsymbol{\\theta}}^{(b_2)}, \\ldots, {\\boldsymbol{\\theta}}^{(b_m)}\\) from \\(\\left\\{{\\boldsymbol{\\theta}}^{(b)}\\right\\}_{b=1}^{B}\\) is utilized as an empirical approximation to iid draws from \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\). 45.7 Remarks The random draw \\({\\boldsymbol{\\theta}}^{*} \\sim q({\\boldsymbol{\\theta}}| {\\boldsymbol{\\theta}}^{(b)})\\) perturbs the current value \\({\\boldsymbol{\\theta}}^{(b)}\\) to the next value \\({\\boldsymbol{\\theta}}^{(b+1)}\\). It is often a Normal distribution for continuous \\({\\boldsymbol{\\theta}}\\). Choosing the variance of \\(q({\\boldsymbol{\\theta}}| {\\boldsymbol{\\theta}}^{(b)})\\) is important as it requires enough variance for the theory to be applicable within a reasonable number of computations, but it cannot be so large that new values of \\({\\boldsymbol{\\theta}}^{(b+1)}\\) are rarely generated. \\(A({\\boldsymbol{\\theta}}^{*}, {\\boldsymbol{\\theta}}^{(b)})\\) is called the acceptance probability. The algorithm must be run for a certain number of iterations (“burn in”) before observed \\({\\boldsymbol{\\theta}}^{(b)}\\) can be utilized. The generated \\({\\boldsymbol{\\theta}}^{(b)}\\) are typically “thinned” (only sampled every so often) to reduce Markov dependence. 45.8 Full Conditionals Suppose that \\({\\boldsymbol{\\theta}}= (\\theta_1, \\theta_2, \\ldots, \\theta_K)\\). Define the subset vector as \\({\\boldsymbol{\\theta}}_{a:b} = (\\theta_a, \\theta_{a+1}, \\ldots, \\theta_{b-1}, \\theta_b)\\) for any \\(1 \\leq a \\leq b \\leq K\\). The full conditional of \\(\\theta_k\\) is \\[ \\Pr(\\theta_k | {\\boldsymbol{\\theta}}_{1:k-1}, {\\boldsymbol{\\theta}}_{k+1:K}, {\\boldsymbol{x}}) \\] 45.9 Gibbs Sampling Gibbs sampling a special type of Metropolis-Hasting MCMC. The algorithm samples one coordinate of \\({\\boldsymbol{\\theta}}\\) at a time. Initialize \\({\\boldsymbol{\\theta}}^{(0)}\\). Sample: \\(\\theta_1^{(b+1)} \\sim \\Pr(\\theta_1 | {\\boldsymbol{\\theta}}_{2:K}^{(b)}, {\\boldsymbol{x}})\\) \\(\\theta_2^{(b+1)} \\sim \\Pr(\\theta_2 | \\theta_{1}^{(b+1)}, {\\boldsymbol{\\theta}}_{3:K}^{(b)}, {\\boldsymbol{x}})\\) \\(\\theta_3^{(b+1)} \\sim \\Pr(\\theta_3 | {\\boldsymbol{\\theta}}_{1:2}^{(b+1)}, {\\boldsymbol{\\theta}}_{3:K}^{(b)}, {\\boldsymbol{x}})\\) \\(\\vdots\\) \\(\\theta_K^{(b+1)} \\sim \\Pr(\\theta_K | {\\boldsymbol{\\theta}}_{1:K-1}^{(b+1)}, {\\boldsymbol{x}})\\) Continue for \\(b = 1, 2, \\ldots, B\\) iterations. 45.10 Gibbs and MH As an exercise, show that Gibbs sampling is a special case of the Metropolis-Hastings algorithm where \\(A({\\boldsymbol{\\theta}}^{*}, {\\boldsymbol{\\theta}}^{(b)}) = 1\\). 45.11 Latent Variables Note that MCMC is often used to calculate a posterior distribution on latent variables. This makes sense because unobserved random paramaters are a special type of latent variable. 45.12 Theory The goal of MCMC is to construct a Markov chain that converges to a stationary distribution that is equivalent to the target probability distribution. Under reasonably general assumptions, one can show that the Metropolis-Hastings algorithm produces a Markov chain that is homogeneous and achieves detailed balance, which implies the Markov chain is ergodic so that \\({\\boldsymbol{\\theta}}^{(B)}\\) converges in distribution to \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\) as \\(B \\rightarrow \\infty\\) and that \\[ \\hat{{\\operatorname{E}}}[f({\\boldsymbol{\\theta}}) | {\\boldsymbol{x}}] = \\frac{1}{B} \\sum_{b=1}^B f\\left({\\boldsymbol{\\theta}}^{(b)}\\right) \\stackrel{B \\rightarrow \\infty}{\\longrightarrow} {\\operatorname{E}}[f({\\boldsymbol{\\theta}}) | {\\boldsymbol{x}}]. \\] 45.13 Software Stan is probably the currently most popular software for doing Bayesian computation, including MCMC and variational inference. There are also popular R packages, such as MCMCpack. "],
["mcmc-example.html", "46 MCMC Example 46.1 Single Nucleotide Polymorphisms 46.2 PSD Admixture Model 46.3 Gibbs Sampling Approach 46.4 The Data 46.5 Model Components 46.6 The Model 46.7 Conditional Independence 46.8 The Posterior 46.9 Full Conditional for \\(\\boldsymbol{Q}\\) 46.10 Full Conditional for \\(\\boldsymbol{P}\\) 46.11 Full Conditional \\(\\boldsymbol{Z}_A\\) &amp; \\(\\boldsymbol{Z}_B\\) 46.12 Gibbs Sampling Updates 46.13 Implementation 46.14 Matrix-wise rdirichlet Function 46.15 Inspect Data 46.16 Model Parameters 46.17 Update \\(\\boldsymbol{P}\\) 46.18 Update \\(\\boldsymbol{Q}\\) 46.19 Update (Each) \\(\\boldsymbol{Z}\\) 46.20 Model Log-likelihood Function 46.21 MCMC Configuration 46.22 Run Sampler 46.23 Posterior Mean of \\(\\boldsymbol{Q}\\) 46.24 Plot Log-likelihood Steps 46.25 What Happens for K=4? 46.26 Run Sampler Again 46.27 Posterior Mean of \\(\\boldsymbol{Q}\\)", " 46 MCMC Example 46.1 Single Nucleotide Polymorphisms SNPs 46.2 PSD Admixture Model PSD PSD model proposed in Pritchard, Stephens, Donnelly (2000) Genetics. 46.3 Gibbs Sampling Approach The Bayesian Gibbs sampling approach to inferring the PSD model touches on many important ideas, such as conjugate priors and mixture models. We will focus on a version of this model for diploid SNPs. 46.4 The Data \\(\\boldsymbol{X}\\), a \\(L \\times N\\) matrix consisting of the genotypes, coded as \\(0,1,2\\). Each row is a SNP, each column is an individual. In order for this model to work, the data needs to be broken down into “phased” genotypes. For the 0 and 2 cases, it’s obvious how to do this, and for the 1 case, it’ll suffice for this model to randomly assign the alleles to chromosomes. We will explore phasing more on HW4. Thus, we wind up with two {0, 1} binary matrices \\(\\boldsymbol{X}_A\\) and \\(\\boldsymbol{X}_B\\), both \\(L \\times N\\). We will refer to allele \\(A\\) and allele \\(B\\). Note \\({\\boldsymbol{X}}= {\\boldsymbol{X}}_A + {\\boldsymbol{X}}_B\\). 46.5 Model Components \\(K\\), the number of populations that we model the genotypes as admixtures of. This is chosen before inference. \\(\\boldsymbol{Q}\\), a \\(N \\times K\\) matrix, the admixture proportions, values are in the interval \\([0, 1]\\) and rows are constrained to sum to 1. \\(\\boldsymbol{P}\\), a \\(L \\times K\\) matrix, the allele frequencies for each population, values are in the interval \\([0, 1]\\). \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\), two \\(L \\times N\\) matrices that tell us which population the respective allele is from. Elements consist of the integers between \\(1\\) and \\(K\\). This is a hidden variable. 46.6 The Model Each allele (elements of \\(\\boldsymbol{X}_A\\) and \\(\\boldsymbol{X}_B\\)) is a Bernoulli random variable, with success probability determined by which population that allele is assigned to (i.e., depends on \\(\\boldsymbol{Z}_A\\), \\(\\boldsymbol{Z}_B\\), and \\(\\boldsymbol{P}\\)). We put a uniform Beta prior, i.e., \\(\\operatorname{Beta}(1, 1)\\), on each element of \\(\\boldsymbol{P}\\). We put a uniform Dirichlet prior, i.e., \\(\\operatorname{Dirichlet(1,\\ldots,1)}\\), on each row of \\(\\boldsymbol{Q}\\). \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) are \\(K\\)-class Multinomial draws where the probability of drawing each class is determined by each row of \\(Q\\). 46.7 Conditional Independence The key observation is to understand which parts of the model are dependent on each other in the data generating process. The data \\(\\boldsymbol{X}_A\\) and \\(\\boldsymbol{X}_B\\) depends directly on \\(\\boldsymbol{Z}_A\\), \\(\\boldsymbol{Z}_B\\), and \\(\\boldsymbol{P}\\) (not \\(\\boldsymbol{Q}\\)!). The latent variable \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) depend only on \\(\\boldsymbol{Q}\\) and they’re conditionally independent given \\(\\boldsymbol{Q}\\). \\(\\boldsymbol{Q}\\) and \\(\\boldsymbol{P}\\) depend only on their priors. \\(\\Pr(\\boldsymbol{X}_A, \\boldsymbol{X}_B, \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{P}, \\boldsymbol{Q}) =\\) \\(\\Pr(\\boldsymbol{X}_A, \\boldsymbol{X}_B | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{P}) \\Pr(\\boldsymbol{Z}_A | \\boldsymbol{Q}) \\Pr(\\boldsymbol{Z}_B | \\boldsymbol{Q}) \\Pr(\\boldsymbol{P}) \\Pr(\\boldsymbol{Q})\\) 46.8 The Posterior We desire to compute the posterior distribution \\(\\Pr(\\boldsymbol{P}, \\boldsymbol{Q}, \\boldsymbol{Z}_A, \\boldsymbol{Z}_B | \\boldsymbol{X}_A, \\boldsymbol{X}_B)\\). Gibbs sampling tells us if we can construct conditional distributions for each random variable in our model, then iteratively sampling and updating our model parameters will result in a stationary distribution that is the same as the posterior distribution. Gibbs sampling is an extremely powerful approach for this model because we can utilize conjugate priors as well as the independence of various parameters in the model to compute these conditional distributions. 46.9 Full Conditional for \\(\\boldsymbol{Q}\\) Note that \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) are the only parts of this model that directly depend on \\(\\boldsymbol{Q}\\). \\[\\begin{align*} &amp;\\Pr(Q_n | \\boldsymbol{Q}_{-n}, \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{P}, \\boldsymbol{X}_A, \\boldsymbol{X}_B)\\\\ =&amp; \\Pr(Q_n | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B) \\\\ \\propto&amp; \\Pr(Z_{An}, Z_{Bn} | Q_n) \\Pr(Q_n)\\\\ =&amp; \\Pr(Z_{An} | Q_n) \\Pr(Z_{Bn} | Q_n) \\Pr(Q_n)\\\\ \\propto&amp; \\left( \\prod_{\\ell=1}^L \\prod_{k=1}^K Q_{nk}^{\\mathbb{1}(Z_{An\\ell}=k)+\\mathbb{1}(Z_{Bn\\ell}=k)} \\right) \\end{align*}\\] \\[=\\prod_{k=1}^K Q_{nk}^{S_{nk}}\\] where \\(S_{nk}\\) is simply the count of the number of alleles for individual \\(n\\) that got assigned to population \\(k\\). Thus, \\(Q_n | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B \\sim \\operatorname{Dirichlet}(S_{j1}+1, \\ldots, S_{jk}+1)\\),. We could have guessed that this distribution is Dirichlet given that \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) are multinomial! Let’s use conjugacy to help us in the future. 46.10 Full Conditional for \\(\\boldsymbol{P}\\) \\[\\begin{align*} &amp;\\Pr(P_\\ell | \\boldsymbol{P}_{-\\ell}, \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{Q}, \\boldsymbol{X}_A, \\boldsymbol{X}_B) \\\\ \\propto&amp; \\Pr(\\boldsymbol{X}_{A\\ell}, \\boldsymbol{X}_{B\\ell} | P_\\ell, \\boldsymbol{Z}_{A\\ell}, \\boldsymbol{Z}_{B\\ell}) \\Pr(P_\\ell) \\end{align*}\\] We know \\(\\Pr(\\boldsymbol{X}_{A\\ell}, \\boldsymbol{X}_{B\\ell} | P_\\ell, \\boldsymbol{Z}_{A\\ell}, \\boldsymbol{Z}_{B\\ell})\\) will be Bernoulli and \\(\\Pr(P_\\ell)\\) will be beta, so the full conditional will be beta as well. In fact, the prior is uniform so it vanishes from the RHS. Thus, all we have to worry about is the Bernoulli portion \\(\\Pr(\\boldsymbol{X}_{A\\ell}, \\boldsymbol{X}_{B\\ell} | P_\\ell, \\boldsymbol{Z}_{A\\ell}, \\boldsymbol{Z}_{B\\ell})\\). Here, we observe that if the \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) are “known”, then we known which value of \\(P_\\ell\\) to plug into our Bernoulli for \\(\\boldsymbol{X}_A\\) and \\(\\boldsymbol{X}_B\\). Following the Week 6 lectures, we find that the full conditional for \\(\\boldsymbol{P}\\) is: \\[P_{\\ell k} | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{X}_A, \\boldsymbol{X}_B \\sim \\operatorname{Beta}(1+T_{\\ell k 0}, 1+T_{\\ell k 1})\\] where \\(T_{\\ell k 0}\\) is the total number of 0 alleles at SNP \\(\\ell\\) for population \\(k\\), and \\(T_{\\ell k 1}\\) is the analogous quantity for the 1 allele. 46.11 Full Conditional \\(\\boldsymbol{Z}_A\\) &amp; \\(\\boldsymbol{Z}_B\\) We’ll save some math by first noting that alleles \\(A\\) and \\(B\\) are independent of each other, so we can write this for only \\(\\boldsymbol{Z}_A\\) without losing any information. Also, all elements of \\(\\boldsymbol{Z}_A\\) are independent of each other. Further, note that each element of \\(\\boldsymbol{Z}_A\\) is a single multinomial draw, so we are working with a discrete random variable. \\[\\begin{align*} &amp;\\Pr(Z_{A\\ell n}=k | \\boldsymbol{X}_A, \\boldsymbol{Q}, \\boldsymbol{P}) \\\\ =&amp; \\Pr (Z_{A\\ell n}=k | X_{A \\ell n}, Q_n, P_\\ell) \\\\ \\propto &amp; \\Pr(X_{A \\ell n} | Z_{A\\ell n}=k, Q_n, P_\\ell) \\Pr(Z_{A\\ell n}=k | Q_n, P_\\ell) \\end{align*}\\] We can look at the two factors. First: \\[\\Pr(Z_{A\\ell n}=k | Q_n, P_\\ell) = \\Pr(Z_{A\\ell n}=k | Q_n) = Q_{nk}\\] Then: \\[\\Pr(X_{A \\ell n} | Z_{A\\ell n}=k, Q_n, P_\\ell) = P_{\\ell k}\\] Thus, we arrive at the formula: \\[\\Pr(Z_{A\\ell n}=k | \\boldsymbol{X}_A, \\boldsymbol{Q}, \\boldsymbol{P}) \\propto P_{\\ell k} Q_{n k}\\] 46.12 Gibbs Sampling Updates It’s neat that we wind up just iteratively counting the various discrete random variables along different dimensions. \\[\\begin{align*} Q_n | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B &amp;\\sim \\operatorname{Dirichlet}(S_{j1}+1, \\ldots, S_{jk}+1)\\\\ P_{\\ell k} | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{X}_A, \\boldsymbol{X}_B &amp;\\sim \\operatorname{Beta}(1+T_{\\ell k 0}, 1+T_{\\ell k 1}) \\\\ Z_{A\\ell n} | \\boldsymbol{X}_A, \\boldsymbol{Q}, \\boldsymbol{P} &amp;\\sim \\operatorname{Multinomial}\\left(\\frac{P_\\ell * Q_n}{P_\\ell \\cdot Q_n}\\right) \\end{align*}\\] where \\(*\\) means element-wise vector multiplication. 46.13 Implementation The Markov chain property means that we can’t use vectorization forward in time, so R is not the best way to implement this algorithm. That being said, we can vectorize the pieces that we can and demonstrate what happens. 46.14 Matrix-wise rdirichlet Function Drawing from a Dirichlet is easy and vectorizable because it consists of normalizing independent gamma draws. &gt; rdirichlet &lt;- function(alpha) { + m &lt;- nrow(alpha) + n &lt;- ncol(alpha) + x &lt;- matrix(rgamma(m * n, alpha), ncol = n) + x/rowSums(x) + } 46.15 Inspect Data &gt; dim(Xa) [1] 400 24 &gt; X[1:3,1:3] NA18516 NA19138 NA19137 rs2051075 0 1 2 rs765546 2 2 0 rs10019399 2 2 2 &gt; Xa[1:3,1:3] NA18516 NA19138 NA19137 rs2051075 0 0 1 rs765546 1 1 0 rs10019399 1 1 1 46.16 Model Parameters &gt; L &lt;- nrow(Xa) &gt; N &lt;- ncol(Xa) &gt; &gt; K &lt;- 3 &gt; &gt; Za &lt;- matrix(sample(1:K, L*N, replace=TRUE), L, N) &gt; Zb &lt;- matrix(sample(1:K, L*N, replace=TRUE), L, N) &gt; P &lt;- matrix(0, L, K) &gt; Q &lt;- matrix(0, N, K) 46.17 Update \\(\\boldsymbol{P}\\) &gt; update_P &lt;- function() { + Na_0 &lt;- Za * (Xa==0) + Na_1 &lt;- Za * (Xa==1) + Nb_0 &lt;- Zb * (Xb==0) + Nb_1 &lt;- Zb * (Xb==1) + for(k in 1:K) { + N0 &lt;- rowSums(Na_0==k)+rowSums(Nb_0==k) + N1 &lt;- rowSums(Na_1==k)+rowSums(Nb_1==k) + P[,k] &lt;- rdirichlet(1+cbind(N1, N0))[,1] + } + P + } 46.18 Update \\(\\boldsymbol{Q}\\) &gt; update_Q &lt;- function() { + M_POP0 &lt;- apply(Za, 2, function(x) {tabulate(x, nbins=K)} ) + M_POP1 &lt;- apply(Zb, 2, function(x) {tabulate(x, nbins=K)} ) + + rdirichlet(t(1+M_POP0+M_POP1)) + } 46.19 Update (Each) \\(\\boldsymbol{Z}\\) &gt; update_Z &lt;- function(X) { + Z &lt;- matrix(0, nrow(X), ncol(X)) + for(n in 1:N) { + PZ0 &lt;- t(t((1-P)) * Q[n,]) + PZ1 &lt;- t(t(P) * Q[n,]) + PZ &lt;- X[,n]*PZ1 + (1-X[,n])*PZ0 + Z[,n] &lt;- apply(PZ, 1, function(p){sample(1:K, 1, prob=p)}) + } + Z + } 46.20 Model Log-likelihood Function &gt; model_ll &lt;- function() { + AFa &lt;- t(sapply(1:L, function(i){P[i,][Za[i,]]})) + AFb &lt;- t(sapply(1:L, function(i){P[i,][Zb[i,]]})) + # hint, hint, HW3 + sum(dbinom(Xa, 1, AFa, log=TRUE)) + + sum(dbinom(Xb, 1, AFb, log=TRUE)) + } 46.21 MCMC Configuration &gt; MAX_IT &lt;- 20000 &gt; BURNIN &lt;- 5000 &gt; THIN &lt;- 20 &gt; &gt; QSUM &lt;- matrix(0, N, K) &gt; &gt; START &lt;- 200 &gt; TAIL &lt;- 500 &gt; LL_start &lt;- rep(0, START) &gt; LL_end &lt;- rep(0, TAIL) 46.22 Run Sampler &gt; set.seed(1234) &gt; &gt; for(it in 1:MAX_IT) { + P &lt;- update_P() + Q &lt;- update_Q() + Za &lt;- update_Z(Xa) + Zb &lt;- update_Z(Xb) + + if(it &gt; BURNIN &amp;&amp; it %% THIN == 0) {QSUM &lt;- QSUM+Q} + if(it &lt;= START) {LL_start[it] &lt;- model_ll()} + if(it &gt; MAX_IT-TAIL) {LL_end[it-(MAX_IT-TAIL)] &lt;- model_ll()} + } &gt; &gt; Q_MEAN &lt;- QSUM/((MAX_IT-BURNIN)/THIN) 46.23 Posterior Mean of \\(\\boldsymbol{Q}\\) 46.24 Plot Log-likelihood Steps Note both the needed burn-in and thinning. 46.25 What Happens for K=4? &gt; K &lt;- 4 &gt; Za &lt;- matrix(sample(1:K, L*N, replace=TRUE), L, N) &gt; Zb &lt;- matrix(sample(1:K, L*N, replace=TRUE), L, N) &gt; P &lt;- matrix(0, L, K) &gt; Q &lt;- matrix(0, N, K) &gt; QSUM &lt;- matrix(0, N, K) 46.26 Run Sampler Again &gt; for(it in 1:MAX_IT) { + P &lt;- update_P() + Q &lt;- update_Q() + Za &lt;- update_Z(Xa) + Zb &lt;- update_Z(Xb) + + if(it &gt; BURNIN &amp;&amp; it %% THIN == 0) { + QSUM &lt;- QSUM+Q + } + } &gt; &gt; Q_MEAN &lt;- QSUM/((MAX_IT-BURNIN)/THIN) 46.27 Posterior Mean of \\(\\boldsymbol{Q}\\) "],
["further-reading.html", "47 Further Reading", " 47 Further Reading Bishop (2016) One of the clearest treatments of the EM algorithm, variational inference, and MCMC can be found in Chapters 9-11 of Pattern Recognition and Machine Learning, by Christopher Bishop. This is a great book in general. EM Algorithm Paper that popularized the method: Dempster, Laird, Rubin (1977) Paper that got the theory correct: Wu (1983) Variational Inference Wainwright and Jordan (2008) Ormerod and Wand (2010) Blei et al. (2016) MCMC MCMC Without All the BS Bayesian Data Analysis by Gelman et al. Monte Carlo Strategies in Scientific Computing by Jun Liu "],
["nonparametric-statistics.html", "48 Nonparametric Statistics 48.1 Parametric Inference 48.2 Nonparametric Inference 48.3 Nonparametric Descriptive Statistics 48.4 Semiparametric Inference", " 48 Nonparametric Statistics 48.1 Parametric Inference Parametric inference is based on a family of known probability distributions governed by a defined parameter space. The goal is to perform inference (or more generally statistics) on the values of the parameters. 48.2 Nonparametric Inference Nonparametric inference or modeling can be described in two ways (not mutually exclusive): An inference procedure or model that does not depend on or utilize the parametrized probability distribution from which the data are generated. An inference procedure or model that may have a specific structure or based on a specific formula, but the complexity is adaptive and can grow to arbitrary levels of complexity as the sample size grows. In All of Nonparametric Statistics, Larry Wasserman says: … it is difficult to give a precise definition of nonparametric inference…. For the purposes of this book, we will use the phrase nonparametric inference to refer to a set of modern statistical methods that aim to keep the number of underlying assumptions as weak as possible. He then lists five estimation examples (see Section 1.1): distributions, functionals, densities, regression curves, and Normal means. 48.3 Nonparametric Descriptive Statistics Almost all of the exploratory data analysis methods we covered in the beginning of the course are nonparametric. Sometimes the exploratory methods are calibrated by known probability distributions, but they are usually informative regardless of the underlying probability distribution (or lack thereof) of the data. 48.4 Semiparametric Inference Semiparametric inference or modeling methods contain both parametric and nonparametric components. An example is \\(X_i | \\mu_i \\sim \\mbox{Normal}(\\mu_i, 1)\\) and \\(\\mu_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\) for some arbitrary distribution \\(F\\). "],
["empirical-distribution-functions.html", "49 Empirical Distribution Functions 49.1 Definition 49.2 Example: Normal 49.3 Pointwise Convergence 49.4 Glivenko-Cantelli Theorem 49.5 Dvoretzky-Kiefer-Wolfowitz (DKW) Inequality 49.6 Statistical Functionals 49.7 Plug-In Estimator 49.8 EDF Standard Error 49.9 EDF CLT", " 49 Empirical Distribution Functions 49.1 Definition Suppose \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\). The empirical distribution function (edf) – or empirical cumulative distribution function (ecdf) – is the distribution that puts probability \\(1/n\\) on each observed value \\(X_i\\). Let \\(1(X_i \\leq y) = 1\\) if \\(X_i \\leq y\\) and \\(1(X_i \\leq y) = 0\\) if \\(X_i &gt; y\\). \\[ \\mbox{Random variable: } \\hat{F}_{{\\boldsymbol{X}}}(y) = \\frac{1}{n} \\sum_{i=1}^{n} 1(X_i \\leq y) \\] \\[ \\mbox{Observed variable: } \\hat{F}_{{\\boldsymbol{x}}}(y) = \\frac{1}{n} \\sum_{i=1}^{n} 1(x_i \\leq y) \\] 49.2 Example: Normal 49.3 Pointwise Convergence Under our assumptions, by the strong law of large numbers for each \\(y \\in \\mathbb{R}\\), \\[ \\hat{F}_{{\\boldsymbol{X}}}(y) \\stackrel{\\text{a.s.}}{\\longrightarrow} F(y) \\] as \\(n \\rightarrow \\infty\\). 49.4 Glivenko-Cantelli Theorem Under our assumptions, we can get a much stronger convergence result: \\[ \\sup_{y \\in \\mathbb{R}} \\left| \\hat{F}_{{\\boldsymbol{X}}}(y) - F(y) \\right| \\stackrel{\\text{a.s.}}{\\longrightarrow} 0 \\] as \\(n \\rightarrow \\infty\\). Here, “sup” is short for supremum, which is a mathematical generalization of maximum. This result says that even the worst difference between the edf and the true cdf converges with probability 1 to zero. 49.5 Dvoretzky-Kiefer-Wolfowitz (DKW) Inequality This result gives us an upper bound on how far off the edf is from the true cdf, which allows us to construct confidence bands about the edf. \\[ \\Pr\\left( \\sup_{y \\in \\mathbb{R}} \\left| \\hat{F}_{{\\boldsymbol{X}}}(y) - F(y) \\right| &gt; \\epsilon \\right) \\leq 2 \\exp{-2 n \\epsilon^2} \\] As outlined in All of Nonparametric Statistics, setting \\[\\epsilon_n = \\sqrt{\\frac{1}{2n} \\log\\left(\\frac{2}{\\alpha}\\right)}\\] \\[L(y) = \\max\\{\\hat{F}_{{\\boldsymbol{X}}}(y) - \\epsilon_n, 0 \\}\\] \\[U(y) = \\min\\{\\hat{F}_{{\\boldsymbol{X}}}(y) + \\epsilon_n, 1 \\}\\] guarantees that \\(\\Pr(L(y) \\leq F(y) \\leq U(y) \\mbox{ for all } y) \\geq 1-\\alpha\\). 49.6 Statistical Functionals A statistical functional \\(T(F)\\) is any function of \\(F\\). Examples: \\(\\mu(F) = \\int x dF(x)\\) \\(\\sigma^2(F) = \\int (x-\\mu(F))^2 dF(x)\\) \\(\\text{median}(F) = F^{-1}(1/2)\\) A linear statistical functional is such that \\(T(F) = \\int a(x) dF(x)\\). 49.7 Plug-In Estimator A plug-in estimator of \\(T(F)\\) based on the edf is \\(T(\\hat{F}_{{\\boldsymbol{X}}})\\). Examples: \\(\\hat{\\mu} = \\mu(\\hat{F}_{{\\boldsymbol{X}}}) = \\int x \\hat{F}_{{\\boldsymbol{X}}}(x) = \\frac{1}{n} \\sum_{i=1}^n X_i\\) \\(\\hat{\\sigma}^2 = \\sigma^2(\\hat{F}_{{\\boldsymbol{X}}}) = \\int (x-\\hat{\\mu})^2 \\hat{F}_{{\\boldsymbol{X}}}(x) = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\hat{\\mu})^2\\) \\(\\text{median}(\\hat{F}_{{\\boldsymbol{X}}}) = \\hat{F}_{{\\boldsymbol{X}}}^{-1}(1/2)\\) 49.8 EDF Standard Error Suppose that \\(T(F) = \\int a(x) dF(x)\\) is a linear functional. Then: \\[ \\begin{aligned} \\ &amp; {\\operatorname{Var}}(T(\\hat{F}_{{\\boldsymbol{X}}})) = \\frac{1}{n^2} \\sum_{i=1}^n {\\operatorname{Var}}(a(X_i)) = \\frac{{\\operatorname{Var}}_F(a(X))}{n} \\\\ \\ &amp; {\\operatorname{se}}(T(\\hat{F}_{{\\boldsymbol{X}}})) = \\sqrt{\\frac{{\\operatorname{Var}}_F(a(X))}{n}} \\\\ \\ &amp; \\hat{{\\operatorname{se}}}(T(\\hat{F}_{{\\boldsymbol{X}}})) = \\sqrt{\\frac{{\\operatorname{Var}}_{\\hat{F}_{{\\boldsymbol{X}}}}(a(X))}{n}} \\end{aligned} \\] Note that \\[ {\\operatorname{Var}}_F(a(X)) = \\int (a(x) - T(F))^2 dF(x) \\] because \\(T(F) = \\int a(x) dF(x) = {\\operatorname{E}}_F[a(X)]\\). Likewise, \\[ {\\operatorname{Var}}_{\\hat{F}_{{\\boldsymbol{X}}}}(a(X)) = \\frac{1}{n} \\sum_{i=1}^n (a(X_i) - T(\\hat{F}_{{\\boldsymbol{X}}}))^2 \\] where \\(T(\\hat{F}_{{\\boldsymbol{X}}}) = \\frac{1}{n} \\sum_{i=1}^n a(X_i)\\). 49.9 EDF CLT Suppose that \\({\\operatorname{Var}}_F(a(X)) &lt; \\infty\\). Then we have the following convergences as \\(n \\rightarrow \\infty\\): \\[ \\frac{{\\operatorname{Var}}_{\\hat{F}_{{\\boldsymbol{X}}}}(a(X))}{{\\operatorname{Var}}_{F}(a(X))} \\stackrel{P}{\\longrightarrow} 1 \\mbox{ , } \\frac{\\hat{{\\operatorname{se}}}(T(\\hat{F}_{{\\boldsymbol{X}}}))}{{\\operatorname{se}}(T(\\hat{F}_{{\\boldsymbol{X}}}))} \\stackrel{P}{\\longrightarrow} 1 \\] \\[ \\frac{T(F) - T(\\hat{F}_{{\\boldsymbol{X}}})}{\\hat{{\\operatorname{se}}}(T(\\hat{F}_{{\\boldsymbol{X}}}))} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1) \\] The estimators are very easy to calculate on real data, so this a powerful set of results. "],
["bootstrap.html", "50 Bootstrap 50.1 Rationale 50.2 Big Picture 50.3 Bootstrap Variance 50.4 Caveat 50.5 Bootstrap Sample 50.6 Bootstrap CIs 50.7 Invoking the CLT 50.8 Percentile Interval 50.9 Pivotal Interval 50.10 Studentized Pivotal Interval 50.11 Bootstrap Hypothesis Testing 50.12 Example: t-test 50.13 Parametric Bootstrap 50.14 Example: Exponential Data", " 50 Bootstrap 50.1 Rationale Suppose \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\). If the edf \\(\\hat{F}_{{\\boldsymbol{X}}}\\) is an accurate approximation for the true cdf \\(F\\), then we can utilize \\(\\hat{F}_{{\\boldsymbol{X}}}\\) in place of \\(F\\) to nonparametrically characterize the sampling distribution of a statistic \\(T({\\boldsymbol{X}})\\). This allows for the sampling distribution of more general statistics to be considered, such as the median or a percentile, as well as more traditional statistics, such as the mean, when the underlying distribution is unknown. When we encounter modeling fitting, the bootstrap may be very useful for characterizing the sampling distribution of complex statistics we calculate from fitted models. 50.2 Big Picture We calculate \\(T({\\boldsymbol{x}})\\) on the observed data, and we also form the edf, \\(\\hat{F}_{{\\boldsymbol{x}}}\\). To approximate the sampling distribution of \\(T({\\boldsymbol{X}})\\) we generate \\(B\\) random samples of \\(n\\) iid data points from \\(\\hat{F}_{{\\boldsymbol{x}}}\\) and calculate \\(T({\\boldsymbol{x}}^{*(b)})\\) for each bootstrap sample \\(b = 1, 2, \\ldots, B\\) where \\({\\boldsymbol{x}}^{*(b)} = (x_1^{*(b)}, x_2^{*(b)}, \\ldots, x_n^{*(b)})^T\\). Sampling \\(X_1^{*}, \\ldots, X_n^{*} {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\hat{F}_{{\\boldsymbol{x}}}\\) is accomplished by sampling \\(n\\) times with replacement from the observed data \\(x_1, x_2, \\ldots, x_n\\). This means \\(\\Pr\\left(X^{*} = x_j\\right) = \\frac{1}{n}\\) for all \\(j\\). 50.3 Bootstrap Variance For each bootstrap sample \\({\\boldsymbol{x}}^{*(b)} = (x_1^{*(b)}, x_2^{*(b)}, \\ldots, x_n^{*(b)})^T\\), calculate bootstrap statistic \\(T({\\boldsymbol{x}}^{*(b)})\\). Repeat this for \\(b = 1, 2, \\ldots, B\\). Estimate the sampling variance of \\(T({\\boldsymbol{x}})\\) by \\[ \\hat{{\\operatorname{Var}}}(T({\\boldsymbol{x}})) = \\frac{1}{B} \\sum_{b=1}^B \\left(T\\left({\\boldsymbol{x}}^{*(b)}\\right) - \\frac{1}{B} \\sum_{k=1}^B T\\left({\\boldsymbol{x}}^{*(k)}\\right) \\right)^2 \\] 50.4 Caveat Why haven’t we just been doing this the entire time?! In All of Nonparametric Statistics, Larry Wasserman states: There is a tendency to treat the bootstrap as a panacea for all problems. But the bootstrap requires regularity conditions to yield valid answers. It should not be applied blindly. The bootstrap is easy to motivate, but it is quite tricky to implement outside of the very standard problems. It sometimes requires deeper knowledge of statistical theory than likelihood-based inference. 50.5 Bootstrap Sample For a sample of size \\(n\\), what percentage of the data is present in any given bootstrap sample? 50.6 Bootstrap CIs Suppose that \\(\\theta = T(F)\\) and \\(\\hat{\\theta} = T(\\hat{F}_{{\\boldsymbol{x}}})\\). We can use the bootstrap to generate data from \\(\\hat{F}_{{\\boldsymbol{x}}}\\). For \\(b = 1, 2, \\ldots, B\\), we draw \\(x_1^{*(b)}, x_2^{*(b)}, \\ldots, x_n^{*(b)}\\) as iid realiztions from \\(\\hat{F}_{{\\boldsymbol{x}}}\\), and calculate \\(\\hat{\\theta}^{*(b)} = T(\\hat{F}_{{\\boldsymbol{x}}^{*(b)}})\\). Let \\(p^{*}_{\\alpha}\\) be the \\(\\alpha\\) percentile of \\(\\left\\{\\hat{\\theta}^{*(1)}, \\hat{\\theta}^{*(2)}, \\ldots, \\hat{\\theta}^{*(B)}\\right\\}\\). Let’s discuss several ways of calculating confidence intervals for \\(\\theta = T(F)\\). 50.7 Invoking the CLT If we have evidence that the central limit theorem can be applied, we can form the \\((1-\\alpha)\\) CI as: \\[ (\\hat{\\theta} - |z_{\\alpha/2}| {\\operatorname{se}}^*, \\hat{\\theta} + |z_{\\alpha/2}| {\\operatorname{se}}^*) \\] where \\({\\operatorname{se}}^*\\) is the bootstrap standard error calculated as \\[ {\\operatorname{se}}^{*} = \\sqrt{\\frac{1}{B} \\sum_{b=1}^B \\left(\\hat{\\theta}^{*(b)} - \\frac{1}{B} \\sum_{k=1}^B \\hat{\\theta}^{*(k)} \\right)^2}. \\] Note that \\({\\operatorname{se}}^*\\) serves as estimate of \\({\\operatorname{se}}(\\hat{\\theta})\\). Note that to get this confidence interval we need to justify that the following pivotal statistics are approximately Normal(0,1): \\[ \\frac{\\hat{\\theta} - \\theta}{{\\operatorname{se}}(\\hat{\\theta})} \\approx \\frac{\\hat{\\theta} - \\theta}{{\\operatorname{se}}^*} \\] 50.8 Percentile Interval If a monotone function \\(m(\\cdot)\\) exists so that \\(m\\left(\\hat{\\theta}\\right) \\sim \\mbox{Normal}(m(\\theta), b^2)\\), then we can form the \\((1-\\alpha)\\) CI as: \\[ \\left(p^*_{\\alpha/2}, p^*_{1-\\alpha/2} \\right) \\] where recall that in general \\(p^{*}_{\\alpha}\\) is the \\(\\alpha\\) percentile of bootstrap estimates \\(\\left\\{\\hat{\\theta}^{*(1)}, \\hat{\\theta}^{*(2)}, \\ldots, \\hat{\\theta}^{*(B)}\\right\\}\\) 50.9 Pivotal Interval Suppose we can calculate percentiles of \\(\\hat{\\theta} - \\theta\\), say \\(q_{\\alpha}\\). Note that the \\(\\alpha\\) percentile of \\(\\hat{\\theta}\\) is \\(q_\\alpha + \\theta\\). The \\(1-\\alpha\\) CI is \\[ (\\hat{\\theta}-q_{1-\\alpha/2}, \\hat{\\theta}-q_{\\alpha/2}) \\] which comes from: \\[ \\begin{aligned} 1-\\alpha &amp; = \\Pr(q_{\\alpha/2} \\leq \\hat{\\theta} - \\theta \\leq q_{1-\\alpha/2}) \\\\ &amp; = \\Pr(-q_{1-\\alpha/2} \\leq \\theta - \\hat{\\theta} \\leq -q_{\\alpha/2}) \\\\ &amp; = \\Pr(\\hat{\\theta}-q_{1-\\alpha/2} \\leq \\theta \\leq \\hat{\\theta}-q_{\\alpha/2}) \\\\ \\end{aligned} \\] Suppose the sampling distribution of \\(\\hat{\\theta}^* - \\hat{\\theta}\\) is an approximation for that of \\(\\hat{\\theta} - \\theta\\). If \\(p^*_{\\alpha}\\) is the \\(\\alpha\\) percentile of \\(\\hat{\\theta}^*\\) then, \\(p^*_{\\alpha} - \\hat{\\theta}\\) is the \\(\\alpha\\) percentile of \\(\\hat{\\theta}^* - \\hat{\\theta}\\). Therefore, \\(p^*_{\\alpha} - \\hat{\\theta}\\) is the bootstrap estimate of \\(q_{\\alpha}\\). Plugging this into \\((\\hat{\\theta}-q_{1-\\alpha/2}, \\hat{\\theta}-q_{\\alpha/2})\\), we get the following \\((1-\\alpha)\\) bootstrap CI: \\[ \\left(2\\hat{\\theta}-p^*_{1-\\alpha/2}, 2\\hat{\\theta}-p^*_{\\alpha/2}\\right). \\] 50.10 Studentized Pivotal Interval In the previous scenario, we needed to assume that the sampling distribution of \\(\\hat{\\theta}^* - \\hat{\\theta}\\) is an approximation for that of \\(\\hat{\\theta} - \\theta\\). Sometimes this will not be the case and instead we can studentize this pivotal quantity. That is, the distribution of \\[ \\frac{\\hat{\\theta} - \\theta}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right)} \\] is well-approximated by that of \\[ \\frac{\\hat{\\theta}^* - \\hat{\\theta}}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*}\\right)}. \\] Let \\(z^{*}_{\\alpha}\\) be the \\(\\alpha\\) percentile of \\[ \\left\\{ \\frac{\\hat{\\theta}^{*(1)} - \\hat{\\theta}}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(1)}\\right)}, \\ldots, \\frac{\\hat{\\theta}^{*(B)} - \\hat{\\theta}}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(B)}\\right)} \\right\\}. \\] Then a \\((1-\\alpha)\\) bootstrap CI is \\[ \\left(\\hat{\\theta} - z^{*}_{1-\\alpha/2} \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right), \\hat{\\theta} - z^{*}_{\\alpha/2} \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right)\\right). \\] Exercise: Why? How do we obtain \\(\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right)\\) and \\(\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(b)}\\right)\\)? If we have an analytical formula for these, then \\(\\hat{{\\operatorname{se}}}(\\hat{\\theta})\\) is calculated from the original data and \\(\\hat{{\\operatorname{se}}}(\\hat{\\theta}^{*(b)})\\) from the bootstrap data sets. But we probably don’t since we’re using the bootstrap. Instead, we can calculate: \\[ \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right) = \\sqrt{\\frac{1}{B} \\sum_{b=1}^B \\left(\\hat{\\theta}^{*(b)} - \\frac{1}{B} \\sum_{k=1}^B \\hat{\\theta}^{*(k)} \\right)^2}. \\] This is what we called \\({\\operatorname{se}}^*\\) above. But what about \\(\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(b)}\\right)\\)? To estimate \\(\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(b)}\\right)\\) we need to do a double bootstrap. For each bootstrap sample \\(b\\) we need to bootstrap that daat set another \\(B\\) times to calculate: \\[ \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(b)}\\right) = \\sqrt{\\frac{1}{B} \\sum_{v=1}^B \\left(\\hat{\\theta}^{*(b)*(v)} - \\frac{1}{B} \\sum_{k=1}^B \\hat{\\theta}^{*(b)*(k)} \\right)^2} \\] where \\(\\hat{\\theta}^{*(b)*(v)}\\) is the statistic calculated from bootstrap sample \\(v\\) within bootstrap sample \\(b\\). This can be very computationally intensive, and it requires a large sample size \\(n\\). 50.11 Bootstrap Hypothesis Testing As we have seen, hypothesis testing and confidence intervals are very related. For a simple null hypothesis, a bootstrap hypothesis test p-value can be calculated by finding the minimum \\(\\alpha\\) for which the \\((1-\\alpha)\\) CI does not contain the null hypothesis value. You showed this on your homework. The general approach is to calculate a test statistic based on the observed data. Then the null distribution of this statistic is approximated by forming bootstrap test statistics under the scenario that the null hypothesis is true. This can often be accomplished because the \\(\\hat{\\theta}\\) estimated from the observed data is the population parameter from the bootstrap distribution. 50.12 Example: t-test Suppose \\(X_1, X_2, \\ldots, X_m {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_X\\) and \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_Y\\). We wish to test \\(H_0: \\mu(F_X) = \\mu(F_Y)\\) vs \\(H_1: \\mu(F_X) \\not= \\mu(F_Y)\\). Suppose that we know \\(\\sigma^2(F_X) = \\sigma^2(F_Y)\\) (if not, it is straightforward to adjust the proecure below). Our test statistic is \\[ t = \\frac{\\overline{x} - \\overline{y}}{\\sqrt{\\left(\\frac{1}{m} + \\frac{1}{n}\\right) s^2}} \\] where \\(s^2\\) is the pooled sample variance. Note that the bootstrap distributions are such that \\(\\mu(\\hat{F}_{X^{*}}) = \\overline{x}\\) and \\(\\mu(\\hat{F}_{Y^{*}}) = \\overline{y}\\). Thus we want to center the bootstrap t-statistics about these known means. Specifically, for a bootstrap data set \\(x^{*} = (x_1^{*}, x_2^{*}, \\ldots, x_m^{*})^T\\) and \\(y^{*} = (y_1^{*}, y_2^{*}, \\ldots, y_n^{*})^T\\), we form null t-statistic \\[ t^{*} = \\frac{\\overline{x}^{*} - \\overline{y}^{*} - (\\overline{x} - \\overline{y})}{\\sqrt{\\left(\\frac{1}{m} + \\frac{1}{n}\\right) s^{2*}}} \\] where again \\(s^{2*}\\) is the pooled sample variance. In order to obtain a p-value, we calculate \\(t^{*(b)}\\) for \\(b=1, 2, \\ldots, B\\) bootstrap data sets. The p-value of \\(t\\) is then the proportion of bootstrap statistics as or more extreme than the observed statistic: \\[ \\mbox{p-value}(t) = \\frac{1}{B} \\sum_{b=1}^{B} 1\\left(|t^{*(b)}| \\geq |t|\\right). \\] 50.13 Parametric Bootstrap Suppose \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_\\theta\\) for some parametric \\(F_\\theta\\). We form estimate \\(\\hat{\\theta}\\), but we don’t have a known sampling distribution we can use to do inference with \\(\\hat{\\theta}\\). The parametric bootstrap generates bootstrap data sets from \\(F_{\\hat{\\theta}}\\) rather than from the edf. It proceeds as we outlined above for these bootstrap data sets. 50.14 Example: Exponential Data In the homework, you will be performing a bootstrap t-test of the mean and a bootstrap percentile CI of the median for the following Exponential(\\(\\lambda\\)) data: &gt; set.seed(1111) &gt; pop.mean &lt;- 2 &gt; X &lt;- matrix(rexp(1000*30, rate=1/pop.mean), nrow=1000, ncol=30) Let’s construct a pivotal bootstrap CI of the median here instead. &gt; # population median 2*log(2) &gt; pop_med &lt;- qexp(0.5, rate=1/pop.mean); pop_med [1] 1.386294 &gt; &gt; obs_meds &lt;- apply(X, 1, median) &gt; plot(density(obs_meds, adj=1.5), main=&quot; &quot;); abline(v=pop_med) Some embarrassingly inefficient code to calculate bootstrap medians. &gt; B &lt;- 1000 &gt; boot_meds &lt;- matrix(0, nrow=1000, ncol=B) &gt; &gt; for(b in 1:B) { + idx &lt;- sample(1:30, replace=TRUE) + boot_meds[,b] &lt;- apply(X[,idx], 1, median) + } Plot the bootstrap medians. &gt; plot(density(obs_meds, adj=1.5), main=&quot; &quot;); abline(v=pop_med) &gt; lines(density(as.vector(boot_meds[1:4,]), adj=1.5), col=&quot;red&quot;) &gt; lines(density(as.vector(boot_meds), adj=1.5), col=&quot;blue&quot;) Compare sampling distribution of \\(\\hat{\\theta}-\\theta\\) to \\(\\hat{\\theta}^{*} - \\hat{\\theta}\\). &gt; v &lt;- obs_meds - pop_med &gt; w &lt;- as.vector(boot_meds - obs_meds) &gt; qqplot(v, w, pch=20); abline(0,1) Does a 95% bootstrap pivotal interval provide coverage? &gt; ci_lower &lt;- apply(boot_meds, 1, quantile, probs=0.975) &gt; ci_upper &lt;- apply(boot_meds, 1, quantile, probs=0.025) &gt; &gt; ci_lower &lt;- 2*obs_meds - ci_lower &gt; ci_upper &lt;- 2*obs_meds - ci_upper &gt; &gt; ci_lower[1]; ci_upper[1] [1] 0.8958224 [1] 2.113859 &gt; &gt; cover &lt;- (pop_med &gt;= ci_lower) &amp; (pop_med &lt;= ci_upper) &gt; mean(cover) [1] 0.809 &gt; &gt; # :-( Let’s check the bootstrap variances. &gt; sampling_var &lt;- var(obs_meds) &gt; boot_var &lt;- apply(boot_meds, 1, var) &gt; plot(density(boot_var, adj=1.5), main=&quot; &quot;) &gt; abline(v=sampling_var) We repeated this simulation over a range of \\(n\\) and \\(B\\). n B coverage avg CI width 1e+02 1000 0.868 0.7805404 1e+02 2000 0.872 0.7882278 1e+02 4000 0.865 0.7852837 1e+02 8000 0.883 0.7817222 1e+03 1000 0.923 0.2465840 1e+03 2000 0.909 0.2477463 1e+03 4000 0.915 0.2475550 1e+03 8000 0.923 0.2458167 1e+04 1000 0.935 0.0781421 1e+04 2000 0.937 0.0784541 1e+04 4000 0.942 0.0784559 1e+04 8000 0.948 0.0785591 1e+05 1000 0.949 0.0246918 1e+05 2000 0.942 0.0246938 "],
["permutation-methods.html", "51 Permutation Methods 51.1 Rationale 51.2 Permutation Test 51.3 Wilcoxon Rank Sum Test 51.4 Wilcoxon Signed Rank-Sum Test 51.5 Examples 51.6 Permutation t-test", " 51 Permutation Methods 51.1 Rationale Permutation methods are useful for testing hypotheses about equality of distributions. Observations can be permuted among populations to simulate the case where the distributions are equivalent. Many permutation methods only depend on the ranks of the data, so they are a class of robust methods for performing hypothesis tests. However, the types of hypotheses that can be tested are limited. 51.2 Permutation Test Suppose \\(X_1, X_2, \\ldots, X_m {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_X\\) and \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_Y\\). We wish to test \\(H_0: F_X = F_Y\\) vs \\(H_1: F_X \\not= F_Y\\). Consider a general test statistic \\(S = S(X_1, X_2, \\ldots, X_m, Y_1, Y_2, \\ldots, Y_n)\\) so that the larger \\(S\\) is the more evidence there is against the null hypothesis. Under the null hypothesis, any reordering of these values, where \\(m\\) are randomly assigned to the “\\(X\\)” population and \\(n\\) are assigned to the “\\(Y\\)” population, should be equivalently distributed. For \\(B\\) permutations (possibly all unique permutations), we calculate \\[ S^{*(b)} = S\\left(Z^{*(b)}_1, Z^{*(b)}_2, \\ldots, Z^{*(b)}_m, Z^{*(b)}_{m+1}, \\ldots, Z^{*(b)}_{m+n}\\right) \\] where \\(Z^{*(b)}_1, Z^{*(b)}_2, \\ldots, Z^{*(b)}_m, Z^{*(b)}_{m+1}, \\ldots, Z^{*(b)}_{m+n}\\) is a random permutation of the values \\(X_1, X_2, \\ldots, X_m, Y_1, Y_2, \\ldots, Y_n\\). Example permutation in R: &gt; z &lt;- c(x, y) &gt; zstar &lt;- sample(z, replace=FALSE) The p-value is calculated as proportion of permutations where the resulting permutation statistic exceeds the observed statistics: \\[ \\mbox{p-value}(s) = \\frac{1}{B} \\sum_{b=1}^{B} 1\\left(S^{*(b)} \\geq S\\right). \\] This can be (1) an exact calculation where all permutations are considered, (2) a Monte Carlo approximation where \\(B\\) random permutations are considered, or (3) a large \\(\\min(m, n)\\) calculation where an asymptotic probabilistic approximation is used. 51.3 Wilcoxon Rank Sum Test Also called the Mann-Whitney-Wilcoxon test. Consider the ranks of the data as a whole, \\(X_1, X_2, \\ldots, X_m, Y_1, Y_2, \\ldots, Y_n\\), where \\(r(X_i)\\) is the rank of \\(X_i\\) and \\(r(Y_j)\\) is the rank of \\(Y_j\\). Note that \\(r(\\cdot) \\in \\{1, 2, \\ldots, m+n\\}\\). The smallest value is such that \\(r(X_i)=1\\) or \\(r(Y_j)=1\\), the next smallest value maps to 2, etc. Note that \\[ \\sum_{i=1}^m r(X_i) + \\sum_{j=1}^n r(Y_j) = \\frac{(m+n)(m+n+1)}{2}. \\] The statistic \\(W\\) is calculated by: \\[ \\begin{aligned} &amp; R_X = \\sum_{i=1}^m r(X_i) &amp; R_Y = \\sum_{j=1}^n r(Y_j) \\\\ &amp; W_X = R_X - \\frac{m(m+1)}{2} &amp; W_Y = R_Y - \\frac{n(n+1)}{2} \\\\ &amp; W = \\min(W_X, W_Y) &amp; \\end{aligned} \\] In this case, the smaller \\(W\\) is, the more significant it is. Note that \\(mn-W = \\max(W_X, W_Y)\\), so we just as well could utilize large \\(\\max(W_X, W_Y)\\) as a test statistic. 51.4 Wilcoxon Signed Rank-Sum Test The Wilcoxon signed rank test is similar to the Wilcoxon two-sample test, except here we have paired observations \\((X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\). An example is an individual’s clinical measurement before (\\(X\\)) and after (\\(Y\\)) treatment. In order to test the hypothesis, we calculate \\(r(X_i, Y_i) = |Y_i - X_i|\\) and also \\(s(X_i, Y_i) = \\operatorname{sign}(Y_i - X_i)\\). The test statistic is \\(|W|\\) where \\[ W = \\sum_{i=1}^n r(X_i, Y_i) s(X_i, Y_i). \\] Both of these tests can be carried out using the wilcox.test() function in R. wilcox.test(x, y = NULL, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), mu = 0, paired = FALSE, exact = NULL, correct = TRUE, conf.int = FALSE, conf.level = 0.95, ...) 51.5 Examples Same population mean and variance. &gt; x &lt;- rnorm(100, mean=1) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 4480, p-value = 0.2043 alternative hypothesis: true location shift is not equal to 0 &gt; qqplot(x, y); abline(0,1) Same population mean and variance. Large sample size. &gt; x &lt;- rnorm(10000, mean=1) &gt; y &lt;- rexp(10000, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 53311735, p-value = 4.985e-16 alternative hypothesis: true location shift is not equal to 0 &gt; qqplot(x, y); abline(0,1) Same mean, very different variances. &gt; x &lt;- rnorm(100, mean=1, sd=0.01) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 6579, p-value = 0.0001148 alternative hypothesis: true location shift is not equal to 0 &gt; qqplot(x, y); abline(0,1) Same variances, different means. &gt; x &lt;- rnorm(100, mean=2) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 7836, p-value = 4.261e-12 alternative hypothesis: true location shift is not equal to 0 &gt; qqplot(x, y); abline(0,1) Same population mean and variance. &gt; x &lt;- rnorm(100, mean=1) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y, paired=TRUE) Wilcoxon signed rank test with continuity correction data: x and y V = 2394, p-value = 0.6536 alternative hypothesis: true location shift is not equal to 0 &gt; hist(y-x) Same population mean and variance. Large sample size. &gt; x &lt;- rnorm(10000, mean=1) &gt; y &lt;- rexp(10000, rate=1) &gt; wilcox.test(x, y, paired=TRUE) Wilcoxon signed rank test with continuity correction data: x and y V = 26769956, p-value = 9.23e-10 alternative hypothesis: true location shift is not equal to 0 &gt; hist(y-x) 51.6 Permutation t-test As above, suppose \\(X_1, X_2, \\ldots, X_m {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_X\\) and \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_Y\\), and we wish to test \\(H_0: F_X = F_Y\\) vs \\(H_1: F_X \\not= F_Y\\). However, suppose we additionally know that \\({\\operatorname{Var}}(X) = {\\operatorname{Var}}(Y)\\). We can use a t-statistic to test this hypothesis: \\[ t = \\frac{\\overline{x} - \\overline{y}}{\\sqrt{\\left(\\frac{1}{m} + \\frac{1}{n}\\right) s^2}} \\] where \\(s^2\\) is the pooled sample variance. To obtain the null distribution, we randomly permute the observations to assign \\(m\\) data points to the \\(X\\) sample and \\(n\\) to the \\(Y\\) sample. This yields permutation data set \\(x^{*} = (x_1^{*}, x_2^{*}, \\ldots, x_m^{*})^T\\) and \\(y^{*} = (y_1^{*}, y_2^{*}, \\ldots, y_n^{*})^T\\). We form null t-statistic \\[ t^{*} = \\frac{\\overline{x}^{*} - \\overline{y}^{*}}{\\sqrt{\\left(\\frac{1}{m} + \\frac{1}{n}\\right) s^{2*}}} \\] where again \\(s^{2*}\\) is the pooled sample variance. In order to obtain a p-value, we calculate \\(t^{*(b)}\\) for \\(b=1, 2, \\ldots, B\\) permutation data sets. The p-value of \\(t\\) is then the proportion of permutation statistics as or more extreme than the observed statistic: \\[ \\mbox{p-value}(t) = \\frac{1}{B} \\sum_{b=1}^{B} 1\\left(|t^{*(b)}| \\geq |t|\\right). \\] "],
["goodness-of-fit.html", "52 Goodness of Fit 52.1 Rationale 52.2 Chi-Square GoF Test 52.3 Example: Hardy-Weinberg 52.4 Kolmogorov–Smirnov Test 52.5 One Sample KS Test 52.6 Two Sample KS Test 52.7 Example: Exponential vs Normal", " 52 Goodness of Fit 52.1 Rationale Sometimes we want to figure out which probability distribution is a reasonable model for the data. This is related to nonparametric inference in that we wish to go from being in a nonparametric framework to a parametric framework. Goodness of fit (GoF) tests allow one to perform a hypothesis test of how well a particular parametric probability model explains variation observed in a data set. 52.2 Chi-Square GoF Test Suppose we have data generating process \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\) for some probability distribution \\(F\\). We wish to test \\(H_0: F \\in \\{F_{{\\boldsymbol{\\theta}}}: {\\boldsymbol{\\theta}}\\in \\boldsymbol{\\Theta} \\}\\) vs \\(H_1: \\mbox{not } H_0\\). Suppose that \\(\\boldsymbol{\\Theta}\\) is \\(d\\)-dimensional. Divide the support of \\(\\{F_{{\\boldsymbol{\\theta}}}: {\\boldsymbol{\\theta}}\\in \\boldsymbol{\\Theta} \\}\\) into \\(k\\) bins \\(I_1, I_2, \\ldots, I_k\\). For \\(j=1, 2, \\ldots, k\\), calculate \\[ q_j({\\boldsymbol{\\theta}}) = \\int_{I_j} dF_{{\\boldsymbol{\\theta}}}(x). \\] Suppose we observe data \\(x_1, x_2, \\ldots, x_n\\). For \\(j = 1, 2, \\ldots, k\\), let \\(n_j\\) be the number of values \\(x_i \\in I_j\\). Let \\(\\tilde{\\theta}_1, \\tilde{\\theta}_2, \\ldots, \\tilde{\\theta}_d\\) be the values that maximize the multinomial likelihood \\[ \\prod_{j=1}^k q_j({\\boldsymbol{\\theta}})^{n_j}. \\] Form GoF statistic \\[ s({\\boldsymbol{x}}) = \\sum_{j=1}^k \\frac{\\left(n_j - n q_j\\left(\\tilde{{\\boldsymbol{\\theta}}} \\right) \\right)^2}{n q_j\\left(\\tilde{{\\boldsymbol{\\theta}}} \\right)} \\] When \\(H_0\\) is true, \\(S \\sim \\chi^2_v\\) where \\(v = k - d - 1\\). The p-value is calculated by \\(\\Pr(S^* \\geq s({\\boldsymbol{x}}))\\) where \\(S^* \\sim \\chi^2_{k-d-1}\\). 52.3 Example: Hardy-Weinberg Suppose at your favorite SNP, we observe genotypes from 100 randomly sampled individuals as follows: AA AT TT 28 60 12 If we code these genotypes as 0, 1, 2, testing for Hardy-Weinberg equilibrium is equivalent to testing whether \\(X_1, X_2, \\ldots, X_{100} {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Binomial}(2, \\theta)\\) for some unknown allele frequency of T, \\(\\theta\\). The parameter dimension is such that \\(d=1\\). We will also set \\(k=3\\), where each bin is a genotype. Therefore, we have \\(n_1 = 28\\), \\(n_2 = 60\\), and \\(n_3 = 12\\). Also, \\[ q_1(\\theta) = (1-\\theta)^2, \\ \\ q_2(\\theta) = 2 \\theta (1-\\theta), \\ \\ q_3(\\theta) = \\theta^2. \\] Forming the multinomial likelihood under these bin probabilities, we find \\(\\tilde{\\theta} = (n_2 + 2n_3)/(2n)\\). The degrees of freedom of the \\(\\chi^2_v\\) null distribution is \\(v = k - d - 1 = 3 - 1 - 1 = 1\\). Let’s carry out the test in R. &gt; n &lt;- 100 &gt; nj &lt;- c(28, 60, 12) &gt; &gt; # parameter estimates &gt; theta &lt;- (nj[2] + 2*nj[3])/(2*n) &gt; qj &lt;- c((1-theta)^2, 2*theta*(1-theta), theta^2) &gt; &gt; # gof statistic &gt; s &lt;- sum((nj - n*qj)^2 / (n*qj)) &gt; &gt; # p-value &gt; 1-pchisq(s, df=1) [1] 0.02059811 52.4 Kolmogorov–Smirnov Test The KS test can be used to compare a sample of data to a particular distribution, or to compare two samples of data. The former is a parametric GoF test, and the latter is a nonparametric test of equal distributions. 52.5 One Sample KS Test Suppose we have data generating process \\(X_1, X_2, \\ldots, X_n \\sim F\\) for some probability distribution \\(F\\). We wish to test \\(H_0: F = F_{{\\boldsymbol{\\theta}}}\\) vs \\(H_1: F \\not= F_{{\\boldsymbol{\\theta}}}\\) for some parametric distribution \\(F_{{\\boldsymbol{\\theta}}}\\). For observed data \\(x_1, x_2, \\ldots, x_n\\) we form the edf \\(\\hat{F}_{{\\boldsymbol{x}}}\\) and test-statistic \\[ D({\\boldsymbol{x}}) = \\sup_{z} \\left| \\hat{F}_{{\\boldsymbol{x}}}(z) - F_{{\\boldsymbol{\\theta}}}(z) \\right|. \\] The null distribution of this test can be approximated based on a stochastic process called the Brownian bridge. 52.6 Two Sample KS Test Suppose \\(X_1, X_2, \\ldots, X_m {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_X\\) and \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_Y\\). We wish to test \\(H_0: F_X = F_Y\\) vs \\(H_1: F_X \\not= F_Y\\). For observed data \\(x_1, x_2, \\ldots, x_m\\) and \\(y_1, y_2, \\ldots, y_n\\) we form the edf’s \\(\\hat{F}_{{\\boldsymbol{x}}}\\) and \\(\\hat{F}_{\\boldsymbol{y}}\\). We then form test-statistic \\[ D({\\boldsymbol{x}},\\boldsymbol{y}) = \\sup_{z} \\left| \\hat{F}_{{\\boldsymbol{x}}}(z) - \\hat{F}_{\\boldsymbol{y}}(z) \\right|. \\] The null distribution of this statistic can be approximated using results on edf’s. Both of these tests can be carried out using the ks.test() function in R. ks.test(x, y, ..., alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), exact = NULL) 52.7 Example: Exponential vs Normal Two sample KS test. &gt; x &lt;- rnorm(100, mean=1) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 5021, p-value = 0.9601 alternative hypothesis: true location shift is not equal to 0 &gt; ks.test(x, y) Two-sample Kolmogorov-Smirnov test data: x and y D = 0.19, p-value = 0.0541 alternative hypothesis: two-sided &gt; qqplot(x, y); abline(0,1) One sample KS tests. &gt; ks.test(x=x, y=&quot;pnorm&quot;) One-sample Kolmogorov-Smirnov test data: x D = 0.41398, p-value = 2.554e-15 alternative hypothesis: two-sided &gt; &gt; ks.test(x=x, y=&quot;pnorm&quot;, mean=1) One-sample Kolmogorov-Smirnov test data: x D = 0.068035, p-value = 0.7436 alternative hypothesis: two-sided Standardize (mean center, sd scale) the observations before comparing to a Normal(0,1) distribution. &gt; ks.test(x=((x-mean(x))/sd(x)), y=&quot;pnorm&quot;) One-sample Kolmogorov-Smirnov test data: ((x - mean(x))/sd(x)) D = 0.05896, p-value = 0.8778 alternative hypothesis: two-sided &gt; &gt; ks.test(x=((y-mean(y))/sd(y)), y=&quot;pnorm&quot;) One-sample Kolmogorov-Smirnov test data: ((y - mean(y))/sd(y)) D = 0.14439, p-value = 0.03092 alternative hypothesis: two-sided "],
["method-of-moments.html", "53 Method of Moments 53.1 Rationale 53.2 Definition 53.3 Example: Normal 53.4 Exploring Goodness of Fit", " 53 Method of Moments 53.1 Rationale Suppose that \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\). By the strong law of large numbers we have, as \\(n \\rightarrow \\infty\\) \\[ \\frac{\\sum_{i=1}^n X_i^k}{n} \\stackrel{\\text{a.s.}}{\\longrightarrow} {\\operatorname{E}}_{F}\\left[X^k\\right] \\] when \\({\\operatorname{E}}_{F}\\left[X^k\\right]\\) exists. This means that we can nonparametrically estimate the moments of a distribution. Also, in the parametric setting, these moments can be used to form parameter estimates. 53.2 Definition Suppose that \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}\\) where \\({\\boldsymbol{\\theta}}\\) is \\(d\\)-dimensional. Calculate moments \\({\\operatorname{E}}\\left[X^k\\right]\\) for \\(k = 1, 2, \\ldots, d&#39;\\) where \\(d&#39; \\geq d\\). For each parameter \\(j = 1, 2, \\ldots, d\\), solve for \\(\\theta_j\\) in terms of \\({\\operatorname{E}}\\left[X^k\\right]\\) for \\(k = 1, 2, \\ldots, d&#39;\\). The method of moments estimator of \\(\\theta_j\\) is formed by replacing the function of moments \\({\\operatorname{E}}\\left[X^k\\right]\\) that equals \\(\\theta_j\\) with the empirical moments \\(\\sum_{i=1}^n X_i^k / n\\). 53.3 Example: Normal For a \\(\\mbox{Normal}(\\mu, \\sigma^2)\\) distribution, we have \\[ {\\operatorname{E}}[X] = \\mu \\] \\[ {\\operatorname{E}}\\left[X^2\\right] = \\sigma^2 + \\mu^2 \\] Solving for \\(\\mu\\) and \\(\\sigma^2\\), we have \\(\\mu = {\\operatorname{E}}[X]\\) and \\(\\sigma^2 = {\\operatorname{E}}[X^2] - {\\operatorname{E}}[X]^2\\). This yields method of moments estimators \\[ \\tilde{\\mu} = \\frac{\\sum_{i=1}^n X_i}{n}, \\ \\ \\ \\tilde{\\sigma}^2 = \\frac{\\sum_{i=1}^n X_i^2}{n} - \\left[\\frac{\\sum_{i=1}^n X_i}{n}\\right]^2. \\] 53.4 Exploring Goodness of Fit As mentioned above, moments can be nonparametrically estimated. At the same time, for a given parametric distribution, these moments can also be written in terms of the parameters. For example, consider a single parameter exponential family distribution. The variance is going to be defined in terms of the parameter. At the same time, we can estimate variance through the empirical moments \\[ \\frac{\\sum_{i=1}^n X_i^2}{n} - \\left[\\frac{\\sum_{i=1}^n X_i}{n}\\right]^2. \\] In the scenario where several sets of variables are measured, the MLEs of the variance in terms of the single parameter can be compared to the moment estimates of variance to assess goodness of fit of that distribution. "],
["types-of-models.html", "54 Types of Models 54.1 Probabilistic Models 54.2 Multivariate Models 54.3 Variables 54.4 Statistical Model 54.5 Parametric vs Nonparametric 54.6 Simple Linear Regression 54.7 Ordinary Least Squares 54.8 Generalized Least Squares 54.9 Matrix Form of Linear Models 54.10 Least Squares Regression 54.11 Generalized Linear Models 54.12 Generalized Additive Models 54.13 Some Trade-offs 54.14 Bias and Variance", " 54 Types of Models 54.1 Probabilistic Models So far we have covered inference of paramters that quantify a population of interest. This is called inference of probabilistic models. 54.2 Multivariate Models Some of the probabilistic models we considered involve calculating conditional probabilities such as \\(\\Pr({\\boldsymbol{Z}}| {\\boldsymbol{X}}; {\\boldsymbol{\\theta}})\\) or \\(\\Pr({\\boldsymbol{\\theta}}| {\\boldsymbol{X}})\\). It is often the case that we would like to build a model that explains the variation of one variable in terms of other variables. Statistical modeling typically refers to this goal. 54.3 Variables Let’s suppose our does comes in the form \\(({\\boldsymbol{X}}_1, Y_1), ({\\boldsymbol{X}}_2, Y_2), \\ldots, ({\\boldsymbol{X}}_n, Y_n) \\sim F\\). We will call \\({\\boldsymbol{X}}_i = (X_{i1}, X_{i2}, \\ldots, X_{ip}) \\in \\mathbb{R}_{1 \\times p}\\) the explanatory variables and \\(Y_i \\in \\mathbb{R}\\) the dependent variable or response variable. We can collect all variables as matrices \\[ {\\boldsymbol{Y}}_{n \\times 1} \\ \\mbox{ and } \\ {\\boldsymbol{X}}_{n \\times p}\\] where each row is a unique observation. 54.4 Statistical Model Statistical models are concerned with how variables are dependent. The most general model would be to infer \\[ \\Pr(Y | {\\boldsymbol{X}}) = h({\\boldsymbol{X}}) \\] where we would specifically study the form of \\(h(\\cdot)\\) to understand how \\(Y\\) is dependent on \\({\\boldsymbol{X}}\\). A more modest goal is to infer the transformed conditional expecation \\[ g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = h({\\boldsymbol{X}}) \\] which sometimes leads us back to an estimate of \\(\\Pr(Y | {\\boldsymbol{X}})\\). 54.5 Parametric vs Nonparametric A parametric model is a pre-specified form of \\(h(X)\\) whose terms can be characterized by a formula and interpreted. This usually involves parameters on which inference can be performed, such as coefficients in a linear model. A nonparametric model is a data-driven form of \\(h(X)\\) that is often very flexible and is not easily expressed or intepreted. A nonparametric model often does not include parameters on which we can do inference. 54.6 Simple Linear Regression For random variables \\((X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\), simple linear regression estimates the model \\[ Y_i = \\beta_1 + \\beta_2 X_i + E_i \\] where \\({\\operatorname{E}}[E_i] = 0\\), \\({\\operatorname{Var}}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). Note that in this model \\({\\operatorname{E}}[Y | X] = \\beta_1 + \\beta_2 X.\\) 54.7 Ordinary Least Squares Ordinary least squares (OLS) estimates the model \\[ \\begin{aligned} Y_i &amp; = \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + E_i \\\\ &amp; = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}+ E_i \\end{aligned} \\] where \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). Note that typically \\(X_{i1} = 1\\) for all \\(i\\) so that \\(\\beta_1 X_{i1} = \\beta_1\\) serves as the intercept. 54.8 Generalized Least Squares Generalized least squares (GLS) assumes the same model as OLS, except it allows for heteroskedasticity and covariance among the \\(E_i\\). Specifically, it is assumed that \\({\\boldsymbol{E}}= (E_1, \\ldots, E_n)^T\\) is distributed as \\[ {\\boldsymbol{E}}_{n \\times 1} \\sim (\\boldsymbol{0}, {\\boldsymbol{\\Sigma}}) \\] where \\(\\boldsymbol{0}\\) is the expected value \\({\\boldsymbol{\\Sigma}}= (\\sigma_{ij})\\) is the \\(n \\times n\\) symmetric covariance matrix. 54.9 Matrix Form of Linear Models We can write the models as \\[ {\\boldsymbol{Y}}_{n \\times 1} = {\\boldsymbol{X}}_{n \\times p} {\\boldsymbol{\\beta}}_{p \\times 1} + {\\boldsymbol{E}}_{n \\times 1} \\] where simple linear regression, OLS, and GLS differ in the value of \\(p\\) or the distribution of the \\(E_i\\). We can also write the conditional expecation and covariance as \\[ {\\operatorname{E}}[{\\boldsymbol{Y}}| {\\boldsymbol{X}}] = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}, \\ {\\operatorname{Cov}}({\\boldsymbol{Y}}| {\\boldsymbol{X}}) = {\\boldsymbol{\\Sigma}}. \\] 54.10 Least Squares Regression In simple linear regression, OLS, and GLS, the \\({\\boldsymbol{\\beta}}\\) parameters are fit by minimizing the sum of squares between \\({\\boldsymbol{Y}}\\) and \\({\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\). Fitting these models by “least squares” satisfies two types of optimality: Gauss-Markov Theorem Maximum likelihood estimate when in addition \\({\\boldsymbol{E}}\\sim \\mbox{MVN}_n(\\boldsymbol{0}, {\\boldsymbol{\\Sigma}})\\) Details will follow on these. 54.11 Generalized Linear Models The generalized linear model (GLM) builds from OLS and GLS to allow the response variable to be distributed according to an exponential family distribution. Suppose that \\(\\eta(\\theta)\\) is function of the expected value into the natural parameter. The estimated model is \\[ \\eta\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\] which is fit by maximized likelihood estimation. 54.12 Generalized Additive Models Next week, we will finally arrive at inferring semiparametric models where \\(Y | {\\boldsymbol{X}}\\) is distributed according to an exponential family distribution. The models, which are called generalized additive models (GAMs), will be of the form \\[ \\eta\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = \\sum_{j=1}^p \\sum_{k=1}^d h_k(X_{j}) \\] where \\(\\eta\\) is the canonical link function and the \\(h_k(\\cdot)\\) functions are very flexible. 54.13 Some Trade-offs There are several important trade-offs encountered in statistical modeling: Bias vs variance Accuracy vs computational time Flexibility vs intepretability These are not mutually exclusive phenomena. 54.14 Bias and Variance Suppose we estimate \\(Y = h({\\boldsymbol{X}}) + E\\) by some \\(\\hat{Y} = \\hat{h}({\\boldsymbol{X}})\\). The following bias-variance trade-off exists: \\[ \\begin{aligned} {\\operatorname{E}}\\left[\\left(Y - \\hat{Y}\\right)^2\\right] &amp; = {\\rm E}\\left[\\left(h({\\boldsymbol{X}}) + E - \\hat{h}({\\boldsymbol{X}})\\right)^2\\right] \\\\ \\ &amp; = {\\rm E}\\left[\\left(h({\\boldsymbol{X}}) - \\hat{h}({\\boldsymbol{X}})\\right)^2\\right] + {\\rm Var}(E) \\\\ \\ &amp; = \\left(h({\\boldsymbol{X}}) - {\\rm E}[\\hat{h}({\\boldsymbol{X}})]\\right)^2 + {\\rm Var}\\left(\\hat{h}({\\boldsymbol{X}})\\right)^2 + {\\rm Var}(E) \\\\ \\ &amp; = \\mbox{bias}^2 + \\mbox{variance} + {\\rm Var}(E) \\end{aligned} \\] "],
["motivating-examples.html", "55 Motivating Examples 55.1 Sample Correlation 55.2 Example: Hand Size Vs. Height 55.3 Cor. of Hand Size and Height 55.4 L/R Hand Sizes 55.5 Correlation of Hand Sizes 55.6 Davis Data 55.7 Height and Weight 55.8 Correlation of Height and Weight 55.9 Correlation Among Females 55.10 Correlation Among Males", " 55 Motivating Examples 55.1 Sample Correlation Least squares regression “modelizes” correlation. Suppose we observe \\(n\\) pairs of data \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\). Their sample correlation is \\[\\begin{eqnarray} r_{xy} &amp; = &amp; \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2 \\sum_{i=1}^n (y_i - \\overline{y})^2}} \\\\ \\ &amp; = &amp; \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{(n-1) s_x s_y} \\end{eqnarray}\\] where \\(s_x\\) and \\(s_y\\) are the sample standard deviations of each measured variable. 55.2 Example: Hand Size Vs. Height &gt; library(&quot;MASS&quot;) &gt; data(&quot;survey&quot;, package=&quot;MASS&quot;) &gt; head(survey) Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height 1 Female 18.5 18.0 Right R on L 92 Left Some Never 173.00 2 Male 19.5 20.5 Left R on L 104 Left None Regul 177.80 3 Male 18.0 13.3 Right L on R 87 Neither None Occas NA 4 Male 18.8 18.9 Right R on L NA Neither None Never 160.00 5 Male 20.0 20.0 Right Neither 35 Right Some Never 165.00 6 Female 18.0 17.7 Right L on R 64 Right Some Never 172.72 M.I Age 1 Metric 18.250 2 Imperial 17.583 3 &lt;NA&gt; 16.917 4 Metric 20.333 5 Metric 23.667 6 Imperial 21.000 &gt; ggplot(data = survey, mapping=aes(x=Wr.Hnd, y=Height)) + + geom_point() + geom_vline(xintercept=mean(survey$Wr.Hnd, na.rm=TRUE)) + + geom_hline(yintercept=mean(survey$Height, na.rm=TRUE)) 55.3 Cor. of Hand Size and Height &gt; cor.test(x=survey$Wr.Hnd, y=survey$Height) Pearson&#39;s product-moment correlation data: survey$Wr.Hnd and survey$Height t = 10.792, df = 206, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.5063486 0.6813271 sample estimates: cor 0.6009909 55.4 L/R Hand Sizes &gt; ggplot(data = survey) + + geom_point(aes(x=Wr.Hnd, y=NW.Hnd)) 55.5 Correlation of Hand Sizes &gt; cor.test(x=survey$Wr.Hnd, y=survey$NW.Hnd) Pearson&#39;s product-moment correlation data: survey$Wr.Hnd and survey$NW.Hnd t = 45.712, df = 234, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.9336780 0.9597816 sample estimates: cor 0.9483103 55.6 Davis Data &gt; library(&quot;car&quot;) &gt; data(&quot;Davis&quot;, package=&quot;car&quot;) Warning in data(&quot;Davis&quot;, package = &quot;car&quot;): data set &#39;Davis&#39; not found &gt; htwt &lt;- tbl_df(Davis) &gt; htwt[12,c(2,3)] &lt;- htwt[12,c(3,2)] &gt; head(htwt) # A tibble: 6 x 5 sex weight height repwt repht &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 M 77 182 77 180 2 F 58 161 51 159 3 F 53 161 54 158 4 M 68 177 70 175 5 F 59 157 59 155 6 M 76 170 76 165 55.7 Height and Weight &gt; ggplot(htwt) + + geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) + + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 55.8 Correlation of Height and Weight &gt; cor.test(x=htwt$height, y=htwt$weight) Pearson&#39;s product-moment correlation data: htwt$height and htwt$weight t = 17.04, df = 198, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.7080838 0.8218898 sample estimates: cor 0.7710743 55.9 Correlation Among Females &gt; htwt %&gt;% filter(sex==&quot;F&quot;) %&gt;% + cor.test(~ height + weight, data = .) Pearson&#39;s product-moment correlation data: height and weight t = 6.2801, df = 110, p-value = 6.922e-09 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.3627531 0.6384268 sample estimates: cor 0.5137293 55.10 Correlation Among Males &gt; htwt %&gt;% filter(sex==&quot;M&quot;) %&gt;% + cor.test(~ height + weight, data = .) Pearson&#39;s product-moment correlation data: height and weight t = 5.9388, df = 86, p-value = 5.922e-08 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.3718488 0.6727460 sample estimates: cor 0.5392906 Why are the stratified correlations lower? "],
["simple-linear-regression-1.html", "56 Simple Linear Regression 56.1 Definition 56.2 Rationale 56.3 Setup 56.4 Line Minimizing Squared Error 56.5 Least Squares Solution 56.6 Visualizing Least Squares Line 56.7 Example: Height and Weight 56.8 Calculate the Line Directly 56.9 Plot the Line 56.10 Observed Data, Fits, and Residuals 56.11 Proportion of Variation Explained", " 56 Simple Linear Regression 56.1 Definition For random variables \\((X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\), simple linear regression estimates the model \\[ Y_i = \\beta_1 + \\beta_2 X_i + E_i \\] where \\({\\operatorname{E}}[E_i] = 0\\), \\({\\operatorname{Var}}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). 56.2 Rationale Least squares linear regression is one of the simplest and most useful modeling systems for building a model that explains the variation of one variable in terms of other variables. It is simple to fit, it satisfies some optimality criteria, and it is straightforward to check assumptions on the data so that statistical inference can be performed. 56.3 Setup Suppose that we have observed \\(n\\) pairs of data \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\). Least squares linear regression models variation of the response variable \\(y\\) in terms of the explanatory variable \\(x\\) in the form of \\(\\beta_1 + \\beta_2 x\\), where \\(\\beta_1\\) and \\(\\beta_2\\) are chosen to satisfy a least squares optimization. 56.4 Line Minimizing Squared Error The least squares regression line is formed from the value of \\(\\beta_1\\) and \\(\\beta_2\\) that minimize: \\[\\sum_{i=1}^n \\left( y_i - \\beta_1 - \\beta_2 x_i \\right)^2.\\] For a given set of data, there is a unique solution to this minimization as long as there are at least two unique values among \\(x_1, x_2, \\ldots, x_n\\). Let \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_2}\\) be the values that minimize this sum of squares. 56.5 Least Squares Solution These values are: \\[\\hat{\\beta}_2 = r_{xy} \\frac{s_y}{s_x}\\] \\[\\hat{\\beta}_1 = \\overline{y} - \\hat{\\beta}_2 \\overline{x}\\] These values have a useful interpretation. 56.6 Visualizing Least Squares Line 56.7 Example: Height and Weight &gt; ggplot(data=htwt, mapping=aes(x=height, y=weight)) + + geom_point(size=2, alpha=0.5) + + geom_smooth(method=&quot;lm&quot;, se=FALSE, formula=y~x) 56.8 Calculate the Line Directly &gt; beta2 &lt;- cor(htwt$height, htwt$weight) * + sd(htwt$weight) / sd(htwt$height) &gt; beta2 [1] 1.150092 &gt; &gt; beta1 &lt;- mean(htwt$weight) - beta2 * mean(htwt$height) &gt; beta1 [1] -130.9104 &gt; &gt; yhat &lt;- beta1 + beta2 * htwt$height 56.9 Plot the Line &gt; df &lt;- data.frame(htwt, yhat=yhat) &gt; ggplot(data=df) + geom_point(aes(x=height, y=weight), size=2, alpha=0.5) + + geom_line(aes(x=height, y=yhat), color=&quot;blue&quot;, size=1.2) 56.10 Observed Data, Fits, and Residuals We observe data \\((x_1, y_1), \\ldots, (x_n, y_n)\\). Note that we only observe \\(X_i\\) and \\(Y_i\\) from the generative model \\(Y_i = \\beta_1 + \\beta_2 X_i + E_i\\). We calculate fitted values and observed residuals: \\[\\hat{y}_i = \\hat{\\beta}_1 + \\hat{\\beta}_2 x_i\\] \\[\\hat{e}_i = y_i - \\hat{y}_i\\] By construction, it is the case that \\(\\sum_{i=1}^n \\hat{e}_i = 0\\). 56.11 Proportion of Variation Explained The proportion of variance explained by the fitted model is called \\(R^2\\) or \\(r^2\\). It is calculated by: \\[r^2 = \\frac{s^2_{\\hat{y}}}{s^2_{y}}\\] "],
["lm-function-in-r.html", "57 lm() Function in R 57.1 Calculate the Line in R 57.2 An lm Object is a List 57.3 From the R Help 57.4 Some of the List Items 57.5 summary() 57.6 summary() List Elements 57.7 Using tidy() 57.8 Proportion of Variation Explained 57.9 Assumptions to Verify 57.10 Residual Distribution 57.11 Normal Residuals Check 57.12 Fitted Values Vs. Obs. Residuals", " 57 lm() Function in R 57.1 Calculate the Line in R The syntax for a model in R is response variable ~ explanatory variables where the explanatory variables component can involve several types of terms. &gt; myfit &lt;- lm(weight ~ height, data=htwt) &gt; myfit Call: lm(formula = weight ~ height, data = htwt) Coefficients: (Intercept) height -130.91 1.15 57.2 An lm Object is a List &gt; class(myfit) [1] &quot;lm&quot; &gt; is.list(myfit) [1] TRUE &gt; names(myfit) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; 57.3 From the R Help lm returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”). The functions summary and anova are used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm. 57.4 Some of the List Items These are some useful items to access from the lm object: coefficients: a named vector of coefficients residuals: the residuals, that is response minus fitted values. fitted.values: the fitted mean values. df.residual: the residual degrees of freedom. call: the matched call. model: if requested (the default), the model frame used. 57.5 summary() &gt; summary(myfit) Call: lm(formula = weight ~ height, data = htwt) Residuals: Min 1Q Median 3Q Max -19.658 -5.381 -0.555 4.807 42.894 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -130.91040 11.52792 -11.36 &lt;2e-16 *** height 1.15009 0.06749 17.04 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 8.505 on 198 degrees of freedom Multiple R-squared: 0.5946, Adjusted R-squared: 0.5925 F-statistic: 290.4 on 1 and 198 DF, p-value: &lt; 2.2e-16 57.6 summary() List Elements &gt; mysummary &lt;- summary(myfit) &gt; names(mysummary) [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; 57.7 Using tidy() &gt; library(broom) &gt; tidy(myfit) # A tibble: 2 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -131. 11.5 -11.4 2.44e-23 2 height 1.15 0.0675 17.0 1.12e-40 57.8 Proportion of Variation Explained The proportion of variance explained by the fitted model is called \\(R^2\\) or \\(r^2\\). It is calculated by: \\[r^2 = \\frac{s^2_{\\hat{y}}}{s^2_{y}}\\] &gt; summary(myfit)$r.squared [1] 0.5945555 &gt; &gt; var(myfit$fitted.values)/var(htwt$weight) [1] 0.5945555 57.9 Assumptions to Verify The assumptions on the above linear model are really about the joint distribution of the residuals, which are not directly observed. On data, we try to verify: The fitted values and the residuals show no trends with respect to each other The residuals are distributed approximately Normal\\((0, \\sigma^2)\\) A constant variance is called homoscedasticity A non-constant variance is called heteroscedascity There are no lurking variables There are two plots we will use in this course to investigate the first two. 57.10 Residual Distribution &gt; plot(myfit, which=1) 57.11 Normal Residuals Check &gt; plot(myfit, which=2) 57.12 Fitted Values Vs. Obs. Residuals "],
["ordinary-least-squares-1.html", "58 Ordinary Least Squares 58.1 OLS Solution 58.2 Sample Variance 58.3 Sample Covariance 58.4 Expected Values 58.5 Standard Error 58.6 Proportion of Variance Explained 58.7 Normal Errors 58.8 Sampling Distribution 58.9 CLT 58.10 Gauss-Markov Theorem", " 58 Ordinary Least Squares Ordinary least squares (OLS) estimates the model \\[ \\begin{aligned} Y_i &amp; = \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + E_i \\\\ &amp; = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}+ E_i \\end{aligned} \\] where \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). Note that typically \\(X_{i1} = 1\\) for all \\(i\\) so that \\(\\beta_1 X_{i1} = \\beta_1\\) serves as the intercept. 58.1 OLS Solution The estimates of \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are found by identifying the values that minimize: \\[ \\begin{aligned} \\sum_{i=1}^n \\left[ Y_i - (\\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip}) \\right]^2 \\\\ = ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}) \\end{aligned} \\] The solution is expressed in terms of matrix algebra computations: \\[ \\hat{{\\boldsymbol{\\beta}}} = ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{Y}}. \\] 58.2 Sample Variance Let the predicted values of the model be \\[ \\hat{{\\boldsymbol{Y}}} = {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}} = {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{Y}}. \\] We estimate \\(\\sigma^2\\) by the OLS sample variance \\[ S^2 = \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{n-p}. \\] 58.3 Sample Covariance The \\(p\\)-vector \\(\\hat{{\\boldsymbol{\\beta}}}\\) has covariance matrix \\[ {\\operatorname{Cov}}(\\hat{{\\boldsymbol{\\beta}}} | {\\boldsymbol{X}}) = ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2. \\] Its estimated covariance matrix is \\[ \\widehat{{\\operatorname{Cov}}}(\\hat{{\\boldsymbol{\\beta}}}) = ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} S^2. \\] 58.4 Expected Values Under the assumption that \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\), we have the following: \\[ {\\operatorname{E}}\\left[ \\left. \\hat{{\\boldsymbol{\\beta}}} \\right| {\\boldsymbol{X}}\\right] = {\\boldsymbol{\\beta}}\\] \\[ {\\operatorname{E}}\\left[ \\left. S^2 \\right| {\\boldsymbol{X}}\\right] = \\sigma^2 \\] \\[ {\\operatorname{E}}\\left[\\left. ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} S^2 \\right| {\\boldsymbol{X}}\\right] = {\\operatorname{Cov}}\\left(\\hat{{\\boldsymbol{\\beta}}}\\right) \\] \\[ {\\operatorname{Cov}}\\left(\\hat{\\beta}_j, Y_i - \\hat{Y}_i\\right) = \\boldsymbol{0}. \\] 58.5 Standard Error The standard error of \\(\\hat{\\beta}_j\\) is the square root of the \\((j, j)\\) diagonal entry of \\(({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2\\) \\[ {\\operatorname{se}}(\\hat{\\beta}_j) = \\sqrt{\\left[({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2\\right]_{jj}} \\] and estimated standard error is \\[ \\hat{{\\operatorname{se}}}(\\hat{\\beta}_j) = \\sqrt{\\left[({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} S^2\\right]_{jj}} \\] 58.6 Proportion of Variance Explained The proportion of variance explained is defined equivalently to the simple linear regression scneario: \\[ R^2 = \\frac{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}. \\] 58.7 Normal Errors Suppose we assume \\(E_1, E_2, \\ldots, E_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(0, \\sigma^2)\\). Then \\[ \\ell\\left({\\boldsymbol{\\beta}}, \\sigma^2 ; {\\boldsymbol{Y}}, {\\boldsymbol{X}}\\right) \\propto -n\\log(\\sigma^2) -\\frac{1}{\\sigma^2} ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}). \\] Since minimizing \\(({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})\\) maximizes the likelihood with respect to \\({\\boldsymbol{\\beta}}\\), this implies \\(\\hat{{\\boldsymbol{\\beta}}}\\) is the MLE for \\({\\boldsymbol{\\beta}}\\). It can also be calculated that \\(\\frac{n-p}{n} S^2\\) is the MLE for \\(\\sigma^2\\). 58.8 Sampling Distribution When \\(E_1, E_2, \\ldots, E_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(0, \\sigma^2)\\), it follows that, conditional on \\({\\boldsymbol{X}}\\): \\[ \\hat{{\\boldsymbol{\\beta}}} \\sim \\mbox{MVN}_p\\left({\\boldsymbol{\\beta}}, ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2 \\right) \\] \\[ \\begin{aligned} S^2 \\frac{n-p}{\\sigma^2} &amp; \\sim \\chi^2_{n-p} \\\\ \\frac{\\hat{\\beta}_j - \\beta_j}{\\hat{{\\operatorname{se}}}(\\hat{\\beta}_j)} &amp; \\sim t_{n-p} \\end{aligned} \\] 58.9 CLT Under the assumption that \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for \\(i \\not= j\\), it follows that as \\(n \\rightarrow \\infty\\), \\[ \\sqrt{n} \\left(\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}}\\right) \\stackrel{D}{\\longrightarrow} \\mbox{MVN}_p\\left( \\boldsymbol{0}, ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2 \\right). \\] 58.10 Gauss-Markov Theorem Under the assumption that \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for \\(i \\not= j\\), the Gauss-Markov theorem shows that among all BLUEs, best linear unbiased estimators, the least squares estimate has the smallest mean-squared error. Specifically, suppose that \\(\\tilde{{\\boldsymbol{\\beta}}}\\) is a linear estimator (calculated from a linear operator on \\({\\boldsymbol{Y}}\\)) where \\({\\operatorname{E}}[\\tilde{{\\boldsymbol{\\beta}}} | {\\boldsymbol{X}}] = {\\boldsymbol{\\beta}}\\). Then \\[ {\\operatorname{E}}\\left[ \\left. ({\\boldsymbol{Y}}- {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}) \\right| {\\boldsymbol{X}}\\right] \\leq {\\operatorname{E}}\\left[ \\left. ({\\boldsymbol{Y}}- {\\boldsymbol{X}}\\tilde{{\\boldsymbol{\\beta}}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}\\tilde{{\\boldsymbol{\\beta}}}) \\right| {\\boldsymbol{X}}\\right]. \\] "],
["generalized-least-squares-1.html", "59 Generalized Least Squares 59.1 GLS Solution 59.2 Other Results", " 59 Generalized Least Squares Generalized least squares (GLS) assumes the same model as OLS, except it allows for heteroskedasticity and covariance among the \\(E_i\\). Specifically, it is assumed that \\({\\boldsymbol{E}}= (E_1, \\ldots, E_n)^T\\) is distributed as \\[ {\\boldsymbol{E}}_{n \\times 1} \\sim (\\boldsymbol{0}, {\\boldsymbol{\\Sigma}}) \\] where \\(\\boldsymbol{0}\\) is the expected value \\({\\boldsymbol{\\Sigma}}= (\\sigma_{ij})\\) is the \\(n \\times n\\) covariance matrix. The most straightforward way to navigate GLS results is to recognize that \\[ {\\boldsymbol{\\Sigma}}^{-1/2} {\\boldsymbol{Y}}= {\\boldsymbol{\\Sigma}}^{-1/2}{\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{\\Sigma}}^{-1/2}{\\boldsymbol{E}}\\] satisfies the assumptions of the OLS model. 59.1 GLS Solution The solution to minimizing \\[ ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})^T {\\boldsymbol{\\Sigma}}^{-1} ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}) \\] is \\[ \\hat{{\\boldsymbol{\\beta}}} = \\left( {\\boldsymbol{X}}^T {\\boldsymbol{\\Sigma}}^{-1} {\\boldsymbol{X}}\\right)^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{\\Sigma}}^{-1} {\\boldsymbol{Y}}. \\] 59.2 Other Results The issue of estimating \\({\\boldsymbol{\\Sigma}}\\) if it is unknown is complicated. Other than estimates of \\(\\sigma^2\\), the results from the OLS section recapitulate by replacing \\({\\boldsymbol{Y}}= {\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{E}}\\) with \\[ {\\boldsymbol{\\Sigma}}^{-1/2} {\\boldsymbol{Y}}= {\\boldsymbol{\\Sigma}}^{-1/2}{\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{\\Sigma}}^{-1/2}{\\boldsymbol{E}}. \\] For example, as \\(n \\rightarrow \\infty\\), \\[ \\sqrt{n} \\left(\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}}\\right) \\stackrel{D}{\\longrightarrow} \\mbox{MNV}_p\\left( \\boldsymbol{0}, ({\\boldsymbol{X}}^T {\\boldsymbol{\\Sigma}}^{-1} {\\boldsymbol{X}})^{-1} \\right). \\] We also still have that \\[ {\\operatorname{E}}\\left[ \\left. \\hat{{\\boldsymbol{\\beta}}} \\right| {\\boldsymbol{X}}\\right] = {\\boldsymbol{\\beta}}. \\] And when \\({\\boldsymbol{E}}\\sim \\mbox{MVN}_n(\\boldsymbol{0}, {\\boldsymbol{\\Sigma}})\\), \\(\\hat{{\\boldsymbol{\\beta}}}\\) is the MLE. "],
["ols-in-r.html", "60 OLS in R 60.1 Weight Regressed on Height + Sex 60.2 One Variable, Two Scales 60.3 Interactions 60.4 More on Interactions 60.5 Visualizing Three Different Models", " 60 OLS in R R implements OLS of multiple explanatory variables exactly the same as with a single explanatory variable, except we need to show the sum of all explanatory variables that we want to use. &gt; lm(weight ~ height + sex, data=htwt) Call: lm(formula = weight ~ height + sex, data = htwt) Coefficients: (Intercept) height sexM -76.6167 0.8106 8.2269 60.1 Weight Regressed on Height + Sex &gt; summary(lm(weight ~ height + sex, data=htwt)) Call: lm(formula = weight ~ height + sex, data = htwt) Residuals: Min 1Q Median 3Q Max -20.131 -4.884 -0.640 5.160 41.490 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -76.6167 15.7150 -4.875 2.23e-06 *** height 0.8105 0.0953 8.506 4.50e-15 *** sexM 8.2269 1.7105 4.810 3.00e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 8.066 on 197 degrees of freedom Multiple R-squared: 0.6372, Adjusted R-squared: 0.6335 F-statistic: 173 on 2 and 197 DF, p-value: &lt; 2.2e-16 60.2 One Variable, Two Scales We can include a single variable but on two different scales: &gt; htwt &lt;- htwt %&gt;% mutate(height2 = height^2) &gt; summary(lm(weight ~ height + height2, data=htwt)) Call: lm(formula = weight ~ height + height2, data = htwt) Residuals: Min 1Q Median 3Q Max -24.265 -5.159 -0.499 4.549 42.965 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 107.117140 175.246872 0.611 0.542 height -1.632719 2.045524 -0.798 0.426 height2 0.008111 0.005959 1.361 0.175 Residual standard error: 8.486 on 197 degrees of freedom Multiple R-squared: 0.5983, Adjusted R-squared: 0.5943 F-statistic: 146.7 on 2 and 197 DF, p-value: &lt; 2.2e-16 60.3 Interactions It is possible to include products of explanatory variables, which is called an interaction. &gt; summary(lm(weight ~ height + sex + height:sex, data=htwt)) Call: lm(formula = weight ~ height + sex + height:sex, data = htwt) Residuals: Min 1Q Median 3Q Max -20.869 -4.835 -0.897 4.429 41.122 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -45.6730 22.1342 -2.063 0.0404 * height 0.6227 0.1343 4.637 6.46e-06 *** sexM -55.6571 32.4597 -1.715 0.0880 . height:sexM 0.3729 0.1892 1.971 0.0502 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 8.007 on 196 degrees of freedom Multiple R-squared: 0.6442, Adjusted R-squared: 0.6388 F-statistic: 118.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 60.4 More on Interactions What happens when there is an interaction between a quantitative explanatory variable and a factor explanatory variable? In the next plot, we show three models: Grey solid: lm(weight ~ height, data=htwt) Color dashed: lm(weight ~ height + sex, data=htwt) Color solid: lm(weight ~ height + sex + height:sex, data=htwt) 60.5 Visualizing Three Different Models "],
["categorical-explanatory-variables.html", "61 Categorical Explanatory Variables 61.1 Example: Chicken Weights 61.2 Factor Variables in lm() 61.3 Plot the Fit 61.4 ANOVA (Version 1) 61.5 anova() 61.6 How It Works 61.7 Top of Design Matrix 61.8 Bottom of Design Matrix 61.9 Model Fits", " 61 Categorical Explanatory Variables 61.1 Example: Chicken Weights &gt; data(&quot;chickwts&quot;, package=&quot;datasets&quot;) &gt; head(chickwts) weight feed 1 179 horsebean 2 160 horsebean 3 136 horsebean 4 227 horsebean 5 217 horsebean 6 168 horsebean &gt; summary(chickwts$feed) casein horsebean linseed meatmeal soybean sunflower 12 10 12 11 14 12 61.2 Factor Variables in lm() &gt; chick_fit &lt;- lm(weight ~ feed, data=chickwts) &gt; summary(chick_fit) Call: lm(formula = weight ~ feed, data = chickwts) Residuals: Min 1Q Median 3Q Max -123.909 -34.413 1.571 38.170 103.091 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 323.583 15.834 20.436 &lt; 2e-16 *** feedhorsebean -163.383 23.485 -6.957 2.07e-09 *** feedlinseed -104.833 22.393 -4.682 1.49e-05 *** feedmeatmeal -46.674 22.896 -2.039 0.045567 * feedsoybean -77.155 21.578 -3.576 0.000665 *** feedsunflower 5.333 22.393 0.238 0.812495 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 54.85 on 65 degrees of freedom Multiple R-squared: 0.5417, Adjusted R-squared: 0.5064 F-statistic: 15.36 on 5 and 65 DF, p-value: 5.936e-10 61.3 Plot the Fit &gt; plot(chickwts$feed, chickwts$weight, xlab=&quot;Feed&quot;, ylab=&quot;Weight&quot;, las=2) &gt; points(chickwts$feed, chick_fit$fitted.values, col=&quot;blue&quot;, pch=20, cex=2) 61.4 ANOVA (Version 1) ANOVA (analysis of variance) was originally developed as a statistical model and method for comparing differences in mean values between various groups. ANOVA quantifies and tests for differences in response variables with respect to factor variables. In doing so, it also partitions the total variance to that due to within and between groups, where groups are defined by the factor variables. 61.5 anova() The classic ANOVA table: &gt; anova(chick_fit) Analysis of Variance Table Response: weight Df Sum Sq Mean Sq F value Pr(&gt;F) feed 5 231129 46226 15.365 5.936e-10 *** Residuals 65 195556 3009 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; n &lt;- length(chick_fit$residuals) # n &lt;- 71 &gt; (n-1)*var(chick_fit$fitted.values) [1] 231129.2 &gt; (n-1)*var(chick_fit$residuals) [1] 195556 &gt; (n-1)*var(chickwts$weight) # sum of above two quantities [1] 426685.2 &gt; (231129/5)/(195556/65) # F-statistic [1] 15.36479 61.6 How It Works &gt; levels(chickwts$feed) [1] &quot;casein&quot; &quot;horsebean&quot; &quot;linseed&quot; &quot;meatmeal&quot; &quot;soybean&quot; &quot;sunflower&quot; &gt; head(chickwts, n=3) weight feed 1 179 horsebean 2 160 horsebean 3 136 horsebean &gt; tail(chickwts, n=3) weight feed 69 222 casein 70 283 casein 71 332 casein &gt; x &lt;- model.matrix(weight ~ feed, data=chickwts) &gt; dim(x) [1] 71 6 61.7 Top of Design Matrix &gt; head(x) (Intercept) feedhorsebean feedlinseed feedmeatmeal feedsoybean 1 1 1 0 0 0 2 1 1 0 0 0 3 1 1 0 0 0 4 1 1 0 0 0 5 1 1 0 0 0 6 1 1 0 0 0 feedsunflower 1 0 2 0 3 0 4 0 5 0 6 0 61.8 Bottom of Design Matrix &gt; tail(x) (Intercept) feedhorsebean feedlinseed feedmeatmeal feedsoybean 66 1 0 0 0 0 67 1 0 0 0 0 68 1 0 0 0 0 69 1 0 0 0 0 70 1 0 0 0 0 71 1 0 0 0 0 feedsunflower 66 0 67 0 68 0 69 0 70 0 71 0 61.9 Model Fits &gt; chick_fit$fitted.values %&gt;% round(digits=4) %&gt;% unique() [1] 160.2000 218.7500 246.4286 328.9167 276.9091 323.5833 &gt; chickwts %&gt;% group_by(feed) %&gt;% summarize(mean(weight)) # A tibble: 6 x 2 feed `mean(weight)` &lt;fct&gt; &lt;dbl&gt; 1 casein 324. 2 horsebean 160. 3 linseed 219. 4 meatmeal 277. 5 soybean 246. 6 sunflower 329. "],
["variable-transformations.html", "62 Variable Transformations 62.1 Rationale 62.2 Power and Log Transformations 62.3 Diamonds Data 62.4 Nonlinear Relationship 62.5 Regression with Nonlinear Relationship 62.6 Residual Distribution 62.7 Normal Residuals Check 62.8 Log-Transformation 62.9 OLS on Log-Transformed Data 62.10 Residual Distribution 62.11 Normal Residuals Check 62.12 Tree Pollen Study 62.13 Tree Pollen Count by Week 62.14 A Clever Transformation 62.15 week Transformed", " 62 Variable Transformations 62.1 Rationale In order to obtain reliable model fits and inference on linear models, the model assumptions described earlier must be satisfied. Sometimes it is necessary to transform the response variable and/or some of the explanatory variables. This process should involve data visualization and exploration. 62.2 Power and Log Transformations It is often useful to explore power and log transforms of the variables, e.g., \\(\\log(y)\\) or \\(y^\\lambda\\) for some \\(\\lambda\\) (and likewise \\(\\log(x)\\) or \\(x^\\lambda\\)). You can read more about the Box-Cox family of power transformations. 62.3 Diamonds Data &gt; data(&quot;diamonds&quot;, package=&quot;ggplot2&quot;) &gt; head(diamonds) # A tibble: 6 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 62.4 Nonlinear Relationship &gt; ggplot(data = diamonds) + + geom_point(mapping=aes(x=carat, y=price, color=clarity), alpha=0.3) 62.5 Regression with Nonlinear Relationship &gt; diam_fit &lt;- lm(price ~ carat + clarity, data=diamonds) &gt; anova(diam_fit) Analysis of Variance Table Response: price Df Sum Sq Mean Sq F value Pr(&gt;F) carat 1 7.2913e+11 7.2913e+11 435639.9 &lt; 2.2e-16 *** clarity 7 3.9082e+10 5.5831e+09 3335.8 &lt; 2.2e-16 *** Residuals 53931 9.0264e+10 1.6737e+06 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 62.6 Residual Distribution &gt; plot(diam_fit, which=1) 62.7 Normal Residuals Check &gt; plot(diam_fit, which=2) 62.8 Log-Transformation &gt; ggplot(data = diamonds) + + geom_point(aes(x=carat, y=price, color=clarity), alpha=0.3) + + scale_y_log10(breaks=c(1000,5000,10000)) + + scale_x_log10(breaks=1:5) 62.9 OLS on Log-Transformed Data &gt; diamonds &lt;- mutate(diamonds, log_price = log(price, base=10), + log_carat = log(carat, base=10)) &gt; ldiam_fit &lt;- lm(log_price ~ log_carat + clarity, data=diamonds) &gt; anova(ldiam_fit) Analysis of Variance Table Response: log_price Df Sum Sq Mean Sq F value Pr(&gt;F) log_carat 1 9771.9 9771.9 1452922.6 &lt; 2.2e-16 *** clarity 7 339.1 48.4 7203.3 &lt; 2.2e-16 *** Residuals 53931 362.7 0.0 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 62.10 Residual Distribution &gt; plot(ldiam_fit, which=1) 62.11 Normal Residuals Check &gt; plot(ldiam_fit, which=2) 62.12 Tree Pollen Study Suppose that we have a study where tree pollen measurements are averaged every week, and these data are recorded for 10 years. These data are simulated: &gt; pollen_study # A tibble: 520 x 3 week year pollen &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 2001 1842. 2 2 2001 1966. 3 3 2001 2381. 4 4 2001 2141. 5 5 2001 2210. 6 6 2001 2585. 7 7 2001 2392. 8 8 2001 2105. 9 9 2001 2278. 10 10 2001 2384. # … with 510 more rows 62.13 Tree Pollen Count by Week &gt; ggplot(pollen_study) + geom_point(aes(x=week, y=pollen)) 62.14 A Clever Transformation We can see there is a linear relationship between pollen and week if we transform week to be number of weeks from the peak week. &gt; pollen_study &lt;- pollen_study %&gt;% + mutate(week_new = abs(week-20)) Note that this is a very different transformation from taking a log or power transformation. 62.15 week Transformed &gt; ggplot(pollen_study) + geom_point(aes(x=week_new, y=pollen)) "],
["ols-goodness-of-fit.html", "63 OLS Goodness of Fit 63.1 Pythagorean Theorem 63.2 OLS Normal Model 63.3 Projection Matrices 63.4 Decomposition 63.5 Distribution of Projection 63.6 Distribution of Residuals 63.7 Degrees of Freedom 63.8 Submodels 63.9 Hypothesis Testing 63.10 Generalized LRT 63.11 Nested Projections 63.12 F Statistic 63.13 F Distribution 63.14 F Test 63.15 Example: Davis Data 63.16 Comparing Linear Models in R 63.17 ANOVA (Version 2) 63.18 Comparing Two Models with anova() 63.19 When There’s a Single Variable Difference 63.20 Calculating the F-statistic 63.21 Calculating the Generalized LRT 63.22 ANOVA on More Distant Models 63.23 Compare Multiple Models at Once", " 63 OLS Goodness of Fit 63.1 Pythagorean Theorem PythMod Least squares model fitting can be understood through the Pythagorean theorem: \\(a^2 + b^2 = c^2\\). However, here we have: \\[ \\sum_{i=1}^n Y_i^2 = \\sum_{i=1}^n \\hat{Y}_i^2 + \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\] where the \\(\\hat{Y}_i\\) are the result of a linear projection of the \\(Y_i\\). 63.2 OLS Normal Model In this section, let’s assume that \\(({\\boldsymbol{X}}_1, Y_1), \\ldots, ({\\boldsymbol{X}}_n, Y_n)\\) are distributed so that \\[ \\begin{aligned} Y_i &amp; = \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + E_i \\\\ &amp; = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}+ E_i \\end{aligned} \\] where \\({\\boldsymbol{E}}| {\\boldsymbol{X}}\\sim \\mbox{MVN}_n({\\boldsymbol{0}}, \\sigma^2 {\\boldsymbol{I}})\\). Note that we haven’t specified the distribution of the \\({\\boldsymbol{X}}_i\\) rv’s. 63.3 Projection Matrices In the OLS framework we have: \\[ \\hat{{\\boldsymbol{Y}}} = {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{Y}}. \\] The matrix \\({\\boldsymbol{P}}_{n \\times n} = {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T\\) is a projection matrix. The vector \\({\\boldsymbol{Y}}\\) is projected into the space spanned by the column space of \\({\\boldsymbol{X}}\\). Project matrices have the following properties: \\({\\boldsymbol{P}}\\) is symmetric \\({\\boldsymbol{P}}\\) is idempotent so that \\({\\boldsymbol{P}}{\\boldsymbol{P}}= {\\boldsymbol{P}}\\) If \\({\\boldsymbol{X}}\\) has column rank \\(p\\), then \\({\\boldsymbol{P}}\\) has rank \\(p\\) The eigenvalues of \\({\\boldsymbol{P}}\\) are \\(p\\) 1’s and \\(n-p\\) 0’s The trace (sum of diagonal entries) is \\(\\operatorname{tr}({\\boldsymbol{P}}) = p\\) \\({\\boldsymbol{I}}- {\\boldsymbol{P}}\\) is also a projection matrix with rank \\(n-p\\) 63.4 Decomposition Note that \\({\\boldsymbol{P}}({\\boldsymbol{I}}- {\\boldsymbol{P}}) = {\\boldsymbol{P}}- {\\boldsymbol{P}}{\\boldsymbol{P}}= {\\boldsymbol{P}}- {\\boldsymbol{P}}= {\\boldsymbol{0}}\\). We have \\[ \\begin{aligned} \\| {\\boldsymbol{Y}}\\|_{2}^{2} = {\\boldsymbol{Y}}^T {\\boldsymbol{Y}}&amp; = ({\\boldsymbol{P}}{\\boldsymbol{Y}}+ ({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}})^T ({\\boldsymbol{P}}{\\boldsymbol{Y}}+ ({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}}) \\\\ &amp; = ({\\boldsymbol{P}}{\\boldsymbol{Y}})^T ({\\boldsymbol{P}}{\\boldsymbol{Y}}) + (({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}})^T (({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}}) \\\\ &amp; = \\| {\\boldsymbol{P}}{\\boldsymbol{Y}}\\|_{2}^{2} + \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}}\\|_{2}^{2} \\end{aligned} \\] where the cross terms disappear because \\({\\boldsymbol{P}}({\\boldsymbol{I}}- {\\boldsymbol{P}}) = {\\boldsymbol{0}}\\). Note: The \\(\\ell_p\\) norm of an \\(n\\)-vector \\(\\boldsymbol{w}\\) is defined as \\[ \\| \\boldsymbol{w} \\|_p = \\left(\\sum_{i=1}^n |w_i|^p\\right)^{1/p}. \\] Above we calculated \\[ \\| \\boldsymbol{w} \\|_2^2 = \\sum_{i=1}^n w_i^2. \\] 63.5 Distribution of Projection Suppose that \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(0,\\sigma^2)\\). This can also be written as \\({\\boldsymbol{Y}}\\sim \\mbox{MVN}_n({\\boldsymbol{0}}, \\sigma^2 {\\boldsymbol{I}})\\). It follows that \\[ {\\boldsymbol{P}}{\\boldsymbol{Y}}\\sim \\mbox{MVN}_{n}({\\boldsymbol{0}}, \\sigma^2 {\\boldsymbol{P}}{\\boldsymbol{I}}{\\boldsymbol{P}}^T). \\] where \\({\\boldsymbol{P}}{\\boldsymbol{I}}{\\boldsymbol{P}}^T = {\\boldsymbol{P}}{\\boldsymbol{P}}^T = {\\boldsymbol{P}}{\\boldsymbol{P}}= {\\boldsymbol{P}}\\). Also, \\(({\\boldsymbol{P}}{\\boldsymbol{Y}})^T ({\\boldsymbol{P}}{\\boldsymbol{Y}}) = {\\boldsymbol{Y}}^T {\\boldsymbol{P}}^T {\\boldsymbol{P}}{\\boldsymbol{Y}}= {\\boldsymbol{Y}}^T {\\boldsymbol{P}}{\\boldsymbol{Y}}\\), a quadratic form. Given the eigenvalues of \\({\\boldsymbol{P}}\\), \\({\\boldsymbol{Y}}^T {\\boldsymbol{P}}{\\boldsymbol{Y}}\\) is equivalent in distribution to \\(p\\) squared iid Normal(0,1) rv’s, so \\[ \\frac{{\\boldsymbol{Y}}^T {\\boldsymbol{P}}{\\boldsymbol{Y}}}{\\sigma^2} \\sim \\chi^2_{p}. \\] 63.6 Distribution of Residuals If \\({\\boldsymbol{P}}{\\boldsymbol{Y}}= \\hat{{\\boldsymbol{Y}}}\\) are the fitted OLS values, then \\(({\\boldsymbol{I}}-{\\boldsymbol{P}}) {\\boldsymbol{Y}}= {\\boldsymbol{Y}}- \\hat{{\\boldsymbol{Y}}}\\) are the residuals. It follows by the same argument as above that \\[ \\frac{{\\boldsymbol{Y}}^T ({\\boldsymbol{I}}-{\\boldsymbol{P}}) {\\boldsymbol{Y}}}{\\sigma^2} \\sim \\chi^2_{n-p}. \\] It’s also straightforward to show that \\(({\\boldsymbol{I}}-{\\boldsymbol{P}}){\\boldsymbol{Y}}\\sim \\mbox{MVN}_{n}({\\boldsymbol{0}}, \\sigma^2({\\boldsymbol{I}}-{\\boldsymbol{P}}))\\) and \\({\\operatorname{Cov}}({\\boldsymbol{P}}{\\boldsymbol{Y}}, ({\\boldsymbol{I}}-{\\boldsymbol{P}}){\\boldsymbol{Y}}) = {\\boldsymbol{0}}\\). 63.7 Degrees of Freedom The degrees of freedom, \\(p\\), of a linear projection model fit is equal to The number of linearly independent columns of \\({\\boldsymbol{X}}\\) The number of nonzero eigenvalues of \\({\\boldsymbol{P}}\\) (where nonzero eigenvalues are equal to 1) The trace of the projection matrix, \\(\\operatorname{tr}({\\boldsymbol{P}})\\). The reason why we divide estimates of variance by \\(n-p\\) is because this is the number of effective independent sources of variation remaining after the model is fit by projecting the \\(n\\) observations into a \\(p\\) dimensional linear space. 63.8 Submodels Consider the OLS model \\({\\boldsymbol{Y}}= {\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{E}}\\) where there are \\(p\\) columns of \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{\\beta}}\\) is a \\(p\\)-vector. Let \\({\\boldsymbol{X}}_0\\) be a subset of \\(p_0\\) columns of \\({\\boldsymbol{X}}\\) and let \\({\\boldsymbol{X}}_1\\) be a subset of \\(p_1\\) columns, where \\(1 \\leq p_0 &lt; p_1 \\leq p\\). Also, assume that the columns of \\({\\boldsymbol{X}}_0\\) are a subset of \\({\\boldsymbol{X}}_1\\). We can form \\(\\hat{{\\boldsymbol{Y}}}_0 = {\\boldsymbol{P}}_0 {\\boldsymbol{Y}}\\) where \\({\\boldsymbol{P}}_0\\) is the projection matrix built from \\({\\boldsymbol{X}}_0\\). We can analogously form \\(\\hat{{\\boldsymbol{Y}}}_1 = {\\boldsymbol{P}}_1 {\\boldsymbol{Y}}\\). 63.9 Hypothesis Testing Without loss of generality, suppose that \\({\\boldsymbol{\\beta}}_0 = (\\beta_1, \\beta_2, \\ldots, \\beta_{p_0})^T\\) and \\({\\boldsymbol{\\beta}}_1 = (\\beta_1, \\beta_2, \\ldots, \\beta_{p_1})^T\\). How do we compare these models, specifically to test \\(H_0: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) = {\\boldsymbol{0}}\\) vs \\(H_1: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) \\not= {\\boldsymbol{0}}\\)? The basic idea to perform this test is to compare the goodness of fits of each model via a pivotal statistic. We will discuss the generalized LRT and ANOVA approaches. 63.10 Generalized LRT Under the OLS Normal model, it follows that \\(\\hat{{\\boldsymbol{\\beta}}}_0 = ({\\boldsymbol{X}}^T_0 {\\boldsymbol{X}}_0)^{-1} {\\boldsymbol{X}}_0^T {\\boldsymbol{Y}}\\) is the MLE under the null hypothesis and \\(\\hat{{\\boldsymbol{\\beta}}}_1 = ({\\boldsymbol{X}}^T_1 {\\boldsymbol{X}}_1)^{-1} {\\boldsymbol{X}}_1^T {\\boldsymbol{Y}}\\) is the unconstrained MLE. Also, the respective MLEs of \\(\\sigma^2\\) are \\[ \\hat{\\sigma}^2_0 = \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_{0,i})^2}{n} \\] \\[ \\hat{\\sigma}^2_1 = \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_{1,i})^2}{n} \\] where \\(\\hat{{\\boldsymbol{Y}}}_{0} = {\\boldsymbol{X}}_0 \\hat{{\\boldsymbol{\\beta}}}_0\\) and \\(\\hat{{\\boldsymbol{Y}}}_{1} = {\\boldsymbol{X}}_1 \\hat{{\\boldsymbol{\\beta}}}_1\\). The generalized LRT statistic is \\[ \\lambda({\\boldsymbol{X}}, {\\boldsymbol{Y}}) = \\frac{L\\left(\\hat{{\\boldsymbol{\\beta}}}_1, \\hat{\\sigma}^2_1; {\\boldsymbol{X}}, {\\boldsymbol{Y}}\\right)}{L\\left(\\hat{{\\boldsymbol{\\beta}}}_0, \\hat{\\sigma}^2_0; {\\boldsymbol{X}}, {\\boldsymbol{Y}}\\right)} \\] where \\(2\\log\\lambda({\\boldsymbol{X}}, {\\boldsymbol{Y}})\\) has a \\(\\chi^2_{p_1 - p_0}\\) null distribution. 63.11 Nested Projections We can apply the Pythagorean theorem we saw earlier to linear subspaces to get: \\[ \\begin{aligned} \\| {\\boldsymbol{Y}}\\|^2_2 &amp; = \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|_{2}^{2} + \\| {\\boldsymbol{P}}_1 {\\boldsymbol{Y}}\\|_{2}^{2} \\\\ &amp; = \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|_{2}^{2} + \\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|_{2}^{2} + \\| {\\boldsymbol{P}}_0 {\\boldsymbol{Y}}\\|_{2}^{2} \\end{aligned} \\] We can also use the Pythagorean theorem to decompose the residuals from the smaller projection \\({\\boldsymbol{P}}_0\\): \\[ \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 = \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2 + \\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 \\] 63.12 F Statistic The \\(F\\) statistic compares the improvement of goodness in fit of the larger model to that of the smaller model in terms of sums of squared residuals, and it scales this improvement by an estimate of \\(\\sigma^2\\): \\[ \\begin{aligned} F &amp; = \\frac{\\left[\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 - \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2\\right]/(p_1 - p_0)}{\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2/(n-p_1)} \\\\ &amp; = \\frac{\\left[\\sum_{i=1}^n (Y_i - \\hat{Y}_{0,i})^2 - \\sum_{i=1}^n (Y_i - \\hat{Y}_{1,i})^2 \\right]/(p_1 - p_0)}{\\sum_{i=1}^n (Y_i - \\hat{Y}_{1,i})^2 / (n - p_1)} \\\\ \\end{aligned} \\] Since \\(\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 - \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2 = \\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2\\), we can equivalently write the \\(F\\) statistic as: \\[ \\begin{aligned} F &amp; = \\frac{\\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 / (p_1 - p_0)}{\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2/(n-p_1)} \\\\ &amp; = \\frac{\\sum_{i=1}^n (\\hat{Y}_{1,i} - \\hat{Y}_{0,i})^2 / (p_1 - p_0)}{\\sum_{i=1}^n (Y_i - \\hat{Y}_{1,i})^2 / (n - p_1)} \\end{aligned} \\] 63.13 F Distribution Suppose we have independent random variables \\(V \\sim \\chi^2_a\\) and \\(W \\sim \\chi^2_b\\). It follows that \\[ \\frac{V/a}{W/b} \\sim F_{a,b} \\] where \\(F_{a,b}\\) is the \\(F\\) distribution with \\((a, b)\\) degrees of freedom. By arguments similar to those given above, we have \\[ \\frac{\\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2}{\\sigma^2} \\sim \\chi^2_{p_1 - p_0} \\] \\[ \\frac{\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2}{\\sigma^2} \\sim \\chi^2_{n-p_1} \\] and these two rv’s are independent. 63.14 F Test Suppose that the OLS model holds where \\({\\boldsymbol{E}}| {\\boldsymbol{X}}\\sim \\mbox{MVN}_n({\\boldsymbol{0}}, \\sigma^2 {\\boldsymbol{I}})\\). In order to test \\(H_0: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) = {\\boldsymbol{0}}\\) vs \\(H_1: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) \\not= {\\boldsymbol{0}}\\), we can form the \\(F\\) statistic as given above, which has null distribution \\(F_{p_1 - p_0, n - p_1}\\). The p-value is calculated as \\(\\Pr(F^* \\geq F)\\) where \\(F\\) is the observed \\(F\\) statistic and \\(F^* \\sim F_{p_1 - p_0, n - p_1}\\). If the above assumption on the distribution of \\({\\boldsymbol{E}}| {\\boldsymbol{X}}\\) only approximately holds, then the \\(F\\) test p-value is also an approximation. 63.15 Example: Davis Data &gt; library(&quot;car&quot;) &gt; data(&quot;Davis&quot;, package=&quot;car&quot;) Warning in data(&quot;Davis&quot;, package = &quot;car&quot;): data set &#39;Davis&#39; not found &gt; htwt &lt;- tbl_df(Davis) &gt; htwt[12,c(2,3)] &lt;- htwt[12,c(3,2)] &gt; head(htwt) # A tibble: 6 x 5 sex weight height repwt repht &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 M 77 182 77 180 2 F 58 161 51 159 3 F 53 161 54 158 4 M 68 177 70 175 5 F 59 157 59 155 6 M 76 170 76 165 63.16 Comparing Linear Models in R Example: Davis Data Suppose we are considering the three following models: &gt; f1 &lt;- lm(weight ~ height, data=htwt) &gt; f2 &lt;- lm(weight ~ height + sex, data=htwt) &gt; f3 &lt;- lm(weight ~ height + sex + height:sex, data=htwt) How do we determine if the additional terms in models f2 and f3 are needed? 63.17 ANOVA (Version 2) A generalization of ANOVA exists that allows us to compare two nested models, quantifying their differences in terms of goodness of fit and performing a hypothesis test of whether this difference is statistically significant. A model is nested within another model if their difference is simply the absence of certain terms in the smaller model. The null hypothesis is that the additional terms have coefficients equal to zero, and the alternative hypothesis is that at least one coefficient is nonzero. Both versions of ANOVA can be described in a single, elegant mathematical framework. 63.18 Comparing Two Models with anova() This provides a comparison of the improvement in fit from model f2 compared to model f1: &gt; anova(f1, f2) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 198 14321 2 197 12816 1 1504.9 23.133 2.999e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 63.19 When There’s a Single Variable Difference Compare above anova(f1, f2) p-value to that for the sex term from the f2 model: &gt; library(broom) &gt; tidy(f2) # A tibble: 3 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -76.6 15.7 -4.88 2.23e- 6 2 height 0.811 0.0953 8.51 4.50e-15 3 sexM 8.23 1.71 4.81 3.00e- 6 63.20 Calculating the F-statistic &gt; anova(f1, f2) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 198 14321 2 197 12816 1 1504.9 23.133 2.999e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How the F-statistic is calculated: &gt; n &lt;- nrow(htwt) &gt; ss1 &lt;- (n-1)*var(f1$residuals) &gt; ss1 [1] 14321.11 &gt; ss2 &lt;- (n-1)*var(f2$residuals) &gt; ss2 [1] 12816.18 &gt; ((ss1 - ss2)/anova(f1, f2)$Df[2])/(ss2/f2$df.residual) [1] 23.13253 63.21 Calculating the Generalized LRT &gt; anova(f1, f2, test=&quot;LRT&quot;) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex Res.Df RSS Df Sum of Sq Pr(&gt;Chi) 1 198 14321 2 197 12816 1 1504.9 1.512e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; library(lmtest) &gt; lrtest(f1, f2) Likelihood ratio test Model 1: weight ~ height Model 2: weight ~ height + sex #Df LogLik Df Chisq Pr(&gt;Chisq) 1 3 -710.9 2 4 -699.8 1 22.205 2.45e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 These tests produce slightly different answers because anova() adjusts for degrees of freedom when estimating the variance, whereas lrtest() is the strict generalized LRT. See here. 63.22 ANOVA on More Distant Models We can compare models with multiple differences in terms: &gt; anova(f1, f3) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex + height:sex Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 198 14321 2 196 12567 2 1754 13.678 2.751e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 63.23 Compare Multiple Models at Once We can compare multiple models at once: &gt; anova(f1, f2, f3) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex Model 3: weight ~ height + sex + height:sex Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 198 14321 2 197 12816 1 1504.93 23.4712 2.571e-06 *** 3 196 12567 1 249.04 3.8841 0.05015 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],
["logistic-regression.html", "64 Logistic Regression 64.1 Goal 64.2 Bernoulli as EFD 64.3 Model 64.4 Maximum Likelihood Estimation 64.5 Iteratively Reweighted Least Squares 64.6 GLMs", " 64 Logistic Regression 64.1 Goal Logistic regression models a Bernoulli distributed response variable in terms of linear combinations of explanatory variables. This extends least squares regression to the case where the response variable captures a “success” or “failure” type outcome. 64.2 Bernoulli as EFD If \\(Y \\sim \\mbox{Bernoulli}(p)\\), then its pmf is: \\[ \\begin{aligned} f(y; p) &amp; = p^{y} (1-p)^{1-y} \\\\ &amp; = \\exp\\left\\{ \\log\\left(\\frac{p}{1-p}\\right)y + \\log(1-p) \\right\\} \\end{aligned} \\] In exponential family distribution (EFD) notation, \\[ \\eta(p) = \\log\\left(\\frac{p}{1-p}\\right) \\equiv {\\operatorname{logit}}(p), \\] \\(A(\\eta(p)) = \\log(1 + \\exp(\\eta)) = \\log(1-p)\\), and \\(y\\) is the sufficient statistic. 64.3 Model \\(({\\boldsymbol{X}}_1, Y_1), ({\\boldsymbol{X}}_2, Y_2), \\ldots, ({\\boldsymbol{X}}_n, Y_n)\\) are distributed so that \\(Y_i | {\\boldsymbol{X}}_i \\sim \\mbox{Bernoulli}(p_i)\\), where \\(\\{Y_i | {\\boldsymbol{X}}_i\\}_{i=1}^n\\) are jointly independent and \\[ {\\operatorname{logit}}\\left({\\operatorname{E}}[Y_i | {\\boldsymbol{X}}_i]\\right) = \\log\\left( \\frac{\\Pr(Y_i = 1 | {\\boldsymbol{X}}_i)}{\\Pr(Y_i = 0 | {\\boldsymbol{X}}_i)} \\right) = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}. \\] From this it follows that \\[ p_i = \\frac{\\exp\\left({\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\right)}{1 + \\exp\\left({\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\right)}. \\] 64.4 Maximum Likelihood Estimation The \\({\\boldsymbol{\\beta}}\\) are estimated from the MLE calculated from: \\[ \\begin{aligned} \\ell\\left({\\boldsymbol{\\beta}}; {\\boldsymbol{y}}, {\\boldsymbol{X}}\\right) &amp; = \\sum_{i=1}^n \\log\\left(\\frac{p_i}{1-p_i}\\right) y_i + \\log(1-p_i) \\\\ &amp; = \\sum_{i=1}^n ({\\boldsymbol{x}}_i {\\boldsymbol{\\beta}}) y_i - \\log\\left(1 + \\exp\\left({\\boldsymbol{x}}_i {\\boldsymbol{\\beta}}\\right) \\right) \\end{aligned} \\] 64.5 Iteratively Reweighted Least Squares Initialize \\({\\boldsymbol{\\beta}}^{(1)}\\). For each iteration \\(t=1, 2, \\ldots\\), set \\[ p_i^{(t)} = {\\operatorname{logit}}^{-1}\\left( {\\boldsymbol{x}}_i {\\boldsymbol{\\beta}}^{(t)} \\right), \\ \\ \\ \\ z_i^{(t)} = {\\operatorname{logit}}\\left(p_i^{(t)}\\right) + \\frac{y_i - p_i^{(t)}}{p_i^{(t)}(1-p_i^{(t)})} \\] and let \\({\\boldsymbol{z}}^{(t)} = \\left\\{z_i^{(t)}\\right\\}_{i=1}^n\\). Form \\(n \\times n\\) diagonal matrix \\(\\boldsymbol{W}^{(t)}\\) with \\((i, i)\\) entry equal to \\(p_i^{(t)}(1-p_i^{(t)})\\). Obtain \\({\\boldsymbol{\\beta}}^{(t+1)}\\) by performing the wieghted least squares regression (see GLS from earlier) \\[ {\\boldsymbol{\\beta}}^{(t+1)} = \\left({\\boldsymbol{X}}^T \\boldsymbol{W}^{(t)} {\\boldsymbol{X}}\\right)^{-1} {\\boldsymbol{X}}^T \\boldsymbol{W}^{(t)} {\\boldsymbol{z}}^{(t)}. \\] Iterate Steps 2-4 over \\(t=1, 2, 3, \\ldots\\) until convergence, setting \\(\\hat{{\\boldsymbol{\\beta}}} = {\\boldsymbol{\\beta}}^{(\\infty)}\\). 64.6 GLMs For exponential family distribution response variables, the generalized linear model is \\[ \\eta\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\] where \\(\\eta(\\theta)\\) is function of the expected value \\(\\theta\\) into the natural parameter. This is called the canonical link function in the GLM setting. The iteratively reweighted least squares algorithm presented above for calculating (local) maximum likelihood estimates of \\({\\boldsymbol{\\beta}}\\) has a generalization to a large class of exponential family distribution response vairables. "],
["glm-function-in-r.html", "65 glm() Function in R 65.1 Example: Grad School Admissions 65.2 Explore the Data 65.3 Logistic Regression in R 65.4 Summary of Fit 65.5 ANOVA of Fit 65.6 Example: Contraceptive Use 65.7 A Different Format 65.8 Fitting the Model 65.9 Summary of Fit 65.10 ANOVA of Fit 65.11 More on this Data Set", " 65 glm() Function in R 65.1 Example: Grad School Admissions &gt; mydata &lt;- + read.csv(&quot;https://stats.idre.ucla.edu/stat/data/binary.csv&quot;) &gt; dim(mydata) [1] 400 4 &gt; head(mydata) admit gre gpa rank 1 0 380 3.61 3 2 1 660 3.67 3 3 1 800 4.00 1 4 1 640 3.19 4 5 0 520 2.93 4 6 1 760 3.00 2 Data and analysis courtesy of http://www.ats.ucla.edu/stat/r/dae/logit.htm. 65.2 Explore the Data &gt; apply(mydata, 2, mean) admit gre gpa rank 0.3175 587.7000 3.3899 2.4850 &gt; apply(mydata, 2, sd) admit gre gpa rank 0.4660867 115.5165364 0.3805668 0.9444602 &gt; &gt; table(mydata$admit, mydata$rank) 1 2 3 4 0 28 97 93 55 1 33 54 28 12 &gt; ggplot(data=mydata) + + geom_boxplot(aes(x=as.factor(admit), y=gre)) &gt; ggplot(data=mydata) + + geom_boxplot(aes(x=as.factor(admit), y=gpa)) 65.3 Logistic Regression in R &gt; mydata$rank &lt;- factor(mydata$rank, levels=c(1, 2, 3, 4)) &gt; myfit &lt;- glm(admit ~ gre + gpa + rank, + data = mydata, family = &quot;binomial&quot;) &gt; myfit Call: glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, data = mydata) Coefficients: (Intercept) gre gpa rank2 rank3 -3.989979 0.002264 0.804038 -0.675443 -1.340204 rank4 -1.551464 Degrees of Freedom: 399 Total (i.e. Null); 394 Residual Null Deviance: 500 Residual Deviance: 458.5 AIC: 470.5 65.4 Summary of Fit &gt; summary(myfit) Call: glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, data = mydata) Deviance Residuals: Min 1Q Median 3Q Max -1.6268 -0.8662 -0.6388 1.1490 2.0790 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.989979 1.139951 -3.500 0.000465 *** gre 0.002264 0.001094 2.070 0.038465 * gpa 0.804038 0.331819 2.423 0.015388 * rank2 -0.675443 0.316490 -2.134 0.032829 * rank3 -1.340204 0.345306 -3.881 0.000104 *** rank4 -1.551464 0.417832 -3.713 0.000205 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 458.52 on 394 degrees of freedom AIC: 470.52 Number of Fisher Scoring iterations: 4 65.5 ANOVA of Fit &gt; anova(myfit, test=&quot;Chisq&quot;) Analysis of Deviance Table Model: binomial, link: logit Response: admit Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 399 499.98 gre 1 13.9204 398 486.06 0.0001907 *** gpa 1 5.7122 397 480.34 0.0168478 * rank 3 21.8265 394 458.52 7.088e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; anova(myfit, test=&quot;LRT&quot;) Analysis of Deviance Table Model: binomial, link: logit Response: admit Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 399 499.98 gre 1 13.9204 398 486.06 0.0001907 *** gpa 1 5.7122 397 480.34 0.0168478 * rank 3 21.8265 394 458.52 7.088e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 65.6 Example: Contraceptive Use &gt; cuse &lt;- + read.table(&quot;http://data.princeton.edu/wws509/datasets/cuse.dat&quot;, + header=TRUE) &gt; dim(cuse) [1] 16 5 &gt; head(cuse) age education wantsMore notUsing using 1 &lt;25 low yes 53 6 2 &lt;25 low no 10 4 3 &lt;25 high yes 212 52 4 &lt;25 high no 50 10 5 25-29 low yes 60 14 6 25-29 low no 19 10 Data and analysis courtesy of http://data.princeton.edu/R/glms.html. 65.7 A Different Format Note that in this data set there are multiple observations per explanatory variable configuration. The last two columns of the data frame count the successes and failures per configuration. &gt; head(cuse) age education wantsMore notUsing using 1 &lt;25 low yes 53 6 2 &lt;25 low no 10 4 3 &lt;25 high yes 212 52 4 &lt;25 high no 50 10 5 25-29 low yes 60 14 6 25-29 low no 19 10 65.8 Fitting the Model When this is the case, we call the glm() function slighlty differently. &gt; myfit &lt;- glm(cbind(using, notUsing) ~ age + education + wantsMore, + data=cuse, family = binomial) &gt; myfit Call: glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, family = binomial, data = cuse) Coefficients: (Intercept) age25-29 age30-39 age40-49 educationlow -0.8082 0.3894 0.9086 1.1892 -0.3250 wantsMoreyes -0.8330 Degrees of Freedom: 15 Total (i.e. Null); 10 Residual Null Deviance: 165.8 Residual Deviance: 29.92 AIC: 113.4 65.9 Summary of Fit &gt; summary(myfit) Call: glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, family = binomial, data = cuse) Deviance Residuals: Min 1Q Median 3Q Max -2.5148 -0.9376 0.2408 0.9822 1.7333 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.8082 0.1590 -5.083 3.71e-07 *** age25-29 0.3894 0.1759 2.214 0.02681 * age30-39 0.9086 0.1646 5.519 3.40e-08 *** age40-49 1.1892 0.2144 5.546 2.92e-08 *** educationlow -0.3250 0.1240 -2.620 0.00879 ** wantsMoreyes -0.8330 0.1175 -7.091 1.33e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 165.772 on 15 degrees of freedom Residual deviance: 29.917 on 10 degrees of freedom AIC: 113.43 Number of Fisher Scoring iterations: 4 65.10 ANOVA of Fit &gt; anova(myfit) Analysis of Deviance Table Model: binomial, link: logit Response: cbind(using, notUsing) Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev NULL 15 165.772 age 3 79.192 12 86.581 education 1 6.162 11 80.418 wantsMore 1 50.501 10 29.917 65.11 More on this Data Set See http://data.princeton.edu/R/glms.html for more on fitting logistic regression to this data set. A number of interesting choices are made that reveal more about the data. "],
["generalized-linear-models-1.html", "66 Generalized Linear Models 66.1 Definition 66.2 Exponential Family Distributions 66.3 Natural Single Parameter EFD 66.4 Dispersion EFDs 66.5 Example: Normal 66.6 EFD for GLMs 66.7 Components of a GLM 66.8 Link Functions 66.9 Calculating MLEs 66.10 Newton-Raphson 66.11 Fisher’s scoring 66.12 Iteratively Reweighted Least Squares 66.13 Estimating Dispersion 66.14 CLT Applied to the MLE 66.15 Approximately Pivotal Statistics 66.16 Deviance 66.17 Generalized LRT 66.18 Example: Grad School Admissions 66.19 glm() Function", " 66 Generalized Linear Models 66.1 Definition The generalized linear model (GLM) builds from OLS and GLS to allow for the case where \\(Y | {\\boldsymbol{X}}\\) is distributed according to an exponential family distribution. The estimated model is \\[ g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\] where \\(g(\\cdot)\\) is called the link function. This model is typically fit by numerical methods to calculate the maximum likelihood estimate of \\({\\boldsymbol{\\beta}}\\). 66.2 Exponential Family Distributions Recall that if \\(Y\\) follows an EFD then it has pdf of the form \\[f(y ; \\boldsymbol{\\theta}) = h(y) \\exp \\left\\{ \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(y) - A(\\boldsymbol{\\eta}) \\right\\} \\] where \\(\\boldsymbol{\\theta}\\) is a vector of parameters, \\(\\{T_k(y)\\}\\) are sufficient statistics, \\(A(\\boldsymbol{\\eta})\\) is the cumulant generating function. The functions \\(\\eta_k(\\boldsymbol{\\theta})\\) for \\(k=1, \\ldots, d\\) map the usual parameters \\({\\boldsymbol{\\theta}}\\) (often moments of the rv \\(Y\\)) to the natural parameters or canonical parameters. \\(\\{T_k(y)\\}\\) are sufficient statistics for \\(\\{\\eta_k\\}\\) due to the factorization theorem. \\(A(\\boldsymbol{\\eta})\\) is sometimes called the log normalizer because \\[A(\\boldsymbol{\\eta}) = \\log \\int h(y) \\exp \\left\\{ \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(y) \\right\\}.\\] 66.3 Natural Single Parameter EFD A natural single parameter EFD simplifies to the scenario where \\(d=1\\) and \\(T(y) = y\\) \\[f(y ; \\eta) = h(y) \\exp \\left\\{ \\eta(\\theta) y - A(\\eta(\\theta)) \\right\\} \\] where without loss of generality we can write \\({\\operatorname{E}}[Y] = \\theta\\). 66.4 Dispersion EFDs The family of distributions for which GLMs are most typically developed are dispersion EFDs. An example of a dispersion EFD that extends the natural single parameter EFD is \\[f(y ; \\eta) = h(y, \\phi) \\exp \\left\\{ \\frac{\\eta(\\theta) y - A(\\eta(\\theta))}{\\phi} \\right\\} \\] where \\(\\phi\\) is the dispersion parameter. 66.5 Example: Normal Let \\(Y \\sim \\mbox{Normal}(\\mu, \\sigma^2)\\). Then: \\[ \\theta = \\mu, \\eta(\\mu) = \\mu \\] \\[ \\phi = \\sigma^2 \\] \\[ A(\\mu) = \\frac{\\mu^2}{2} \\] \\[ h(y, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2}\\frac{y^2}{\\sigma^2}} \\] 66.6 EFD for GLMs There has been a very broad development of GLMs and extensions. A common setting for introducting GLMs is the dispersion EFD with a general link function \\(g(\\cdot)\\). See the classic text Generalized Linear Models, by McCullagh and Nelder, for such a development. 66.7 Components of a GLM Random: The particular exponential family distribution. \\[ Y \\sim f(y ; \\eta, \\phi) \\] Systematic: The determination of each \\(\\eta_i\\) from \\({\\boldsymbol{X}}_i\\) and \\({\\boldsymbol{\\beta}}\\). \\[ \\eta_i = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\] Parametric Link: The connection between \\(E[Y_i|{\\boldsymbol{X}}_i]\\) and \\({\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\). \\[ g(E[Y_i|{\\boldsymbol{X}}_i]) = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\] 66.8 Link Functions Even though the link function \\(g(\\cdot)\\) can be considered in a fairly general framework, the canonical link function \\(\\eta(\\cdot)\\) is often utilized. The canonical link function is the function that maps the expected value into the natural paramater. In this case, \\(Y | {\\boldsymbol{X}}\\) is distributed according to an exponential family distribution with \\[ \\eta \\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}. \\] 66.9 Calculating MLEs Given the model \\(g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\), the EFD should be fully parameterized. The Newton-Raphson method or Fisher’s scoring method can be utilized to find the MLE of \\({\\boldsymbol{\\beta}}\\). 66.10 Newton-Raphson Initialize \\({\\boldsymbol{\\beta}}^{(0)}\\). For \\(t = 1, 2, \\ldots\\) Calculate the score \\(s({\\boldsymbol{\\beta}}^{(t)}) = \\nabla \\ell({\\boldsymbol{\\beta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\mid_{{\\boldsymbol{\\beta}}= {\\boldsymbol{\\beta}}^{(t)}}\\) and observed Fisher information \\[H({\\boldsymbol{\\beta}}^{(t)}) = - \\nabla \\nabla^T \\ell({\\boldsymbol{\\beta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\mid_{{\\boldsymbol{\\beta}}= {\\boldsymbol{\\beta}}^{(t)}}\\]. Note that the observed Fisher information is also the negative Hessian matrix. Update \\({\\boldsymbol{\\beta}}^{(t+1)} = {\\boldsymbol{\\beta}}^{(t)} + H({\\boldsymbol{\\beta}}^{(t)})^{-1} s({\\boldsymbol{\\beta}}^{(t)})\\). Iterate until convergence, and set \\(\\hat{{\\boldsymbol{\\beta}}} = {\\boldsymbol{\\beta}}^{(\\infty)}\\). 66.11 Fisher’s scoring Initialize \\({\\boldsymbol{\\beta}}^{(0)}\\). For \\(t = 1, 2, \\ldots\\) Calculate the score \\(s({\\boldsymbol{\\beta}}^{(t)}) = \\nabla \\ell({\\boldsymbol{\\beta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\mid_{{\\boldsymbol{\\beta}}= {\\boldsymbol{\\beta}}^{(t)}}\\) and expected Fisher information \\[I({\\boldsymbol{\\beta}}^{(t)}) = - {\\operatorname{E}}\\left[\\nabla \\nabla^T \\ell({\\boldsymbol{\\beta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\mid_{{\\boldsymbol{\\beta}}= {\\boldsymbol{\\beta}}^{(t)}} \\right].\\] Update \\({\\boldsymbol{\\beta}}^{(t+1)} = {\\boldsymbol{\\beta}}^{(t)} + I({\\boldsymbol{\\beta}}^{(t)})^{-1} s({\\boldsymbol{\\beta}}^{(t)})\\). Iterate until convergence, and set \\(\\hat{{\\boldsymbol{\\beta}}} = {\\boldsymbol{\\beta}}^{(\\infty)}\\). When the canonical link function is used, the Newton-Raphson algorithm and Fisher’s scoring algorithm are equivalent. Exercise: Prove this. 66.12 Iteratively Reweighted Least Squares For the canonical link, Fisher’s scoring method can be written as an iteratively reweighted least squares algorithm, as shown earlier for logistic regression. Note that the Fisher information is \\[ I({\\boldsymbol{\\beta}}^{(t)}) = {\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}}\\] where \\({\\boldsymbol{W}}\\) is an \\(n \\times n\\) diagonal matrix with \\((i, i)\\) entry equal to \\({\\operatorname{Var}}(Y_i | {\\boldsymbol{X}}; {\\boldsymbol{\\beta}}^{(t)})\\). The score function is \\[ s({\\boldsymbol{\\beta}}^{(t)}) = {\\boldsymbol{X}}^T \\left( {\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}^{(t)} \\right) \\] and the current coefficient value \\({\\boldsymbol{\\beta}}^{(t)}\\) can be written as \\[ {\\boldsymbol{\\beta}}^{(t)} = ({\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}}{\\boldsymbol{\\beta}}^{(t)}. \\] Putting this together we get \\[ {\\boldsymbol{\\beta}}^{(t)} + I({\\boldsymbol{\\beta}}^{(t)})^{-1} s({\\boldsymbol{\\beta}}^{(t)}) = ({\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{z}}^{(t)} \\] where \\[ {\\boldsymbol{z}}^{(t)} = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}^{(t)} + {\\boldsymbol{W}}^{-1} \\left( {\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}^{(t)} \\right). \\] This is a generalization of the iteratively reweighted least squares algorithm we showed earlier for logistic regression. 66.13 Estimating Dispersion For the simple dispersion model above, it is typically straightforward to calculate the MLE \\(\\hat{\\phi}\\) once \\(\\hat{{\\boldsymbol{\\beta}}}\\) has been calculated. 66.14 CLT Applied to the MLE Given that \\(\\hat{{\\boldsymbol{\\beta}}}\\) is a maximum likelihood estimate, we have the following CLT result on its distribution as \\(n \\rightarrow \\infty\\): \\[ \\sqrt{n} (\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}}) \\stackrel{D}{\\longrightarrow} \\mbox{MVN}_{p}({\\boldsymbol{0}}, \\phi ({\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}})^{-1}) \\] 66.15 Approximately Pivotal Statistics The previous CLT gives us the following two approximations for pivtoal statistics. The first statistic facilitates getting overall measures of uncertainty on the estimate \\(\\hat{{\\boldsymbol{\\beta}}}\\). \\[ \\hat{\\phi}^{-1} (\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}})^T ({\\boldsymbol{X}}^T \\hat{{\\boldsymbol{W}}} {\\boldsymbol{X}}) (\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}}) {\\; \\stackrel{.}{\\sim}\\;}\\chi^2_1 \\] This second pivotal statistic allows for performing a Wald test or forming a confidence interval on each coefficient, \\(\\beta_j\\), for \\(j=1, \\ldots, p\\). \\[ \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\hat{\\phi} [({\\boldsymbol{X}}^T \\hat{{\\boldsymbol{W}}} {\\boldsymbol{X}})^{-1}]_{jj}}} {\\; \\stackrel{.}{\\sim}\\;}\\mbox{Normal}(0,1) \\] 66.16 Deviance Let \\(\\hat{\\boldsymbol{\\eta}}\\) be the estimated natural parameters from a GLM. For example, \\(\\hat{\\boldsymbol{\\eta}} = {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}\\) when the canonical link function is used. Let \\(\\hat{\\boldsymbol{\\eta}}_n\\) be the saturated model wwhere \\(Y_i\\) is directly used to estimate \\(\\eta_i\\) without model constraints. For example, in the Bernoulli logistic regression model \\(\\hat{\\boldsymbol{\\eta}}_n = {\\boldsymbol{Y}}\\), the observed outcomes. The deviance for the model is defined to be \\[ D\\left(\\hat{\\boldsymbol{\\eta}}\\right) = 2 \\ell(\\hat{\\boldsymbol{\\eta}}_n; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) - 2 \\ell(\\hat{\\boldsymbol{\\eta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\] 66.17 Generalized LRT Let \\({\\boldsymbol{X}}_0\\) be a subset of \\(p_0\\) columns of \\({\\boldsymbol{X}}\\) and let \\({\\boldsymbol{X}}_1\\) be a subset of \\(p_1\\) columns, where \\(1 \\leq p_0 &lt; p_1 \\leq p\\). Also, assume that the columns of \\({\\boldsymbol{X}}_0\\) are a subset of \\({\\boldsymbol{X}}_1\\). Without loss of generality, suppose that \\({\\boldsymbol{\\beta}}_0 = (\\beta_1, \\beta_2, \\ldots, \\beta_{p_0})^T\\) and \\({\\boldsymbol{\\beta}}_1 = (\\beta_1, \\beta_2, \\ldots, \\beta_{p_1})^T\\). Suppose we wish to test \\(H_0: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) = {\\boldsymbol{0}}\\) vs \\(H_1: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) \\not= {\\boldsymbol{0}}\\) We can form \\(\\hat{\\boldsymbol{\\eta}}_0 = {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}_0\\) from the GLM model \\(g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}_0]\\right) = {\\boldsymbol{X}}_0 {\\boldsymbol{\\beta}}_0\\). We can analogously form \\(\\hat{\\boldsymbol{\\eta}}_1 = {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}_1\\) from the GLM model \\(g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}_1]\\right) = {\\boldsymbol{X}}_1 {\\boldsymbol{\\beta}}_1\\). The \\(2\\log\\) generalized LRT can then be formed from the two deviance statistics \\[ 2 \\log \\lambda({\\boldsymbol{X}}, {\\boldsymbol{Y}}) = 2 \\log \\frac{L(\\hat{\\boldsymbol{\\eta}}_1; {\\boldsymbol{X}}, {\\boldsymbol{Y}})}{L(\\hat{\\boldsymbol{\\eta}}_0; {\\boldsymbol{X}}, {\\boldsymbol{Y}})} = D\\left(\\hat{\\boldsymbol{\\eta}}_0\\right) - D\\left(\\hat{\\boldsymbol{\\eta}}_1\\right) \\] where the null distribution is \\(\\chi^2_{p_1-p_0}\\). 66.18 Example: Grad School Admissions Let’s revisit a logistic regression example now that we know how the various statistics are calculated. &gt; mydata &lt;- + read.csv(&quot;http://www.ats.ucla.edu/stat/data/binary.csv&quot;) &gt; dim(mydata) &gt; head(mydata) Fit the model with basic output. Note the argument family = &quot;binomial&quot;. &gt; mydata$rank &lt;- factor(mydata$rank, levels=c(1, 2, 3, 4)) &gt; myfit &lt;- glm(admit ~ gre + gpa + rank, + data = mydata, family = &quot;binomial&quot;) &gt; myfit Call: glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, data = mydata) Coefficients: (Intercept) gre gpa rank2 rank3 -3.989979 0.002264 0.804038 -0.675443 -1.340204 rank4 -1.551464 Degrees of Freedom: 399 Total (i.e. Null); 394 Residual Null Deviance: 500 Residual Deviance: 458.5 AIC: 470.5 This shows the fitted coefficient values, which is on the link function scale – logit aka log odds here. Also, a Wald test is performed for each coefficient. &gt; summary(myfit) Call: glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, data = mydata) Deviance Residuals: Min 1Q Median 3Q Max -1.6268 -0.8662 -0.6388 1.1490 2.0790 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.989979 1.139951 -3.500 0.000465 *** gre 0.002264 0.001094 2.070 0.038465 * gpa 0.804038 0.331819 2.423 0.015388 * rank2 -0.675443 0.316490 -2.134 0.032829 * rank3 -1.340204 0.345306 -3.881 0.000104 *** rank4 -1.551464 0.417832 -3.713 0.000205 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 458.52 on 394 degrees of freedom AIC: 470.52 Number of Fisher Scoring iterations: 4 Here we perform a generalized LRT on each variable. Note the rank variable is now tested as a single factor variable as opposed to each dummy variable. &gt; anova(myfit, test=&quot;LRT&quot;) Analysis of Deviance Table Model: binomial, link: logit Response: admit Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 399 499.98 gre 1 13.9204 398 486.06 0.0001907 *** gpa 1 5.7122 397 480.34 0.0168478 * rank 3 21.8265 394 458.52 7.088e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; mydata &lt;- data.frame(mydata, probs = myfit$fitted.values) &gt; ggplot(mydata) + geom_point(aes(x=gpa, y=probs, color=rank)) + + geom_jitter(aes(x=gpa, y=admit), width=0, height=0.01, alpha=0.3) &gt; ggplot(mydata) + geom_point(aes(x=gre, y=probs, color=rank)) + + geom_jitter(aes(x=gre, y=admit), width=0, height=0.01, alpha=0.3) &gt; ggplot(mydata) + geom_boxplot(aes(x=rank, y=probs)) + + geom_jitter(aes(x=rank, y=probs), width=0.1, height=0.01, alpha=0.3) 66.19 glm() Function The glm() function has many different options available to the user. glm(formula, family = gaussian, data, weights, subset, na.action, start = NULL, etastart, mustart, offset, control = list(...), model = TRUE, method = &quot;glm.fit&quot;, x = FALSE, y = TRUE, contrasts = NULL, ...) To see the different link functions available, type: help(family) "],
["nonparametric-regression.html", "67 Nonparametric Regression 67.1 Simple Linear Regression 67.2 Simple Nonparametric Regression 67.3 Smooth Functions 67.4 Smoothness Parameter \\(\\lambda\\) 67.5 The Solution 67.6 Natural Cubic Splines 67.7 Basis Functions 67.8 Calculating the Solution 67.9 Linear Operator 67.10 Degrees of Freedom 67.11 Bayesian Intepretation 67.12 Bias and Variance Trade-off 67.13 Choosing \\(\\lambda\\) 67.14 Smoothers and Spline Models 67.15 Smoothers in R 67.16 Example", " 67 Nonparametric Regression 67.1 Simple Linear Regression Recall the set up for simple linear regression. For random variables \\((X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\), simple linear regression estimates the model \\[ Y_i = \\beta_1 + \\beta_2 X_i + E_i \\] where \\({\\operatorname{E}}[E_i | X_i] = 0\\), \\({\\operatorname{Var}}(E_i | X_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j | X_i, X_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). Note that in this model \\({\\operatorname{E}}[Y | X] = \\beta_1 + \\beta_2 X.\\) 67.2 Simple Nonparametric Regression In simple nonparametric regression, we define a similar model while eliminating the linear assumption: \\[ Y_i = s(X_i) + E_i \\] for some function \\(s(\\cdot)\\) with the same assumptions on the distribution of \\({\\boldsymbol{E}}| {\\boldsymbol{X}}\\). In this model, we also have \\[ {\\operatorname{E}}[Y | X] = s(X). \\] 67.3 Smooth Functions Suppose we consider fitting the model \\(Y_i = s(X_i) + E_i\\) with the restriction that \\(s \\in C^2\\), the class of functions with continuous second derivatives. We can set up an objective function that regularizes how smooth vs wiggly \\(s\\) is. Specifically, suppose for a given set of observed data \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\) we wish to identify a function \\(s \\in C^2\\) that minimizes for some \\(\\lambda\\) \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s&#39;&#39;(x)|^2 dx \\] 67.4 Smoothness Parameter \\(\\lambda\\) When minimizing \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s^{\\prime\\prime}(x)|^2 dx \\] it follows that if \\(\\lambda=0\\) then any function \\(s \\in C^2\\) that interpolates the data is a solution. As \\(\\lambda \\rightarrow \\infty\\), then the minimizing function is the simple linear regression solution. 67.5 The Solution For an observed data set \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\) where \\(n \\geq 4\\) and a fixed value \\(\\lambda\\), there is an exact solution to minimizing \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s^{\\prime\\prime}(x)|^2 dx. \\] The solution is called a natural cubic spline, which is constructed to have knots at \\(x_1, x_2, \\ldots, x_n\\). 67.6 Natural Cubic Splines Suppose without loss of generality that we have ordered \\(x_1 &lt; x_2 &lt; \\cdots &lt; x_n\\). We assume all \\(x_i\\) are unique to simplify the explanation here, but ties can be deal with. A natural cubic spline (NCS) is a function constructed from a set of piecewise cubic functions over the range \\([x_1, x_n]\\) joined at the knots so that the second derivative is continuous at the knots. Outside of the range (\\(&lt; x_1\\) or \\(&gt; x_n\\)), the spline is linear and it has continuous second derivatives at the endpoint knots. 67.7 Basis Functions Depending on the value \\(\\lambda\\), a different ncs will be constructed, but the entire family of ncs solutions over \\(0 &lt; \\lambda &lt; \\infty\\) can be constructed from a common set of basis functions. We construct \\(n\\) basis functions \\(N_1(x), N_2(x), \\ldots, N_n(x)\\) with coefficients \\(\\theta_1(\\lambda), \\theta_2(\\lambda), \\ldots, \\theta_n(\\lambda)\\). The NCS takes the form \\[ s(x) = \\sum_{i=1}^n \\theta_i(\\lambda) N_i(x). \\] Define \\(N_1(x) = 1\\) and \\(N_2(x) = x\\). For \\(i = 3, \\ldots, n\\), define \\(N_i(x) = d_{i-1}(x) - d_{i-2}(x)\\) where \\[ d_{i}(x) = \\frac{(x - x_i)^3 - (x - x_n)^3}{x_n - x_i}. \\] Recall that we’ve labeled indices so that \\(x_1 &lt; x_2 &lt; \\cdots &lt; x_n\\). 67.8 Calculating the Solution Let \\({\\boldsymbol{\\theta}}_\\lambda = (\\theta_1(\\lambda), \\theta_2(\\lambda), \\ldots, \\theta_n(\\lambda))^T\\) and let \\({\\boldsymbol{N}}\\) be the \\(n \\times n\\) matrix with \\((i, j)\\) entry equal to \\(N_j(x_i)\\). Finally, let \\({\\boldsymbol{\\Omega}}\\) be the \\(n \\times n\\) matrix with \\((i,j)\\) entry equal to \\(\\int N_i^{\\prime\\prime}(x) N_j^{\\prime\\prime}(x) dx\\). The solution to \\({\\boldsymbol{\\theta}}_\\lambda\\) are the values that minimize \\[ ({\\boldsymbol{y}}- {\\boldsymbol{N}}{\\boldsymbol{\\theta}})^T ({\\boldsymbol{y}}- {\\boldsymbol{N}}{\\boldsymbol{\\theta}}) + \\lambda {\\boldsymbol{\\theta}}^T {\\boldsymbol{\\Omega}}{\\boldsymbol{\\theta}}. \\] which results in \\[ \\hat{{\\boldsymbol{\\theta}}}_\\lambda = ({\\boldsymbol{N}}^T {\\boldsymbol{N}}+ \\lambda {\\boldsymbol{\\Omega}})^{-1} {\\boldsymbol{N}}^T {\\boldsymbol{y}}. \\] 67.9 Linear Operator Letting \\[ {\\boldsymbol{S}}_\\lambda = {\\boldsymbol{N}}({\\boldsymbol{N}}^T {\\boldsymbol{N}}+ \\lambda {\\boldsymbol{\\Omega}})^{-1} {\\boldsymbol{N}}^T \\] it folows that the fitted values are \\[ \\hat{{\\boldsymbol{y}}} = {\\boldsymbol{S}}_\\lambda {\\boldsymbol{y}}. \\] Thus, the fitted values from a NCS are contructed by taking linear combination of the response variable values \\(y_1, y_2, \\ldots, y_n\\). 67.10 Degrees of Freedom Recall that in OLS, we formed projection matrix \\({\\boldsymbol{P}}= {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T\\) and noted that the number of columns \\(p\\) of \\({\\boldsymbol{X}}\\) is also found in the trace of \\({\\boldsymbol{P}}\\) where \\(\\operatorname{tr}({\\boldsymbol{P}}) = p\\). The effective degrees of freedom for a model fit by a linear operator is calculated as the trace of the operator. Therefore, for a given \\(\\lambda\\), the effective degrees of freedom is \\[ \\operatorname{df}_\\lambda = \\operatorname{tr}({\\boldsymbol{S}}_\\lambda). \\] 67.11 Bayesian Intepretation Minimizing \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s^{\\prime\\prime}(x)|^2 dx \\] is equivalent to maximizing \\[ \\exp\\left\\{ -\\frac{\\sum_{i=1}^n (y_i - s(x_i))^2}{2\\sigma^2} \\right\\} \\exp\\left\\{-\\frac{\\lambda}{2\\sigma^2} \\int |s^{\\prime\\prime}(x)|^2 dx\\right\\}. \\] Therefore, the NCS solution can be interpreted as calculting the MAP where \\(Y | X\\) is Normal and there’s an Exponential prior on the smoothness of \\(s\\). 67.12 Bias and Variance Trade-off Typically we will choose some \\(0 &lt; \\lambda &lt; \\infty\\) in an effort to balance the bias and variance. Let \\(\\hat{Y} = \\hat{s}(X; \\lambda)\\) where \\(\\hat{s}(\\cdot; \\lambda)\\) minimizes the above for some chosen \\(\\lambda\\) on an independent data set. Then \\[ \\begin{aligned} {\\operatorname{E}}\\left[\\left(Y - \\hat{Y}\\right)^2\\right] &amp; = {\\rm E}\\left[\\left(s(x) + E - \\hat{s}(x; \\lambda)\\right)^2 \\right] \\\\ \\ &amp; = {\\rm E}\\left[\\left( s(x) - \\hat{s}(x; \\lambda) \\right)^2 \\right] + {\\rm Var}(E) \\\\ \\ &amp; = \\left( s(x) - {\\rm E}[\\hat{s}(x; \\lambda)]\\right)^2 + {\\rm Var}\\left(\\hat{s}(x; \\lambda)\\right) + {\\rm Var}(E) \\\\ \\ &amp; = \\mbox{bias}^2_{\\lambda} + \\mbox{variance}_{\\lambda} + {\\rm Var}(E) \\end{aligned} \\] where all of the above calculations are conditioned on \\(X=x\\). In minimizing \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s^{\\prime\\prime}(x)|^2 dx \\] the relationship is such that: \\[ \\uparrow \\lambda \\Longrightarrow \\mbox{bias}^2 \\uparrow, \\mbox{variance} \\downarrow \\] \\[ \\downarrow \\lambda \\Longrightarrow \\mbox{bias}^2 \\downarrow, \\mbox{variance} \\uparrow \\] 67.13 Choosing \\(\\lambda\\) There are several approaches that are commonly used to identify a value of \\(\\lambda\\), including: Scientific knowledge that guides the acceptable value of \\(\\operatorname{df}_\\lambda\\) Cross-validation or some other prediction quality measure Model selection measures, such as Akaike information criterion (AIC) or Mallows \\(C_p\\) 67.14 Smoothers and Spline Models We investigated one type of nonparametric regression model here, the NCS. However, in general there are many such “smoother” methods available in the simple nonparametric regression scenario. Splines are particularly popular since splines are constructed from putting together polynomials and are therefore usually tractable to compute and analyze. 67.15 Smoothers in R There are several functions and packages available in R for computing smoothers and tuning smoothness parameters. Examples include: splines library smooth.spline() loess() lowess() 67.16 Example &gt; y2 &lt;- smooth.spline(x=x, y=y, df=2) &gt; y5 &lt;- smooth.spline(x=x, y=y, df=5) &gt; y25 &lt;- smooth.spline(x=x, y=y, df=25) &gt; ycv &lt;- smooth.spline(x=x, y=y) &gt; ycv Call: smooth.spline(x = x, y = y) Smoothing Parameter spar= 0.5162045 lambda= 0.0002730906 (11 iterations) Equivalent Degrees of Freedom (Df): 7.293673 Penalized Criterion (RSS): 14.80602 GCV: 1.180651 "],
["generalized-additive-models-1.html", "68 Generalized Additive Models 68.1 Ordinary Least Squares 68.2 Additive Models 68.3 Backfitting 68.4 GAM Definition 68.5 Overview of Fitting GAMs 68.6 GAMs in R 68.7 Example", " 68 Generalized Additive Models 68.1 Ordinary Least Squares Recall that OLS estimates the model \\[ \\begin{aligned} Y_i &amp; = \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + E_i \\\\ &amp; = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}+ E_i \\end{aligned} \\] where \\({\\operatorname{E}}[{\\boldsymbol{E}}| {\\boldsymbol{X}}] = {\\boldsymbol{0}}\\) and \\({\\operatorname{Cov}}({\\boldsymbol{E}}| {\\boldsymbol{X}}) = \\sigma^2 {\\boldsymbol{I}}\\). 68.2 Additive Models The additive model (which could also be called “ordinary nonparametric additive regression”) is of the form \\[ \\begin{aligned} Y_i &amp; = s_1(X_{i1}) + s_2(X_{i2}) + \\ldots + s_p(X_{ip}) + E_i \\\\ &amp; = \\sum_{j=1}^p s_j(X_{ij}) + E_i \\end{aligned} \\] where the \\(s_j(\\cdot)\\) for \\(j = 1, \\ldots, p\\) are a set of nonparametric (or flexible) functions. Again, we assume that \\({\\operatorname{E}}[{\\boldsymbol{E}}| {\\boldsymbol{X}}] = {\\boldsymbol{0}}\\) and \\({\\operatorname{Cov}}({\\boldsymbol{E}}| {\\boldsymbol{X}}) = \\sigma^2 {\\boldsymbol{I}}\\). 68.3 Backfitting The additive model can be fit through a technique called backfitting. Intialize \\(s^{(0)}_j(x)\\) for \\(j = 1, \\ldots, p\\). For \\(t=1, 2, \\ldots\\), fit \\(s_j^{(t)}(x)\\) on response variable \\[y_i - \\sum_{k \\not= j} s_k^{(t-1)}(x_{ij}).\\] Repeat until convergence. Note that some extra steps have to be taken to deal with the intercept. 68.4 GAM Definition \\(Y | {\\boldsymbol{X}}\\) is distributed according to an exponential family distribution. The extension of additive models to this family of response variable is called generalized additive models (GAMs). The model is of the form \\[ g\\left({\\operatorname{E}}[Y_i | {\\boldsymbol{X}}_i]\\right) = \\sum_{j=1}^p s_j(X_{ij}) \\] where \\(g(\\cdot)\\) is the link function and the \\(s_j(\\cdot)\\) are flexible and/or nonparametric functions. 68.5 Overview of Fitting GAMs Fitting GAMs involves putting together the following three tools: We know how to fit a GLM via IRLS We know how to fit a smoother of a single explanatory variable via a least squares solution, as seen for the NCS We know how to combine additive smoothers by backfitting 68.6 GAMs in R Three common ways to fit GAMs in R: Utilize glm() on explanatory variables constructed from ns() or bs() The gam library The mgcv library 68.7 Example &gt; set.seed(508) &gt; x1 &lt;- seq(1, 10, length.out=50) &gt; n &lt;- length(x1) &gt; x2 &lt;- rnorm(n) &gt; f &lt;- 4*log(x1) + sin(x1) - 7 + 0.5*x2 &gt; p &lt;- exp(f)/(1+exp(f)) &gt; summary(p) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.001842 0.074171 0.310674 0.436162 0.860387 0.944761 &gt; y &lt;- rbinom(n, size=1, prob=p) &gt; mean(y) [1] 0.42 &gt; df &lt;- data.frame(x1=x1, x2=x2, y=y) Here, we use the gam() function from the mgcv library. It automates choosing the smoothness of the splines. &gt; library(mgcv) &gt; mygam &lt;- gam(y ~ s(x1) + s(x2), family = binomial(), data=df) &gt; library(broom) &gt; tidy(mygam) # A tibble: 2 x 5 term edf ref.df statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 s(x1) 1.87 2.37 12.7 0.00531 2 s(x2) 1.00 1.00 1.16 0.281 &gt; summary(mygam) Family: binomial Link function: logit Formula: y ~ s(x1) + s(x2) Parametric coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.1380 0.6723 -1.693 0.0905 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Approximate significance of smooth terms: edf Ref.df Chi.sq p-value s(x1) 1.87 2.375 12.743 0.00531 ** s(x2) 1.00 1.000 1.163 0.28084 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 R-sq.(adj) = 0.488 Deviance explained = 47% UBRE = -0.12392 Scale est. = 1 n = 50 True probabilities vs. estimated probabilities. &gt; plot(p, mygam$fitted.values, pch=19); abline(0,1) Smoother estimated for x1. &gt; plot(mygam, select=1) Smoother estimated for x2. &gt; plot(mygam, select=2) Here, we use the glm() function and include as an explanatory variable a NCS built from the ns() function from the splines library. We include a df argument in the ns() call. &gt; library(splines) &gt; myglm &lt;- glm(y ~ ns(x1, df=2) + x2, family = binomial(), data=df) &gt; tidy(myglm) # A tibble: 4 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -10.9 5.31 -2.06 0.0396 2 ns(x1, df = 2)1 21.4 10.1 2.11 0.0348 3 ns(x1, df = 2)2 6.33 2.11 3.00 0.00272 4 x2 0.734 0.609 1.21 0.228 The spline basis evaluated at x1 values. &gt; ns(x1, df=2) 1 2 [1,] 0.00000000 0.00000000 [2,] 0.03114456 -0.02075171 [3,] 0.06220870 -0.04138180 [4,] 0.09311200 -0.06176867 [5,] 0.12377405 -0.08179071 [6,] 0.15411442 -0.10132630 [7,] 0.18405270 -0.12025384 [8,] 0.21350847 -0.13845171 [9,] 0.24240131 -0.15579831 [10,] 0.27065081 -0.17217201 [11,] 0.29817654 -0.18745121 [12,] 0.32489808 -0.20151430 [13,] 0.35073503 -0.21423967 [14,] 0.37560695 -0.22550571 [15,] 0.39943343 -0.23519080 [16,] 0.42213406 -0.24317334 [17,] 0.44362840 -0.24933170 [18,] 0.46383606 -0.25354429 [19,] 0.48267660 -0.25568949 [20,] 0.50006961 -0.25564569 [21,] 0.51593467 -0.25329128 [22,] 0.53019136 -0.24850464 [23,] 0.54275927 -0.24116417 [24,] 0.55355797 -0.23114825 [25,] 0.56250705 -0.21833528 [26,] 0.56952943 -0.20260871 [27,] 0.57462513 -0.18396854 [28,] 0.57787120 -0.16253131 [29,] 0.57934806 -0.13841863 [30,] 0.57913614 -0.11175212 [31,] 0.57731586 -0.08265339 [32,] 0.57396762 -0.05124405 [33,] 0.56917185 -0.01764570 [34,] 0.56300897 0.01802003 [35,] 0.55555939 0.05563154 [36,] 0.54690354 0.09506722 [37,] 0.53712183 0.13620546 [38,] 0.52629468 0.17892464 [39,] 0.51450251 0.22310315 [40,] 0.50182573 0.26861939 [41,] 0.48834478 0.31535174 [42,] 0.47414005 0.36317859 [43,] 0.45929198 0.41197833 [44,] 0.44388099 0.46162934 [45,] 0.42798748 0.51201003 [46,] 0.41169188 0.56299877 [47,] 0.39507460 0.61447395 [48,] 0.37821607 0.66631397 [49,] 0.36119670 0.71839720 [50,] 0.34409692 0.77060206 attr(,&quot;degree&quot;) [1] 3 attr(,&quot;knots&quot;) 50% 5.5 attr(,&quot;Boundary.knots&quot;) [1] 1 10 attr(,&quot;intercept&quot;) [1] FALSE attr(,&quot;class&quot;) [1] &quot;ns&quot; &quot;basis&quot; &quot;matrix&quot; Plot of basis function values vs x1. &gt; summary(myglm) Call: glm(formula = y ~ ns(x1, df = 2) + x2, family = binomial(), data = df) Deviance Residuals: Min 1Q Median 3Q Max -2.0214 -0.3730 -0.0162 0.5762 1.7616 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -10.9229 5.3079 -2.058 0.03960 * ns(x1, df = 2)1 21.3848 10.1318 2.111 0.03480 * ns(x1, df = 2)2 6.3266 2.1103 2.998 0.00272 ** x2 0.7342 0.6089 1.206 0.22795 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 68.029 on 49 degrees of freedom Residual deviance: 35.682 on 46 degrees of freedom AIC: 43.682 Number of Fisher Scoring iterations: 7 &gt; anova(myglm, test=&quot;LRT&quot;) Analysis of Deviance Table Model: binomial, link: logit Response: y Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 49 68.029 ns(x1, df = 2) 2 30.755 47 37.274 2.097e-07 *** x2 1 1.592 46 35.682 0.207 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 True probabilities vs. estimated probabilities. &gt; plot(p, myglm$fitted.values, pch=19); abline(0,1) "],
["bootstrap-for-statistical-models.html", "69 Bootstrap for Statistical Models 69.1 Homoskedastic Models 69.2 Residuals 69.3 Studentized Residuals 69.4 Confidence Intervals 69.5 Hypothesis Testing 69.6 Parametric Bootstrap", " 69 Bootstrap for Statistical Models 69.1 Homoskedastic Models Let’s first discuss how one can utilize the bootstrap on any of the three homoskedastic models: Simple linear regression Ordinary least squares Additive models 69.2 Residuals In each of these scenarios we sample data \\(({\\boldsymbol{X}}_1, Y_1), ({\\boldsymbol{X}}_2, Y_2), \\ldots, ({\\boldsymbol{X}}_n, Y_n)\\). Let suppose we calculate fitted values \\(\\hat{Y}_i\\) and they are unbiased: \\[ {\\operatorname{E}}[\\hat{Y}_i | {\\boldsymbol{X}}] = {\\operatorname{E}}[Y_i | {\\boldsymbol{X}}]. \\] We can calculate residuals \\(\\hat{E}_i = Y_i - \\hat{Y}_i\\) for \\(i=1, 2, \\ldots, n\\). 69.3 Studentized Residuals One complication is that the residuals have a covariance. For example, in OLS we showed that \\[ {\\operatorname{Cov}}(\\hat{{\\boldsymbol{E}}}) = \\sigma^2 ({\\boldsymbol{I}}- {\\boldsymbol{P}}) \\] where \\({\\boldsymbol{P}}= {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T\\). To correct for this induced heteroskedasticity, we studentize the residuals by calculating \\[ R_i = \\frac{\\hat{E}_i}{\\sqrt{1-P_{ii}}} \\] which gives \\({\\operatorname{Cov}}({\\boldsymbol{R}}) = \\sigma^2 {\\boldsymbol{I}}\\). 69.4 Confidence Intervals The following is a bootstrap procedure for calculating a confidence interval on some statistic \\(\\hat{\\theta}\\) calculated from a homoskedastic model fit. An example is \\(\\hat{\\beta}_j\\) in an OLS. Fit the model to obtain fitted values \\(\\hat{Y}_i\\), studentized residuals \\(R_i\\), and the statistic of interest \\(\\hat{\\theta}\\). For \\(b = 1, 2, \\ldots, B\\). Sample \\(n\\) observations with replacement from \\(\\{R_i\\}_{i=1}^n\\) to obtain bootstrap residuals \\(R_1^{*}, R_2^{*}, \\ldots, R_n^{*}\\). Form new response variables \\(Y_i^{*} = \\hat{Y}_i + R_i^{*}\\). Fit the model to obtain \\(\\hat{Y}^{*}_i\\) and all other fitted parameters. Calculate statistic of interest \\(\\hat{\\theta}^{*(b)}\\). The bootstrap statistics \\(\\hat{\\theta}^{*(1)}, \\hat{\\theta}^{*(2)}, \\ldots, \\hat{\\theta}^{*(B)}\\) are then utilized through one of the techniques discussed earlier (percentile, pivotal, studentized pivotal) to calculate a bootstrap CI. 69.5 Hypothesis Testing Suppose we are testing the hypothesis \\(H_0: {\\operatorname{E}}[Y | {\\boldsymbol{X}}] = f_0({\\boldsymbol{X}})\\) vs \\(H_1: {\\operatorname{E}}[Y | {\\boldsymbol{X}}] = f_1({\\boldsymbol{X}})\\). Suppose it is possible to form unbiased estimates \\(f_0({\\boldsymbol{X}})\\) and \\(f_1({\\boldsymbol{X}})\\) given \\({\\boldsymbol{X}}\\), and \\(f_0\\) is a restricted version of \\(f_1\\). Suppose also we have a statistic \\(T(\\hat{f}_0, \\hat{f}_1)\\) for performing this test so that the larger the statistic, the more evidence there is against the null hypothesis in favor of the alternative. The big picture strategy is to bootstrap studentized residuals from the unconstrained (alternative hypothesis) fitted model and then add those to the constrained (null hypothesis) fitted model to generate bootstrap null data sets. Fit the models to obtain fitted values \\(\\hat{f}_0({\\boldsymbol{X}}_i)\\) and \\(\\hat{f}_1({\\boldsymbol{X}}_i)\\), studentized residuals \\(R_i\\) from the fit \\(\\hat{f}_1({\\boldsymbol{X}}_i)\\), and the observed statistic \\(T(\\hat{f}_0, \\hat{f}_1)\\). For \\(b = 1, 2, \\ldots, B\\). Sample \\(n\\) observations with replacement from \\(\\{R_i\\}_{i=1}^n\\) to obtain bootstrap residuals \\(R_1^{*}, R_2^{*}, \\ldots, R_n^{*}\\). Form new response variables \\(Y_i^{*} = \\hat{f}_0({\\boldsymbol{X}}_i) + R_i^{*}\\). Fit the models on the response variables \\(Y_i^{*}\\) to obtain \\(\\hat{f}^{*}_0\\) and \\(\\hat{f}^{*}_1\\). Calculate statistic \\(T(\\hat{f}^{*(b)}_0, \\hat{f}^{*(b)}_1)\\). The p-value is then calculated as \\[ \\frac{\\sum_{b=1}^B 1\\left(T(\\hat{f}^{*(b)}_0, \\hat{f}^{*(b)}_1) \\geq T(\\hat{f}_0, \\hat{f}_1) \\right) }{B} \\] 69.6 Parametric Bootstrap For more complex scenarios, such as GLMs, GAMs, and heteroskedastic models, it is typically more straightforward to utilize a parametric bootstrap. "],
["high-dimensional-data-and-inference.html", "70 High-Dimensional Data and Inference 70.1 Definition 70.2 Examples 70.3 HD Gene Expression Data 70.4 Many Responses Model 70.5 HD SNP Data 70.6 Many Regressors Model 70.7 Goals 70.8 Challenges", " 70 High-Dimensional Data and Inference 70.1 Definition High-dimesional inference is the scenario where we perform inference simultaneously on “many” paramaters. “Many” can be as few as three parameters (which is where things start to get interesting), but in modern applications this is typically on the order of thousands to billions of paramaters. High-dimesional data is a data set where the number of variables measured is many. Large sample size data is a data set where few variables are measured, but many observations are measured. Big data is a data set where there are so many data points that it cannot be managed straightforwardly in memory, but must rather be stored and accessed elsewhere. Big data can be high-dimensional, large sample size, or both. We will abbreviate high-dimensional with HD. 70.2 Examples In all of these examples, many measurements are taken and the goal is often to perform inference on many paramaters simultaneously. Spatial epidemiology Environmental monitoring Internet user behavior Genomic profiling Neuroscience imaging Financial time series 70.3 HD Gene Expression Data &gt; knitr::include_graphics(&quot;./images/rna_sequencing.png&quot;) It’s possible to measure the level of gene expression – how much mRNA is being transcribed – from thousands of cell simultaneously in a single biological sample. Typically, gene expression is measured over varying biological conditions, and the goal is to perform inference on the relationship between expression and the varying conditions. This results in thousands of simultaneous inferences. The typical sizes of these data sets are 1000 to 50,000 genes and 10 to 1000 observations. The gene expression values are typically modeled as approximately Normal or overdispersed Poisson. There is usually shared signal among the genes, and there are often unobserved latent variables. \\[ \\begin{array}{rc} &amp; {\\boldsymbol{Y}}_{m \\times n} \\\\ &amp; \\text{observations} \\\\ \\text{genes} &amp; \\begin{bmatrix} y_{11} &amp; y_{12} &amp; \\cdots &amp; y_{1n} \\\\ y_{21} &amp; y_{22} &amp; \\cdots &amp; y_{2n} \\\\ &amp; &amp; &amp; \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; \\\\ y_{m1} &amp; y_{m2} &amp; \\cdots &amp; y_{mn} \\end{bmatrix} \\\\ &amp; \\\\ &amp; {\\boldsymbol{X}}_{d \\times n} \\ \\text{study design} \\\\ &amp; \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{d1} &amp; x_{d2} &amp; \\cdots &amp; x_{dn} \\end{bmatrix} \\end{array} \\] The \\({\\boldsymbol{Y}}\\) matrix contains gene expression measurements for \\(m\\) genes (rows) by \\(n\\) observations (columns). The values \\(y_{ij}\\) are either in \\(\\mathbb{R}\\) or \\(\\mathbb{Z}^{+} = \\{0, 1, 2, \\ldots \\}\\). The \\({\\boldsymbol{X}}\\) matrix contains the study design of \\(d\\) explanatory variables (rows) by the \\(n\\) observations (columns). Note that \\(m \\gg n \\gg d\\). 70.4 Many Responses Model Gene expression is an example of what I call the many responses model. We’re interested in performing simultaneous inference on \\(d\\) paramaters for each of \\(m\\) models such as: \\[ \\begin{aligned} &amp; {\\boldsymbol{Y}}_{1} = {\\boldsymbol{\\beta}}_1 {\\boldsymbol{X}}+ {\\boldsymbol{E}}_1 \\\\ &amp; {\\boldsymbol{Y}}_{2} = {\\boldsymbol{\\beta}}_2 {\\boldsymbol{X}}+ {\\boldsymbol{E}}_2 \\\\ &amp; \\vdots \\\\ &amp; {\\boldsymbol{Y}}_{m} = {\\boldsymbol{\\beta}}_m {\\boldsymbol{X}}+ {\\boldsymbol{E}}_m \\\\ \\end{aligned} \\] For example, \\({\\boldsymbol{Y}}_{1} = {\\boldsymbol{\\beta}}_1 {\\boldsymbol{X}}+ {\\boldsymbol{E}}_1\\) is vector notation of (in terms of observations \\(j\\)): \\[ \\left\\{ Y_{1j} = \\beta_{11} X_{1j} + \\beta_{12} X_{2j} + \\cdots + \\beta_{1d} X_{dj} + E_{1j} \\right\\}_{j=1}^n \\] We have made two changes from last week: We have transposed \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{\\beta}}\\). We have changed the number of explanatory variables from \\(p\\) to \\(d\\). Let \\({\\boldsymbol{B}}_{m \\times d}\\) be the matrix of parameters \\((\\beta_{ik})\\) relating the \\(m\\) response variables to the \\(d\\) explanatory variables. The full HD model is \\[ \\begin{array}{cccccc} {\\boldsymbol{Y}}_{m \\times n} &amp; = &amp; {\\boldsymbol{B}}_{m \\times d} &amp; {\\boldsymbol{X}}_{d \\times n} &amp; + &amp; {\\boldsymbol{E}}_{m \\times n} \\\\ \\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ Y_{i1} &amp; \\cdots &amp; Y_{in}\\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} &amp; \\\\ &amp; \\\\ &amp; \\\\ \\beta_{i1} &amp; \\beta_{id} \\\\ &amp; \\\\ &amp; \\\\ &amp; \\\\ \\end{bmatrix} &amp; \\begin{bmatrix} X_{11} &amp; \\cdots &amp; X_{1n} \\\\ X_{d1} &amp; \\cdots &amp; X_{dn} \\\\ \\end{bmatrix} &amp; + &amp; \\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ E_{i1} &amp; \\cdots &amp; E_{in} \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix} \\end{array} \\] Note that if we make OLS assumptions, then we can calculate: \\[ \\hat{{\\boldsymbol{B}}}^{\\text{OLS}} = {\\boldsymbol{Y}}{\\boldsymbol{X}}^T ({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} \\] \\[ \\hat{{\\boldsymbol{Y}}} = \\hat{{\\boldsymbol{B}}} {\\boldsymbol{X}}= {\\boldsymbol{Y}}{\\boldsymbol{X}}^T ({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} {\\boldsymbol{X}}\\] so here the projection matrix is \\({\\boldsymbol{P}}= {\\boldsymbol{X}}^T ({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} {\\boldsymbol{X}}\\) and acts from the RHS, \\(\\hat{{\\boldsymbol{Y}}} = {\\boldsymbol{Y}}{\\boldsymbol{P}}\\). We will see this week and next that \\(\\hat{{\\boldsymbol{B}}}^{\\text{OLS}}\\) has nontrivial drawbacks. Thefore, we will be exploring other ways of estimating \\({\\boldsymbol{B}}\\). We of course aren’t limited to OLS models. We could consider the many response GLM: \\[ g\\left({\\operatorname{E}}\\left[ \\left. {\\boldsymbol{Y}}_{m \\times n} \\right| {\\boldsymbol{X}}\\right]\\right) = {\\boldsymbol{B}}_{m \\times d} {\\boldsymbol{X}}_{d \\times n} \\] and we could even replace \\({\\boldsymbol{B}}_{m \\times d} {\\boldsymbol{X}}_{d \\times n}\\) with \\(d\\) smoothers for each of the \\(m\\) response variable. 70.5 HD SNP Data &gt; knitr::include_graphics(&quot;./images/snp_dna.png&quot;) It is possible to measure single nucleotide polymorphisms at millions of locations across the genome. The base (A, C, G, or T) is measured from one of the strands. For example, on the figure to the left, the individual is heterozygous CT at this SNP location. \\[ \\begin{array}{rc} &amp; {\\boldsymbol{X}}_{m \\times n} \\\\ &amp; \\text{individuals} \\\\ \\text{SNPs} &amp; \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1n} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2n} \\\\ &amp; &amp; &amp; \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; \\\\ x_{m1} &amp; x_{m2} &amp; \\cdots &amp; x_{mn} \\end{bmatrix} \\\\ &amp; \\\\ &amp; {\\boldsymbol{y}}_{1 \\times n} \\ \\text{trait} \\\\ &amp; \\begin{bmatrix} y_{11} &amp; y_{12} &amp; \\cdots &amp; y_{1n} \\end{bmatrix} \\end{array} \\] The \\({\\boldsymbol{X}}\\) matrix contains SNP genotypes for \\(m\\) SNPs (rows) by \\(n\\) individuals (columns). The values \\(x_{ij} \\in \\{0, 1, 2\\}\\) are conversions of genotypes (e.g., CC, CT, TT) to counts of one of the alleles. The \\({\\boldsymbol{y}}\\) vector contains the trait values of the \\(n\\) individuals. Note that \\(m \\gg n\\). 70.6 Many Regressors Model The SNP-trait model is an example of what I call the many regressors model. A single model is fit of a response variable on many regressors (i.e., explanatory variables) simultaneously. This involves simultaneously inferring \\(m\\) paramaters \\({\\boldsymbol{\\beta}}= (\\beta_1, \\beta_2, \\ldots, \\beta_m)\\) in models such as: \\[ {\\boldsymbol{Y}}= \\alpha \\boldsymbol{1} + {\\boldsymbol{\\beta}}{\\boldsymbol{X}}+ {\\boldsymbol{E}}\\] which is an \\(n\\)-vector with component \\(j\\) being: \\[ Y_j = \\alpha + \\sum_{i=1}^m \\beta_i X_{ij} + E_{j} \\] As with the many responses model, we do not need to limit the model to the OLS type where the response variable is approximately Normal distributed. Instead we can consider more general models such as \\[ g\\left({\\operatorname{E}}\\left[ \\left. {\\boldsymbol{Y}}\\right| {\\boldsymbol{X}}\\right]\\right) = \\alpha \\boldsymbol{1} + {\\boldsymbol{\\beta}}{\\boldsymbol{X}}\\] for some link function \\(g(\\cdot)\\). 70.7 Goals In both types of models we are interested in: Forming point estimates Testing statistical hypothesis Calculating posterior distributions Leveraging the HD data to increase our power and accuracy Sometimes we are also interested in confidence intervals in high dimensions, but this is less common. 70.8 Challenges Here are several of the new challenges we face when analyzing high-dimensional data: Standard estimation methods may be suboptimal in high dimensions New measures of significance are needed There may be dependence and latent variables among the high-dimensional variables The fact that \\(m \\gg n\\) poses challenges, especially in the many regressors model HD data provide new challenges, but they also provide opportunities to model variation in the data in ways not possible for low-dimensional data. "],
["many-responses-model-1.html", "71 Many Responses Model", " 71 Many Responses Model "],
["shrinkage-and-empirical-bayes.html", "72 Shrinkage and Empirical Bayes 72.1 Estimating Several Means 72.2 Usual MLE 72.3 Loss Function 72.4 Stein’s Paradox 72.5 Inverse Regression Approach 72.6 Empirical Bayes Estimate 72.7 EB for a Many Responses Model", " 72 Shrinkage and Empirical Bayes 72.1 Estimating Several Means Let’s start with the simplest many responses model where there is only an interncept and only one observation per variable. This means that \\(n=1\\) and \\(d=1\\) where \\({\\boldsymbol{X}}= 1\\). This model can be written as \\(Y_i \\sim \\mbox{Normal}(\\beta_i, 1)\\) for the \\(i=1, 2, \\ldots, m\\) response variables. Suppose also that \\(Y_1, Y_2, \\ldots, Y_m\\) are jointly independent. Let’s assume that \\(\\beta_1, \\beta_2, \\ldots, \\beta_m\\) are fixed, nonrandom parameters. 72.2 Usual MLE The usual estimates of \\(\\beta_i\\) are to set \\[ \\hat{\\beta}_i^{\\text{MLE}} = {\\boldsymbol{Y}}_i. \\] This is also the OLS solution. 72.3 Loss Function Suppose we are interested in the simultaneous loss function \\[ L({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}) = \\sum_{i=1} (\\beta_i - \\hat{\\beta}_i)^2 \\] with risk \\(R({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}) = {\\operatorname{E}}[L({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}})]\\). 72.4 Stein’s Paradox Consider the following James-Stein estimator: \\[ \\hat{\\beta}_i^{\\text{JS}} = \\left(1 - \\frac{m-2}{\\sum_{k=1}^m Y_k^2} \\right) Y_i. \\] In a shocking result called Stein’s paradox, it was shown that when \\(m \\geq 3\\) then \\[ R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{JS}}\\right) &lt; R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{MLE}}\\right). \\] This means that the usual MLE is dominated by this JS estimator for any, even nonrandom, configuration of \\(\\beta_1, \\beta_2, \\ldots, \\beta_m\\)! What is going on? Let’s first take a linear regression point of view to better understand this paradox. Then we will return to the empirical Bayes example from earlier. &gt; beta &lt;- seq(-1, 1, length.out=50) &gt; y &lt;- beta + rnorm(length(beta)) The blue line is the least squares regression line. &gt; beta &lt;- seq(-10, 10, length.out=50) &gt; y &lt;- beta + rnorm(length(beta)) The blue line is the least squares regression line. 72.5 Inverse Regression Approach While \\(Y_i = \\beta_i + E_i\\) where \\(E_i \\sim \\mbox{Normal}(0,1)\\), it is also the case that \\(\\beta_i = Y_i - E_i\\) where \\(-E_i \\sim \\mbox{Normal}(0,1)\\). Even though we’re assuming the \\(\\beta_i\\) are fixed, suppose we imagine for the moment that the \\(\\beta_i\\) are random and take a least squares appraoch. We will try to estimate the linear model \\[ {\\operatorname{E}}[\\beta_i | Y_i] = a + b Y_i. \\] Why would we do this? The loss function is \\[ \\sum_{i=1}^m (\\beta_i - \\hat{\\beta}_i)^2 \\] so it makes sense to estimate \\(\\beta_i\\) by setting \\(\\hat{\\beta}_i\\) to a regression line. The least squares solution tells us to set \\[ \\begin{aligned} \\hat{\\beta}_i &amp; = \\hat{a} + \\hat{b} Y_i \\\\ &amp; = (\\bar{\\beta} - \\hat{b} \\bar{Y}) + \\hat{b} Y_i \\\\ &amp; = \\bar{\\beta} + \\hat{b} (Y_i - \\bar{Y}) \\end{aligned} \\] where \\[ \\hat{b} = \\frac{\\sum_{i=1}^m (Y_i - \\bar{Y})(\\beta_i - \\bar{\\beta})}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2}. \\] We can estimate \\(\\bar{\\beta}\\) with \\(\\bar{Y}\\) since \\({\\operatorname{E}}[\\bar{\\beta}] = {\\operatorname{E}}[\\bar{Y}]\\). We also need to find an estimate of \\(\\sum_{i=1}^m (Y_i - \\bar{Y})(\\beta_i - \\bar{\\beta})\\). Note that \\[ \\beta_i - \\bar{\\beta} = Y_i - \\bar{Y} - (E_i + \\bar{E}) \\] so that \\[ \\begin{aligned} \\sum_{i=1}^m (Y_i - \\bar{Y})(\\beta_i - \\bar{\\beta}) = &amp; \\sum_{i=1}^m (Y_i - \\bar{Y})(Y_i - \\bar{Y}) \\\\ &amp; + \\sum_{i=1}^m (Y_i - \\bar{Y})(E_i - \\bar{E}) \\end{aligned} \\] Since \\(Y_i = \\beta_i + E_i\\) it follows that \\[ {\\operatorname{E}}\\left[\\sum_{i=1}^m (Y_i - \\bar{Y})(E_i - \\bar{E})\\right] = {\\operatorname{E}}\\left[\\sum_{i=1}^m (E_i - \\bar{E})(E_i - \\bar{E})\\right] = m-1. \\] Therefore, \\[ {\\operatorname{E}}\\left[\\sum_{i=1}^m (Y_i - \\bar{Y})(\\beta_i - \\bar{\\beta})\\right] = {\\operatorname{E}}\\left[\\sum_{i=1}^m (Y_i - \\bar{Y})^2 - (m-1)\\right]. \\] This yields \\[ \\hat{b} = \\frac{\\sum_{i=1}^m (Y_i - \\bar{Y})^2 - (m-1)}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2} = 1 - \\frac{m-1}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2} \\] and \\[ \\hat{\\beta}_i^{\\text{IR}} = \\bar{Y} + \\left(1 - \\frac{m-1}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2}\\right) (Y_i - \\bar{Y}) \\] If instead we had started with the no intercept model \\[ {\\operatorname{E}}[\\beta_i | Y_i] = b Y_i. \\] we would have ended up with \\[ \\hat{\\beta}_i^{\\text{IR}} = \\left(1 - \\frac{m-1}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2}\\right) Y_i \\] In either case, it can be shown that \\[ R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{IR}}\\right) &lt; R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{MLE}}\\right). \\] The blue line is the least squares regression line of \\(\\beta_i\\) on \\(Y_i\\), and the red line is \\(\\hat{\\beta}_i^{\\text{IR}}\\). 72.6 Empirical Bayes Estimate Suppose that \\(Y_i | \\beta_i \\sim \\mbox{Normal}(\\beta_i, 1)\\) where these rv’s are jointly independent. Also suppose that \\(\\beta_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(a, b^2)\\). Taking the empirical Bayes approach, we get: \\[ f(y_i ; a, b) = \\int f(y_i | \\beta_i) f(\\beta_i; a, b) d\\beta_i \\sim \\mbox{Normal}(a, 1+b^2). \\] \\[ \\implies \\hat{a} = \\overline{Y}, \\ 1+\\hat{b}^2 = \\frac{\\sum_{k=1}^m (Y_k - \\overline{Y})^2}{n} \\] \\[ \\begin{aligned} {\\operatorname{E}}[\\beta_i | Y_i] &amp; = \\frac{1}{1+b^2}a + \\frac{b^2}{1+b^2}Y_i \\implies \\\\ &amp; \\\\ \\hat{\\beta}_i^{\\text{EB}} &amp; = \\hat{{\\operatorname{E}}}[\\beta_i | Y_i] = \\frac{1}{1+\\hat{b}^2}\\hat{a} + \\frac{\\hat{b}^2}{1+\\hat{b}^2}Y_i \\\\ &amp; = \\frac{m}{\\sum_{k=1}^m (Y_k - \\overline{Y})^2} \\overline{Y} + \\left(1-\\frac{m}{\\sum_{k=1}^m (Y_k - \\overline{Y})^2}\\right) Y_i \\end{aligned} \\] As with \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{JS}}\\) and \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{IR}}\\), we have \\[ R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{EB}}\\right) &lt; R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{MLE}}\\right). \\] 72.7 EB for a Many Responses Model Consider the many responses model where \\({\\boldsymbol{Y}}_{i} | {\\boldsymbol{X}}\\sim \\mbox{MVN}_n({\\boldsymbol{\\beta}}_i {\\boldsymbol{X}}, \\sigma^2 {\\boldsymbol{I}})\\) where the vectors \\({\\boldsymbol{Y}}_{i} | {\\boldsymbol{X}}\\) are jointly independent (\\(i=1, 2, \\ldots, m\\)). Here we’ve made the simplifying assumption that the variance \\(\\sigma^2\\) is equal across all responses, but this would not be generally true. The OLS (and MLE) solution is \\[ \\hat{{\\boldsymbol{B}}}= {\\boldsymbol{Y}}{\\boldsymbol{X}}^T ({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1}. \\] Suppose we extend this so that \\({\\boldsymbol{Y}}_{i} | {\\boldsymbol{X}}, {\\boldsymbol{\\beta}}_i \\sim \\mbox{MVN}_n({\\boldsymbol{\\beta}}_i {\\boldsymbol{X}}, \\sigma^2 {\\boldsymbol{I}})\\) and \\({\\boldsymbol{\\beta}}_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{MVN}_d({\\boldsymbol{u}}, {\\boldsymbol{V}})\\). Since \\(\\hat{{\\boldsymbol{\\beta}}}_i | {\\boldsymbol{\\beta}}_i \\sim \\mbox{MVN}_d({\\boldsymbol{\\beta}}_i, \\sigma^2({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1})\\), it follows that marginally \\[ \\hat{{\\boldsymbol{\\beta}}}_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{MVN}_d({\\boldsymbol{u}}, \\sigma^2({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} + {\\boldsymbol{V}}). \\] Therefore, \\[ \\hat{{\\boldsymbol{u}}} = \\frac{\\sum_{i=1}^m \\hat{{\\boldsymbol{\\beta}}}_i}{m} \\] \\[ \\hat{{\\boldsymbol{V}}} = \\hat{{\\operatorname{Cov}}}\\left(\\hat{{\\boldsymbol{\\beta}}}\\right) - \\hat{\\sigma}^2({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} \\] where \\(\\hat{{\\operatorname{Cov}}}\\left(\\hat{{\\boldsymbol{\\beta}}}\\right)\\) is the \\(d \\times d\\) sample covariance (or MLE covariance) of the \\(\\hat{{\\boldsymbol{\\beta}}}_i\\) estimates. Also, \\(\\hat{\\sigma}^2\\) is obtained by averaging the estimate over all \\(m\\) regressions. We then do inference based on the prior distribution \\({\\boldsymbol{\\beta}}_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{MVN}_d(\\hat{{\\boldsymbol{u}}}, \\hat{{\\boldsymbol{V}}})\\). The posterior distribution of \\({\\boldsymbol{\\beta}}_i | {\\boldsymbol{Y}}, {\\boldsymbol{X}}\\) is MVN with mean \\[ \\left(\\frac{1}{\\hat{\\sigma}^2}({\\boldsymbol{X}}{\\boldsymbol{X}}^T) + \\hat{{\\boldsymbol{V}}}^{-1}\\right)^{-1} \\left(\\frac{1}{\\hat{\\sigma}^2}({\\boldsymbol{X}}{\\boldsymbol{X}}^T) \\hat{{\\boldsymbol{\\beta}}}_i + \\hat{{\\boldsymbol{V}}}^{-1} \\hat{{\\boldsymbol{u}}} \\right) \\] and covariance \\[ \\left(\\frac{1}{\\hat{\\sigma}^2}({\\boldsymbol{X}}{\\boldsymbol{X}}^T) + \\hat{{\\boldsymbol{V}}}^{-1}\\right)^{-1}. \\] "],
["multiple-testing.html", "73 Multiple Testing 73.1 Motivating Example 73.2 Challenges 73.3 Outcomes 73.4 Error Rates 73.5 Bonferroni Correction 73.6 False Discovery Rate 73.7 Point Estimate 73.8 Adaptive Threshold 73.9 Conservative Properties 73.10 Q-Values 73.11 Bayesian Mixture Model 73.12 Bayesian-Frequentist Connection 73.13 Local FDR", " 73 Multiple Testing 73.1 Motivating Example Hedenfalk et al. (2001) NEJM measured gene expression in three different breast cancer tumor types. In your homework, you have analyzed these data and have specifically compared BRCA1 mutation positive tumors to BRCA2 mutation positive tumors. The qvalue package has the p-values when testing for a difference in population means between these two groups (called “differential expression”). There are 3170 genes tested, resulting in 3170 p-values. Note that this analysis is a version of the many responses model. &gt; library(qvalue) &gt; data(hedenfalk); df &lt;- data.frame(p=hedenfalk$p) &gt; ggplot(df, aes(x = p)) + + ggtitle(&quot;p-value density histogram&quot;) + + geom_histogram(aes_string(y = &#39;..density..&#39;), colour = &quot;black&quot;, + fill = &quot;white&quot;, binwidth = 0.04, center=0.02) 73.2 Challenges Traditional p-value thresholds such as 0.01 or 0.05 may result in too many false positives. For example, in the above example, a 0.05 threshold could result in 158 false positives. A careful balance of true positives and false positives must be achieved in a manner that is scientifically interpretable. There is information in the joint distribution of the p-values that can be leveraged. Dependent p-values may make this type of analysis especially difficult (next week’s topic). &gt; qobj &lt;- qvalue(hedenfalk$p) &gt; hist(qobj) 73.3 Outcomes Possible outcomes from \\(m\\) hypothesis tests based on applying a significance threshold \\(0 &lt; t \\leq 1\\) to their corresponding p-values. Not Significant Significant Total Null True \\(U\\) \\(V\\) \\(m_0\\) Alternative True \\(T\\) \\(S\\) \\(m_1\\) Total \\(W\\) \\(R\\) \\(m\\) 73.4 Error Rates Suppose we are testing \\(m\\) hypotheses based on p-values \\(p_1, p_2, \\ldots, p_m\\). Multiple hypothesis testing is the process of deciding which of these p-values should be called statistically significant. This requires formulating and estimating a compound error rate that quantifies the quality of the decision. 73.5 Bonferroni Correction The family-wise error rate is the probability of any false positive occurring among all tests called significant. The Bonferroni correction is a result that shows that utilizing a p-value threshold of \\(\\alpha/m\\) results in FWER \\(\\leq \\alpha\\). Specifically, \\[ \\begin{aligned} \\text{FWER} &amp; \\leq \\Pr(\\cup \\{P_i \\leq \\alpha/m\\}) \\\\ &amp; \\leq \\sum_{i=1}^m \\Pr(P_i \\leq \\alpha/m) = \\sum_{i=1}^m \\alpha/m = \\alpha \\end{aligned} \\] where the above probability calculations are done under the assumption that all \\(H_0\\) are true. 73.6 False Discovery Rate The false discovery rate (FDR) measures the proportion of Type I errors — or “false discoveries” — among all hypothesis tests called statistically significant. It is defined as \\[ {{\\rm FDR}}= {\\operatorname{E}}\\left[ \\frac{V}{R \\vee 1} \\right] = {\\operatorname{E}}\\left[ \\left. \\frac{V}{R} \\right| R&gt;0 \\right] \\Pr(R&gt;0). \\] This is less conservative than the FWER and it offers a clearer balance between true positives and false positives. There are two other false discovery rate definitions, where the main difference is in how the \\(R=0\\) event is handled. These quantities are called the positive false discovery rate (pFDR) and the marginal false discovery rate (mFDR), defined as follows: \\[ {{\\rm pFDR}}= {\\operatorname{E}}\\left[ \\left. \\frac{V}{R} \\right| R&gt;0 \\right], \\] \\[ {{\\rm mFDR}}= \\frac{{\\operatorname{E}}\\left[ V \\right]}{{\\operatorname{E}}\\left[ R \\right]}. \\] Note that \\({{\\rm pFDR}}= {{\\rm mFDR}}= 1\\) whenever all null hypotheses are true, whereas FDR can always be made arbitrarily small because of the extra term \\(\\Pr(R &gt; 0)\\). 73.7 Point Estimate Let \\({{\\rm FDR}}(t)\\) denote the FDR when calling null hypotheses significant whenever \\(p_i \\leq t\\), for \\(i = 1, 2, \\ldots, m\\). For \\(0 &lt; t \\leq 1\\), we define the following random variables: \\[ \\begin{aligned} V(t) &amp; = \\#\\{\\mbox{true null } p_i: p_i \\leq t \\} \\\\ R(t) &amp; = \\#\\{p_i: p_i \\leq t\\} \\end{aligned} \\] In terms of these, we have \\[ {{\\rm FDR}}(t) = {\\operatorname{E}}\\left[ \\frac{V(t)}{R(t) \\vee 1} \\right]. \\] For fixed \\(t\\), the following defines a family of conservatively biased point estimates of \\({{\\rm FDR}}(t)\\): \\[ \\hat{{{\\rm FDR}}}(t) = \\frac{\\hat{m}_0(\\lambda) \\cdot t}{[R(t) \\vee 1]}. \\] The term \\(\\hat{m}_0({\\lambda})\\) is an estimate of \\(m_0\\), the number of true null hypotheses. This estimate depends on the tuning parameter \\({\\lambda}\\), and it is defined as \\[ \\hat{m}_0({\\lambda}) = \\frac{m - R({\\lambda})}{(1-{\\lambda})}. \\] Sometimes instead of \\(m_0\\), the quantity \\[ \\pi_0 = \\frac{m_0}{m} \\] is estimated, where simply \\[ \\hat{\\pi}_0({\\lambda}) = \\frac{\\hat{m}_0({\\lambda})}{m} = \\frac{m - R({\\lambda})}{m(1-{\\lambda})}. \\] It can be shown that \\({\\operatorname{E}}[\\hat{m}_0({\\lambda})] \\geq m_0\\) when the p-values corresponding to the true null hypotheses are Uniform(0,1) distributed (or stochastically greater). There is an inherent bias/variance trade-off in the choice of \\({\\lambda}\\). In most cases, when \\({\\lambda}\\) gets smaller, the bias of \\(\\hat{m}_0({\\lambda})\\) gets larger, but the variance gets smaller. Therefore, \\({\\lambda}\\) can be chosen to try to balance this trade-off. 73.8 Adaptive Threshold If we desire a FDR level of \\(\\alpha\\), it is tempting to use the p-value threshold \\[ t^*_\\alpha = \\max \\left\\{ t: \\hat{{{\\rm FDR}}}(t) \\leq \\alpha \\right\\} \\] which identifies the largest estimated FDR less than or equal to \\(\\alpha\\). 73.9 Conservative Properties When the p-value corresponding to true null hypothesis are distributed iid Uniform(0,1), then we have the following two conservative properties. \\[ \\begin{aligned} &amp; {\\operatorname{E}}\\left[ \\hat{{{\\rm FDR}}}(t) \\right] \\geq {{\\rm FDR}}(t) \\\\ &amp; {\\operatorname{E}}\\left[ \\hat{{{\\rm FDR}}}(t^*_\\alpha) \\right] \\leq \\alpha \\end{aligned} \\] 73.10 Q-Values In single hypothesis testing, it is common to report the p-value as a measure of significance. The q-value is the FDR based measure of significance that can be calculated simultaneously for multiple hypothesis tests. The p-value is constructed so that a threshold of \\(\\alpha\\) results in a Type I error rate \\(\\leq \\alpha\\). Likewise, the q-value is constructed so that a threshold of \\(\\alpha\\) results in a FDR \\(\\leq \\alpha\\). Initially it seems that the q-value should capture the FDR incurred when the significance threshold is set at the p-value itself, \\({{\\rm FDR}}(p_i)\\). However, unlike Type I error rates, the FDR is not necessarily strictly increasing with an increasing significance threshold. To accommodate this property, the q-value is defined to be the minimum FDR (or pFDR) at which the test is called significant: \\[ {\\operatorname{q}}{\\rm\\mbox{-}value}(p_i) = \\min_{t \\geq p_i} {{\\rm FDR}}(t) \\] or \\[ {\\operatorname{q}}{\\rm\\mbox{-}value}(p_i) = \\min_{t \\geq p_i} {{\\rm pFDR}}(t). \\] To estimate this in practice, a simple plug-in estimate is formed, for example: \\[ \\hat{{\\operatorname{q}}}{\\rm\\mbox{-}value}(p_i) = \\min_{t \\geq p_i} \\hat{{{\\rm FDR}}}(t). \\] Various theoretical properties have been shown for these estimates under certain conditions, notably that the estimated q-values of the entire set of tests are simultaneously conservative as the number of hypothesis tests grows large. &gt; plot(qobj) 73.11 Bayesian Mixture Model Let’s return to the Bayesian classification set up from earlier. Suppose that \\(H_i =\\) 0 or 1 according to whether the \\(i\\)th null hypothesis is true or not \\(H_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Bernoulli}(1-\\pi_0)\\) so that \\(\\Pr(H_i=0)=\\pi_0\\) and \\(\\Pr(H_i=1)=1-\\pi_0\\) \\(P_i | H_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}(1-H_i) \\cdot F_0 + H_i \\cdot F_1\\), where \\(F_0\\) is the null distribution and \\(F_1\\) is the alternative distribution 73.12 Bayesian-Frequentist Connection Under these assumptions, it has been shown that \\[ \\begin{aligned} {{\\rm pFDR}}(t) &amp; = {\\operatorname{E}}\\left[ \\left. \\frac{V(t)}{R(t)} \\right| R(t) &gt; 0 \\right] \\\\ \\ &amp; = \\Pr(H_i = 0 | P_i \\leq t) \\end{aligned} \\] where \\(\\Pr(H_i = 0 | P_i \\leq t)\\) is the same for each \\(i\\) because of the iid assumptions. Under these modeling assumptions, it follows that \\[ \\mbox{q-value}(p_i) = \\min_{t \\geq p_i} \\Pr(H_i = 0 | P_i \\leq t) \\] which is a Bayesian analogue of the p-value — or rather a “Bayesian posterior Type I error rate”. 73.13 Local FDR In this scenario, it also follows that \\[ {{\\rm pFDR}}(t) = \\int \\Pr(H_i = 0 | P_i = p_i) dF(p_i | p_i \\leq t) \\] where \\(F = \\pi_0 F_0 + (1-\\pi_0) F_1\\). This connects the pFDR to the posterior error probability \\[\\Pr(H_i = 0 | P_i = p_i)\\] making this latter quantity sometimes interpreted as a local false discovery rate. &gt; hist(qobj) "],
["many-regressors-model-1.html", "74 Many Regressors Model", " 74 Many Regressors Model "],
["ridge-regression.html", "75 Ridge Regression 75.1 Motivation 75.2 Optimization Goal 75.3 Solution 75.4 Preprocessing 75.5 Shrinkage 75.6 Example 75.7 Existence of Solution 75.8 Effective Degrees of Freedom 75.9 Bias and Covariance 75.10 Ridge vs OLS 75.11 Bayesian Interpretation 75.12 Example: Diabetes Data 75.13 GLMs", " 75 Ridge Regression 75.1 Motivation Ridge regression is a technique for shrinking the coefficients towards zero in linear models. It also deals with collinearity among explanatory variables. Collinearity is the presence of strong correlation among two or more explanatory variables. 75.2 Optimization Goal Under the OLS model assumptions, ridge regression fits model by minimizing the following: \\[ \\sum_{j=1}^n \\left(y_j - \\sum_{i=1}^m \\beta_{i} x_{ij}\\right)^2 + \\lambda \\sum_{k=1}^m \\beta_k^2. \\] Recall the \\(\\ell_2\\) norm: \\(\\sum_{k=1}^m \\beta_k^2 = \\| {\\boldsymbol{\\beta}}\\|^2_2\\). Sometimes ridge regression is called \\(\\ell_2\\) penalized regression. As with natural cubic splines, the paramater \\(\\lambda\\) is a tuning paramater that controls how much shrinkage occurs. 75.3 Solution The ridge regression solution is \\[ \\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}} = {\\boldsymbol{y}}{\\boldsymbol{X}}^T \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1}. \\] As \\(\\lambda \\rightarrow 0\\), the \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}} \\rightarrow \\hat{{\\boldsymbol{\\beta}}}^{\\text{OLS}}\\). As \\(\\lambda \\rightarrow \\infty\\), the \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}} \\rightarrow {\\boldsymbol{0}}\\). 75.4 Preprocessing Implicitly… We mean center \\({\\boldsymbol{y}}\\). We also mean center and standard deviation scale each explanatory variable. Why? 75.5 Shrinkage When \\({\\boldsymbol{X}}{\\boldsymbol{X}}^T = {\\boldsymbol{I}}\\), then \\[ \\hat{\\beta}^{\\text{Ridge}}_{j} = \\frac{\\hat{\\beta}^{\\text{OLS}}_{j}}{1+\\lambda}. \\] This shows how ridge regression acts as a technique for shrinking regression coefficients towards zero. It also shows that when \\(\\hat{\\beta}^{\\text{OLS}}_{j} \\not= 0\\), then for all finite \\(\\lambda\\), \\(\\hat{\\beta}^{\\text{Ridge}}_{j} \\not= 0\\). 75.6 Example &gt; set.seed(508) &gt; x1 &lt;- rnorm(20) &gt; x2 &lt;- x1 + rnorm(20, sd=0.1) &gt; y &lt;- 1 + x1 + x2 + rnorm(20) &gt; tidy(lm(y~x1+x2)) # A tibble: 3 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 0.965 0.204 4.74 0.000191 2 x1 0.493 2.81 0.175 0.863 3 x2 1.26 2.89 0.436 0.668 &gt; lm.ridge(y~x1+x2, lambda=1) # from MASS package x1 x2 0.9486116 0.8252948 0.8751979 75.7 Existence of Solution When \\(m &gt; n\\) or when there is high collinearity, then \\(({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1}\\) will not exist. However, for \\(\\lambda &gt; 0\\), it is always the case that \\(\\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1}\\) exists. Therefore, one can always compute a unique \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}}\\) for each \\(\\lambda &gt; 0\\). 75.8 Effective Degrees of Freedom Similarly to natural cubic splines, we can calculate an effective degrees of freedom by noting that: \\[ \\hat{{\\boldsymbol{y}}} = {\\boldsymbol{y}}{\\boldsymbol{X}}^T \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1} {\\boldsymbol{X}}\\] The effective degrees of freedom is then the trace of the linear operator: \\[ \\operatorname{tr} \\left({\\boldsymbol{X}}^T \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1} {\\boldsymbol{X}}\\right) \\] 75.9 Bias and Covariance Under the OLS model assumptions, \\[ {\\operatorname{Cov}}\\left(\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}}\\right) = \\sigma^2 \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1} {\\boldsymbol{X}}{\\boldsymbol{X}}^T \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1} \\] and \\[ \\text{bias} = {\\operatorname{E}}\\left[\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}}\\right] - {\\boldsymbol{\\beta}}= - \\lambda {\\boldsymbol{\\beta}}\\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1}. \\] 75.10 Ridge vs OLS When the OLS model is true, there exists a \\(\\lambda &gt; 0\\) such that the MSE of the ridge estimate is lower than than of the OLS estimate: \\[ {\\operatorname{E}}\\left[ \\| {\\boldsymbol{\\beta}}- \\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}} \\|^2_2 \\right] &lt; {\\operatorname{E}}\\left[ \\| {\\boldsymbol{\\beta}}- \\hat{{\\boldsymbol{\\beta}}}^{\\text{OLS}} \\|^2_2 \\right]. \\] This says that by sacrificing some bias in the ridge estimator, we can obtain a smaller overall MSE, which is bias\\(^2\\) + variance. 75.11 Bayesian Interpretation The ridge regression solution is equivalent to maximizing \\[ -\\frac{1}{2\\sigma^2}\\sum_{j=1}^n \\left(y_j - \\sum_{i=1}^m \\beta_{i} x_{ij}\\right)^2 - \\frac{\\lambda}{2\\sigma^2} \\sum_{k=1}^m \\beta_k^2 \\] which means it can be interpreted as the MAP solution with a Normal prior on the \\(\\beta_i\\) values. 75.12 Example: Diabetes Data &gt; library(lars) &gt; data(diabetes) &gt; x &lt;- diabetes$x2 %&gt;% unclass() %&gt;% as.data.frame() &gt; y &lt;- diabetes$y &gt; dim(x) [1] 442 64 &gt; length(y) [1] 442 &gt; df &lt;- cbind(x,y) &gt; names(df) [1] &quot;age&quot; &quot;sex&quot; &quot;bmi&quot; &quot;map&quot; &quot;tc&quot; &quot;ldl&quot; &quot;hdl&quot; [8] &quot;tch&quot; &quot;ltg&quot; &quot;glu&quot; &quot;age^2&quot; &quot;bmi^2&quot; &quot;map^2&quot; &quot;tc^2&quot; [15] &quot;ldl^2&quot; &quot;hdl^2&quot; &quot;tch^2&quot; &quot;ltg^2&quot; &quot;glu^2&quot; &quot;age:sex&quot; &quot;age:bmi&quot; [22] &quot;age:map&quot; &quot;age:tc&quot; &quot;age:ldl&quot; &quot;age:hdl&quot; &quot;age:tch&quot; &quot;age:ltg&quot; &quot;age:glu&quot; [29] &quot;sex:bmi&quot; &quot;sex:map&quot; &quot;sex:tc&quot; &quot;sex:ldl&quot; &quot;sex:hdl&quot; &quot;sex:tch&quot; &quot;sex:ltg&quot; [36] &quot;sex:glu&quot; &quot;bmi:map&quot; &quot;bmi:tc&quot; &quot;bmi:ldl&quot; &quot;bmi:hdl&quot; &quot;bmi:tch&quot; &quot;bmi:ltg&quot; [43] &quot;bmi:glu&quot; &quot;map:tc&quot; &quot;map:ldl&quot; &quot;map:hdl&quot; &quot;map:tch&quot; &quot;map:ltg&quot; &quot;map:glu&quot; [50] &quot;tc:ldl&quot; &quot;tc:hdl&quot; &quot;tc:tch&quot; &quot;tc:ltg&quot; &quot;tc:glu&quot; &quot;ldl:hdl&quot; &quot;ldl:tch&quot; [57] &quot;ldl:ltg&quot; &quot;ldl:glu&quot; &quot;hdl:tch&quot; &quot;hdl:ltg&quot; &quot;hdl:glu&quot; &quot;tch:ltg&quot; &quot;tch:glu&quot; [64] &quot;ltg:glu&quot; &quot;y&quot; The glmnet() function will perform ridge regression when we set alpha=0. &gt; library(glmnetUtils) &gt; ridgefit &lt;- glmnetUtils::glmnet(y ~ ., data=df, alpha=0) &gt; plot(ridgefit) Cross-validation to tune the shrinkage parameter. &gt; cvridgefit &lt;- glmnetUtils::cv.glmnet(y ~ ., data=df, alpha=0) &gt; plot(cvridgefit) 75.13 GLMs The glmnet library (and the glmnetUtils wrapper library) allow one to perform ridge regression on generalized linear models. A penalized maximum likelihood estimate is calculated based on \\[ -\\lambda \\sum_{i=1}^m \\beta_i^2 \\] added to the log-likelihood. "],
["lasso-regression.html", "76 Lasso Regression 76.1 Motivation 76.2 Optimization Goal 76.3 Solution 76.4 Preprocessing 76.5 Bayesian Interpretation 76.6 Inference 76.7 GLMs", " 76 Lasso Regression 76.1 Motivation One drawback of the ridge regression approach is that coefficients will be small, but they will be nonzero. An alternative appraoch is the lasso, which stands for “Least Absolute Shrinkage and Selection Operator”. This performs a similar optimization as ridge, but with an \\(\\ell_1\\) penalty instead. This changes the geometry of the problem so that coefficients may be zero. 76.2 Optimization Goal Starting with the OLS model assumptions again, we wish to find \\({\\boldsymbol{\\beta}}\\) that minimizes \\[ \\sum_{j=1}^n \\left(y_j - \\sum_{i=1}^m \\beta_{i} x_{ij}\\right)^2 + \\lambda \\sum_{k=1}^m |\\beta_k|. \\] Note that \\(\\sum_{k=1}^m |\\beta_k| = \\| {\\boldsymbol{\\beta}}\\|_1\\), which is the \\(\\ell_1\\) vector norm. As before, the paramater \\(\\lambda\\) is a tuning paramater that controls how much shrinkage and selection occurs. 76.3 Solution There is no closed form solution to this optimization problem, so it must be solved numerically. Originally, a quadratic programming solution was proposed with has \\(O(n 2^m)\\) operations. Then a least angle regression solution reduced the solution to \\(O(nm^2)\\) operations. Modern coordinate descent methods have further reduced this to \\(O(nm)\\) operations. 76.4 Preprocessing Implicitly… We mean center \\({\\boldsymbol{y}}\\). We also mean center and standard deviation scale each explanatory variable. Why? Let’s return to the diabetes data set. To do lasso regression, we set alpha=1. &gt; lassofit &lt;- glmnetUtils::glmnet(y ~ ., data=df, alpha=1) &gt; plot(lassofit) Cross-validation to tune the shrinkage parameter. &gt; cvlassofit &lt;- glmnetUtils::cv.glmnet(y ~ ., data=df, alpha=1) &gt; plot(cvlassofit) 76.5 Bayesian Interpretation The ridge regression solution is equivalent to maximizing \\[ -\\frac{1}{2\\sigma^2}\\sum_{j=1}^n \\left(y_j - \\sum_{i=1}^m \\beta_{i} x_{ij}\\right)^2 - \\frac{\\lambda}{2\\sigma^2} \\sum_{k=1}^m |\\beta_k| \\] which means it can be interpreted as the MAP solution with a Exponential prior on the \\(\\beta_i\\) values. 76.6 Inference Inference on the lasso model fit is difficult. However, there has been recent progress. One idea proposes a conditional covariance statistic, but this requires all explanatory variables to be uncorrelated. Another idea called the knockoff filter controls the false discovery rate and allows for correlation among explanatory variables. Both of these ideas have some restrictive assumptions and require the number of observations to exceed the number of explanatory variables, \\(n &gt; m\\). 76.7 GLMs The glmnet library (and the glmnetUtils wrapper library) allow one to perform lasso regression on generalized linear models. A penalized maximum likelihood estimate is calculated based on \\[ -\\lambda \\sum_{i=1}^m |\\beta_i| \\] added to the log-likelihood. "],
["hd-latent-variable-models.html", "77 HD Latent Variable Models 77.1 Definition 77.2 Model 77.3 Estimation", " 77 HD Latent Variable Models 77.1 Definition Latent variables (or hidden variables) are random variables that are present in the underlying probabilistic model of the data, but they are unobserved. In high-dimensional data, there may be latent variables present that affect many variables simultaneously. These are latent variables that induce systematic variation. A topic of much interest is how to estimate these and incorporate them into further HD inference procedures. 77.2 Model Suppose we have observed data \\({\\boldsymbol{Y}}_{m \\times n}\\) of \\(m\\) variables with \\(n\\) observations each. Suppose there are \\(r\\) latent variables contained in the \\(r\\) rows of \\({\\boldsymbol{Z}}_{r \\times n}\\) where \\[ {\\operatorname{E}}\\left[{\\boldsymbol{Y}}_{m \\times n} \\left. \\right| {\\boldsymbol{Z}}_{r \\times n} \\right] = {\\boldsymbol{\\Phi}}_{m \\times r} {\\boldsymbol{Z}}_{r \\times n}. \\] Let’s also assume that \\(m \\gg n &gt; r\\). The latent variables \\({\\boldsymbol{Z}}\\) induce systematic variation in variable \\({\\boldsymbol{y}}_i\\) parameterized by \\({\\boldsymbol{\\phi}}_i\\) for \\(i = 1, 2, \\ldots, m\\). 77.3 Estimation There exist methods for estimating the row space of \\({\\boldsymbol{Z}}\\) with probability 1 as \\(m \\rightarrow \\infty\\) for a fixed \\(n\\) in two scenarios. Leek (2011) shows how to do this when \\({\\boldsymbol{y}}_i | {\\boldsymbol{Z}}\\sim \\text{MVN}({\\boldsymbol{\\phi}}_i {\\boldsymbol{Z}}, \\sigma^2_i {\\boldsymbol{I}})\\), and the \\({\\boldsymbol{y}}_i | {\\boldsymbol{Z}}\\) are jointly independent. Chen and Storey (2015) show how to do this when the \\({\\boldsymbol{y}}_i | {\\boldsymbol{Z}}\\) are distributed according to a single parameter exponential family distribution with mean \\({\\boldsymbol{\\phi}}_i {\\boldsymbol{Z}}\\), and the \\({\\boldsymbol{y}}_i | {\\boldsymbol{Z}}\\) are jointly independent. "],
["jackstraw.html", "78 Jackstraw 78.1 Procedure 78.2 Example: Yeast Cell Cycle", " 78 Jackstraw Suppose we have a reasonable method for estimating \\({\\boldsymbol{Z}}\\) in the model \\[ {\\operatorname{E}}\\left[{\\boldsymbol{Y}}\\left. \\right| {\\boldsymbol{Z}}\\right] = {\\boldsymbol{\\Phi}}{\\boldsymbol{Z}}. \\] The jackstraw method allows us to perform hypothesis tests of the form \\[ H_0: {\\boldsymbol{\\phi}}_i = {\\boldsymbol{0}}\\mbox{ vs } H_1: {\\boldsymbol{\\phi}}_i \\not= {\\boldsymbol{0}}. \\] We can also perform this hypothesis test on any subset of the columns of \\({\\boldsymbol{\\Phi}}\\). This is a challening problem because we have to “double dip” in the data \\({\\boldsymbol{Y}}\\), first to estimate \\({\\boldsymbol{Z}}\\), and second to perform significance tests on \\({\\boldsymbol{\\Phi}}\\). 78.1 Procedure The first step is to form estimate \\(\\hat{{\\boldsymbol{Z}}}\\) and then test statistic \\(t_i\\) that performs the hypothesis test for each \\({\\boldsymbol{\\phi}}_i\\) from \\({\\boldsymbol{y}}_i\\) and \\(\\hat{{\\boldsymbol{Z}}}\\) (\\(i=1, \\ldots, m\\)). Assume that the larger \\(t_i\\) is, the more evidence there is against the null hypothesis in favor of the alternative. Next we randomly select \\(s\\) rows of \\({\\boldsymbol{Y}}\\) and permute them to create data set \\({\\boldsymbol{Y}}^{0}\\). Let this set of \\(s\\) variables be indexed by \\(\\mathcal{S}\\). This breaks the relationship between \\({\\boldsymbol{y}}_i\\) and \\({\\boldsymbol{Z}}\\), thereby inducing a true \\(H_0\\), for each \\(i \\in \\mathcal{S}\\). We estimate \\(\\hat{{\\boldsymbol{Z}}}^{0}\\) from \\({\\boldsymbol{Y}}^{0}\\) and again obtain test statistics \\(t_i^{0}\\). Specifically, the test statistics \\(t_i^{0}\\) for \\(i \\in \\mathcal{S}\\) are saved as draws from the null distribution. We repeat permutation procedure \\(B\\) times, and then utilize all saved \\(sB\\) permutation null statistics to calculate empirical p-values: \\[ p_i = \\frac{1}{sB} \\sum_{b=1}^B \\sum_{k \\in \\mathcal{S}_b} 1\\left(t_i \\geq t_k^{0b} \\right). \\] 78.2 Example: Yeast Cell Cycle Recall the yeast cell cycle data from earlier. We will test which genes have expression significantly associated with PC1 and PC2 since these both capture cell cycle regulation. &gt; load(&quot;./data/spellman.RData&quot;) &gt; time [1] 0 30 60 90 120 150 180 210 240 270 330 360 390 &gt; dim(gene_expression) [1] 5981 13 &gt; dat &lt;- t(scale(t(gene_expression), center=TRUE, scale=FALSE)) Test for associations between PC1 and each gene, conditioning on PC1 and PC2 being relevant sources of systematic variation. &gt; jsobj &lt;- jackstraw_pca(dat, r1=1, r=2, B=500, s=50, verbose=FALSE) &gt; jsobj$p.value %&gt;% qvalue() %&gt;% hist() This is the most significant gene plotted with PC1. Test for associations between PC2 and each gene, conditioning on PC1 and PC2 being relevant sources of systematic variation. &gt; jsobj &lt;- jackstraw_pca(dat, r1=2, r=2, B=500, s=50, verbose=FALSE) &gt; jsobj$p.value %&gt;% qvalue() %&gt;% hist() This is the most significant gene plotted with PC2. "],
["surrogate-variable-analysis.html", "79 Surrogate Variable Analysis 79.1 Procedure 79.2 Example: Kidney Expr by Age", " 79 Surrogate Variable Analysis The surrogate variable analysis (SVA) model combines the many responses model with the latent variable model introduced above: \\[ {\\boldsymbol{Y}}_{m \\times n} = {\\boldsymbol{B}}_{m \\times d} {\\boldsymbol{X}}_{d \\times n} + {\\boldsymbol{\\Phi}}_{m \\times r} {\\boldsymbol{Z}}_{r \\times n} + {\\boldsymbol{E}}_{m \\times n} \\] where \\(m \\gg n &gt; d + r\\). Here, only \\({\\boldsymbol{Y}}\\) and \\({\\boldsymbol{X}}\\) are observed, so we must combine many regressors model fitting techniques with latent variable estimation. The variables \\({\\boldsymbol{Z}}\\) are called surrogate variables for what would be a complete model of all systematic variation. 79.1 Procedure The main challenge is that the row spaces of \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{Z}}\\) may overlap. Even when \\({\\boldsymbol{X}}\\) is the result of a randomized experiment, there will be a high probability that the row spaces of \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{Z}}\\) have some overlap. Therefore, one cannot simply estimate \\({\\boldsymbol{Z}}\\) by applying a latent variable esitmation method on the residuals \\({\\boldsymbol{Y}}- \\hat{{\\boldsymbol{B}}} {\\boldsymbol{X}}\\) or on the observed response data \\({\\boldsymbol{Y}}\\). In the former case, we will only estimate \\({\\boldsymbol{Z}}\\) in the space orthogonal to \\(\\hat{{\\boldsymbol{B}}} {\\boldsymbol{X}}\\). In the latter case, the estimate of \\({\\boldsymbol{Z}}\\) may modify the signal we can estimate in \\({\\boldsymbol{B}}{\\boldsymbol{X}}\\). A recent method, takes an EM approach to esitmating \\({\\boldsymbol{Z}}\\) in the model \\[ {\\boldsymbol{Y}}_{m \\times n} = {\\boldsymbol{B}}_{m \\times d} {\\boldsymbol{X}}_{d \\times n} + {\\boldsymbol{\\Phi}}_{m \\times r} {\\boldsymbol{Z}}_{r \\times n} + {\\boldsymbol{E}}_{m \\times n}. \\] It is shown to be necessary to penalize the likelihood in the estimation of \\({\\boldsymbol{B}}\\) — i.e., form shrinkage estimates of \\({\\boldsymbol{B}}\\) — in order to properly balance the row spaces of \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{Z}}\\). The regularized EM algorithm, called cross-dimensonal inference (CDI) iterates between Estimate \\({\\boldsymbol{Z}}\\) from \\({\\boldsymbol{Y}}- \\hat{{\\boldsymbol{B}}}^{\\text{Reg}} {\\boldsymbol{X}}\\) Estimate \\({\\boldsymbol{B}}\\) from \\({\\boldsymbol{Y}}- \\hat{{\\boldsymbol{\\Phi}}} \\hat{{\\boldsymbol{Z}}}\\) where \\(\\hat{{\\boldsymbol{B}}}^{\\text{Reg}}\\) is a regularized or shrunken estimate of \\({\\boldsymbol{B}}\\). It can be shown that when the regularization can be represented by a prior distribution on \\({\\boldsymbol{B}}\\) then this algorithm achieves the MAP. 79.2 Example: Kidney Expr by Age In Storey et al. (2005), we considered a study where kidney samples were obtained on individuals across a range of ages. The goal was to identify genes with expression associated with age. &gt; library(edge) &gt; library(splines) &gt; load(&quot;./data/kidney.RData&quot;) &gt; age &lt;- kidcov$age &gt; sex &lt;- kidcov$sex &gt; dim(kidexpr) [1] 34061 72 &gt; cov &lt;- data.frame(sex = sex, age = age) &gt; null_model &lt;- ~sex &gt; full_model &lt;- ~sex + ns(age, df = 3) &gt; de_obj &lt;- build_models(data = kidexpr, cov = cov, + null.model = null_model, + full.model = full_model) &gt; de_lrt &lt;- lrt(de_obj, nullDistn = &quot;bootstrap&quot;, bs.its = 100, verbose=FALSE) &gt; qobj1 &lt;- qvalueObj(de_lrt) &gt; hist(qobj1) Now that we have completed a standard generalized LRT, let’s estimate \\({\\boldsymbol{Z}}\\) (the surrogate variables) using the sva package as accessed via the edge package. &gt; dim(nullMatrix(de_obj)) [1] 72 2 &gt; de_sva &lt;- apply_sva(de_obj, n.sv=4, method=&quot;irw&quot;, B=10) Number of significant surrogate variables is: 4 Iteration (out of 10 ):1 2 3 4 5 6 7 8 9 10 &gt; dim(nullMatrix(de_sva)) [1] 72 6 &gt; de_svalrt &lt;- lrt(de_sva, nullDistn = &quot;bootstrap&quot;, bs.its = 100, verbose=FALSE) &gt; qobj2 &lt;- qvalueObj(de_svalrt) &gt; hist(qobj2) &gt; summary(qobj1) Call: qvalue(p = pval) pi0: 0.8081212 Cumulative number of significant calls: &lt;1e-04 &lt;0.001 &lt;0.01 &lt;0.025 &lt;0.05 &lt;0.1 &lt;1 p-value 27 161 798 1676 2906 5271 34061 q-value 0 0 2 4 10 27 34061 local FDR 0 0 2 2 5 18 34061 &gt; summary(qobj2) Call: qvalue(p = pval) pi0: 0.6925105 Cumulative number of significant calls: &lt;1e-04 &lt;0.001 &lt;0.01 &lt;0.025 &lt;0.05 &lt;0.1 &lt;1 p-value 28 151 1001 2051 3549 6168 34061 q-value 0 0 3 4 6 51 34061 local FDR 0 0 2 2 3 28 34053 P-values from two analyses are fairly different. &gt; data.frame(lrt=-log10(qobj1$pval), sva=-log10(qobj2$pval)) %&gt;% + ggplot() + geom_point(aes(x=lrt, y=sva), alpha=0.3) + geom_abline() "],
["references.html", "References", " References The following books have served as references while writing Foundations of Applied Statistics: All of Statistics, by Larry Wasserman All of Nonparametric Statistics, by Larry Wasserman The Elements of Data Analytic Style, by Jeff Leek The Elements of Statistical Learning, by Hastie, Tibshirani, and Friedman Nonparametric Regression and Generalized Linear Models: A roughness penalty approach, by Green and Silverman Pattern Recognition and Machine Learning, by Christopher Bishop R for Data Science, by Wickham and Grolemund R Programming for Data Science, by Roger Peng Statistical Inference, by Casella and Berger "]
]
