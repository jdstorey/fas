[
["index.html", "Foundations of Applied Statistics Data Wrangling, Exploratory Data Analysis, Inference, and Modeling", " Foundations of Applied Statistics Data Wrangling, Exploratory Data Analysis, Inference, and Modeling John D. Storey Created 2017-02-01; Last modified 2019-02-25 "],
["preface.html", "Preface", " Preface Hello, current and future statistics enthusiasts! Welcome to my ambitious book project. I’m a professor at Princeton University who works at the interface of statistics, genetics, and genomics. My primary background is in statistics. I have worked with undergraduate and graduate students who come from many backgrounds: biology, computer science, engineering, mathematics, physics, and statistics. The goal for my students is to turn them into skilled applied statsiticians who are interested in tackling real world problems. The content of this book is what I want them to learn as soon as possible after joining my lab. More generally, this book establishes a foundation in applied statistics and data science for those interested in pursuing data-driven research. I have included many different types of data sets here, but the book places a special emphasis on modern biological problems and data sets due to my particular interests. This book also serves as the primary reference for my Princeton University course verbosely titled, Foundations of Applied Statistics and Data Science (with Applications in Biology), which has a web site, http://jdstorey.org/asdscourse/. The content of this book started with an undergraduate course I developed, called Introduction to Data Science. In that course, I made slides using the amazing R package revealjs. When I then developed Foundations of Applied Statistics and Data Science (with Applications in Biology), I created new slides also using revealjs; these slides can be found at http://jdstorey.org/asdscourse2017/lectures/, and the source code at https://github.com/jdstorey/asdslectures. Although, I no longer use slides to teach this course, I was nevertheless able to easily move the R Markdown used to make the slides into a book format, using the also amazing bookdown package. The current draft of this book contains most of the statistics and R code that I intend to include. However, because it is being build from slides, there are two things to keep in mind. First, I will be adding a substantial amount of exposition and data analyses in the book. It currently mostly reads as terse slides. I will also be modifying some of the formatting to better suit bookdown. Second, it is common to quote from sources verbatim when making slides (citing the source, of course). It is not so common to do this in a book. Threfore, you will see more material quoted verbatim from outside sources than is typical in a book. I intend to remedy this over time. This books is organized into several parts: Introduction R Data wrangling Explortatory data analysis Probability Frequentist inference Bayesian inference Numerical methods for likelihood Nonparametric inference Statistical models High-dimensional inference Latent variable models Source Files The source files for this book are maintained on GitHub: https://github.com/jdstorey/fas Feel free to visit this repository to help me make the book better. About The Author John Storey received his PhD from Stanford University in statistics with a PhD minor in genetics. He then held faculty positions at the University of California, Berkeley and the University of Washington. Since 2008, he has a been a professor in the Lewis-Sigler Institute for Integrative Genomics at Princeton University. Storey’s research has been concerned with developing and applying statistical methods in genetics and genomics. He has made pioneering contributions to the development and application of methods for significance testing and inference on high-dimensional data. In 2014, Storey was appointed the founding Director of the Center for Statistics and Machine Learning at Princeton University and he was also named the William R. Harman ‘63 and Mary-Love Harman Professor in Genomics. He is an elected fellow of the American Association for the Advancement of Science as well as the Institute of Mathematical Statistics. He is the winner of the 2015 COPSS Presidents’ Award. He is also the winner of the 2015 Mortimer Spiegelman Award given by the American Public Health Association for outstanding contributions to public health statistics. "],
["stat-overview.html", "1 Statistics 1.1 History 1.2 Definition 1.3 Relationship to Machine Learning 1.4 Relationship to Data Science", " 1 Statistics 1.1 History The practice of statistics has been around for hundreds of years. The early application of statistics was centered around collecting demographic and economic data for governments. The terms “statistics” was coined in the mid-1700s, which is derived from its relationship to state or government data. Early methodological ideas introduced in statistics involved procedures such as calculating the mean or median, representing data graphically, and trying to model the distribution of “errors” between observed data and a model. In the 1800s, probability started to become integrated into statistical thinking, which lead to the era of inferential statistics; it gave rise to deep results on how we design and analyze studies to use finite amounts of data that have been collected through a probabilistic mechanism. During the early to mid 1900s, inferential statistics became a well developed and understood component to statistics. Its ties to probability were deepened, especially with the formal axiomatization and rigorous development of probability. The latter half of the 1900s resulted in another major leap forward for the field of statistics, with the introduction of modern computing. This had a massive impact on how we collect and analyze data. Statistics became less reliant on mathematical models and more immersed in computational approaches. In the 2000s we have witnessed yet another major leap forward. Data collection has never been faster, cheaper, or larger than today. We are able to collect and analyze massive amounts of data in most areas of science and industry. This has led to a sea-change in statistics, both in its scope and in its importance. The field of statistics has never been more challenging or impactful than it is today. 1.2 Definition The modern definition of Statistics is the study of how to extract information from data, including how to collect, organize, analyze, and present information in data. Applied Statistics is concerned with the practical considerations and implementations needed to carry out a statistical analysis. In Chapter 2 below, we make this definition concrete by discussing the various problems that statistics tackles. 1.3 Relationship to Machine Learning Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Machine learning is closely related to and often overlaps with computational statistics; a discipline which also focuses in prediction-making through the use of computers. https://en.wikipedia.org/wiki/Machine_learning 1.4 Relationship to Data Science Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured, which is a continuation of some of the data analysis fields such as statistics, data mining, and predictive analytics. https://en.wikipedia.org/wiki/Data_science 1.4.1 Some History John Tukey John Tukey pioneered a field called “exploratory data analysis” (EDA) From The Future of Data Analysis (1962) Annals of Mathematical Statistics … For a long time I have thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt. All in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. Data analysis is a larger and more varied field than inference, or incisive procedures, or allocation. Jeff Wu In November 1997, C.F. Jeff Wu gave the inaugural lecture entitled “Statistics = Data Science?”. In this lecture, he characterized statistical work as a trilogy of data collection, data modeling and analysis, and decision making. In his conclusion, he initiated the modern, non-computer science, usage of the term “data science” and advocated that statistics be renamed data science and statisticians data scientists. https://en.wikipedia.org/wiki/Data_science William Cleveland In 2001, William Cleveland introduced data science as an independent discipline, extending the field of statistics to incorporate “advances in computing with data” in his article Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics in International Statistical Review Cleveland establishes six technical areas which he believed to encompass the field of data science: multidisciplinary investigations models and methods for data computing with data pedagogy tool evaluation theory https://en.wikipedia.org/wiki/Data_science 1.4.2 Industry Individuals working in industry began to call themselves “data scientists” in the late 2000’s, but this was far after statisticians had introduced the field. For exampe, "],
["challenges.html", "2 Components of Applied Statistics 2.1 Study Design 2.2 Data Wrangling 2.3 Data Analysis 2.4 Communication", " 2 Components of Applied Statistics Let’s first cosnider the central challenges of applied statistics, shown in the following schematic I’ll call the “central dogma of applied statistics”, shown in Figure 2.1 Figure 2.1: Central Dogma of Statistics 2.1 Study Design An applied statistics project is usually preceded by a scientific question that involves the collection and analysis of data. The design of the data should involve careful application of statistical principles to design which data are to be collected and how the data will be measured. The study design should also be driven by the questions that will be answered and the type of applied statistical analysis techniques will be employed. Study design is an area that is almost solely studied by statisticians and it is one of the core strengths of the field of statistics. We will be considering study design throughout this book. 2.2 Data Wrangling Data wrangling is a new terms that refers to the process of convertng raw data, which is often very messy, into data that can be readily analyzed. The importance of this activity has grown substantially in recent years as data sets have becoem larger and more comoplex. Data wrangling 2.3 Data Analysis 2.3.1 Exploratory Data Analysis 2.3.2 Modeling 2.3.3 Inference 2.3.4 Prediciton 2.4 Communication "],
["data-sets-used-in-this-book.html", "3 Data Sets Used in this Book", " 3 Data Sets Used in this Book I will eventually include several data sets that will be used throughout the book. These data sets will be described here. As of now, all of the R examples are completed with a variety of smaller data sets. Threfore, the R code I demonstrate will not change much, but there will be a more consistent use of data sets that I find interesting. "],
["r-basics.html", "4 R Basics 4.1 What is R? 4.2 Pros and Cons of R 4.3 RStudio 4.4 Getting Started in R", " 4 R Basics 4.1 What is R? R is a programming language, a high-level “interpreted language” R is an interactive environment R is used for doing statistics and data science 4.2 Pros and Cons of R R is free and open-source R stays on the cutting-edge because of its ability to utilize independently developed “packages” R has some peculiar featues that experienced programmers should note (see The R Inferno) R has an amazing community of passionate users and developers 4.3 RStudio RStudio is an IDE (integrated development environment) for R It contains many useful features for using R We will use the free version of RStudio in this course 4.4 Getting Started in R 4.4.1 Calculator Operations on numbers: + - * / ^ &gt; 2+1 [1] 3 &gt; 6+3*4-2^3 [1] 10 &gt; 6+(3*4)-(2^3) [1] 10 4.4.2 Atomic Classes There are five atomic classes (or modes) of objects in R: character complex integer logical numeric (real number) There is a sixth called “raw” that we will not discuss. 4.4.3 Assigning Values to Variables &gt; x &lt;- &quot;qcb508&quot; # character &gt; x &lt;- 2+1i # complex &gt; x &lt;- 4L # integer &gt; x &lt;- TRUE # logical &gt; x &lt;- 3.14159 # numeric Note: Anything typed after the # sign is not evaluated. The # sign allows you to add comments to your code. 4.4.4 More Ways to Assign Values &gt; x &lt;- 1 &gt; 1 -&gt; x &gt; x = 1 In this class, we ask that you only use x &lt;- 1. 4.4.5 Evaluation When a complete expression is entered at the prompt, it is evaluated and the result of the evaluated expression is returned. The result may be auto-printed. &gt; x &lt;- 1 &gt; x+2 [1] 3 &gt; print(x) [1] 1 &gt; print(x+2) [1] 3 4.4.6 Functions There are many useful functions included in R. “Packages” (covered later) can be loaded as libraries to provide additional functions. You can also write your own functions in any R session. Here are some examples of built-in functions: &gt; x &lt;- 2 &gt; print(x) [1] 2 &gt; sqrt(x) [1] 1.414214 &gt; log(x) [1] 0.6931472 &gt; class(x) [1] &quot;numeric&quot; &gt; is.vector(x) [1] TRUE 4.4.7 Accessing Help in R You can open the help file for any function by typing ? with the functions name. Here is an example: &gt; ?sqrt There’s also a function help.search that can do general searches for help. You can learn about it by typing: &gt; ?help.search It’s also useful to use Google: for example, “r help square root”. The R help files are also on the web. 4.4.8 Variable Names In the previous examples, we used x as our variable name. Do not use the following variable names, as they have special meanings in R: c, q, s, t, C, D, F, I, T When combining two words for a given variable, we recommend one of these options: &gt; my_variable &lt;- 1 &gt; myVariable &lt;- 1 Variable names such as my.variable are problematic because of the special use of “.” in R. 4.4.9 Vectors The vector is the most basic object in R. You can create vectors in a number of ways. &gt; x &lt;- c(1, 2, 3, 4, 5) &gt; x [1] 1 2 3 4 5 &gt; &gt; y &lt;- 1:40 &gt; y [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 &gt; &gt; z &lt;- seq(from=0, to=100, by=10) &gt; z [1] 0 10 20 30 40 50 60 70 80 90 100 &gt; length(z) [1] 11 4.4.10 Vectors Programmers: vectors are indexed starting at 1, not 0 A vector can only contain elements of a single class: &gt; x &lt;- &quot;a&quot; &gt; x[0] character(0) &gt; x[1] [1] &quot;a&quot; &gt; &gt; y &lt;- 1:3 &gt; z &lt;- c(x, y, TRUE, FALSE) &gt; z [1] &quot;a&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;TRUE&quot; &quot;FALSE&quot; 4.4.11 Matrices Like vectors, matrices are objects that can contain elements of only one class. &gt; m &lt;- matrix(1:6, nrow=2, ncol=3) &gt; m [,1] [,2] [,3] [1,] 1 3 5 [2,] 2 4 6 &gt; &gt; m &lt;- matrix(1:6, nrow=2, ncol=3, byrow=TRUE) &gt; m [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 4.4.12 Factors In statistics, factors encode categorical data. &gt; paint &lt;- factor(c(&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, + &quot;red&quot;)) &gt; paint [1] red white blue blue red red Levels: blue red white &gt; &gt; table(paint) paint blue red white 2 3 1 &gt; unclass(paint) [1] 2 3 1 1 2 2 attr(,&quot;levels&quot;) [1] &quot;blue&quot; &quot;red&quot; &quot;white&quot; 4.4.13 Lists Lists allow you to hold different classes of objects in one variable. &gt; x &lt;- list(1:3, &quot;a&quot;, c(TRUE, FALSE)) &gt; x [[1]] [1] 1 2 3 [[2]] [1] &quot;a&quot; [[3]] [1] TRUE FALSE &gt; &gt; ## access any element of the list &gt; x[[2]] [1] &quot;a&quot; &gt; x[[3]][2] [1] FALSE 4.4.14 Lists with Names The elements of a list can be given names. &gt; x &lt;- list(counting=1:3, char=&quot;a&quot;, logic=c(TRUE, FALSE)) &gt; x $counting [1] 1 2 3 $char [1] &quot;a&quot; $logic [1] TRUE FALSE &gt; &gt; ## access any element of the list &gt; x$char [1] &quot;a&quot; &gt; x$logic[2] [1] FALSE 4.4.15 Missing Values In data analysis and model fitting, we often have missing values. NA represents missing values and NaN means “not a number”, which is a special type of missing value. &gt; m &lt;- matrix(nrow=3, ncol=3) &gt; m [,1] [,2] [,3] [1,] NA NA NA [2,] NA NA NA [3,] NA NA NA &gt; 0/1 [1] 0 &gt; 1/0 [1] Inf &gt; 0/0 [1] NaN 4.4.16 NULL NULL is a special type of reserved value in R. &gt; x &lt;- vector(mode=&quot;list&quot;, length=3) &gt; x [[1]] NULL [[2]] NULL [[3]] NULL 4.4.17 Coercion We saw earlier that when we mixed classes in a vector they were all coerced to be of type character: &gt; c(&quot;a&quot;, 1:3, TRUE, FALSE) [1] &quot;a&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;TRUE&quot; &quot;FALSE&quot; You can directly apply coercion with functions as.numeric(), as.character(), as.logical(), etc. This doesn’t always work out well: &gt; x &lt;- 1:3 &gt; as.character(x) [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &gt; &gt; y &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) &gt; as.numeric(y) Warning: NAs introduced by coercion [1] NA NA NA 4.4.18 Data Frames The data frame is one of the most important objects in R. Data sets very often come in tabular form of mixed classes, and data frames are constructed exactly for this. Data frames are lists where each element has the same length. 4.4.19 Data Frames &gt; df &lt;- data.frame(counting=1:3, char=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), + logic=c(TRUE, FALSE, TRUE)) &gt; df counting char logic 1 1 a TRUE 2 2 b FALSE 3 3 c TRUE &gt; &gt; nrow(df) [1] 3 &gt; ncol(df) [1] 3 4.4.20 Data Frames &gt; dim(df) [1] 3 3 &gt; &gt; names(df) [1] &quot;counting&quot; &quot;char&quot; &quot;logic&quot; &gt; &gt; attributes(df) $names [1] &quot;counting&quot; &quot;char&quot; &quot;logic&quot; $class [1] &quot;data.frame&quot; $row.names [1] 1 2 3 4.4.21 Attributes Attributes give information (or meta-data) about R objects. The previous slide shows attributes(df), the attributes of the data frame df. &gt; x &lt;- 1:3 &gt; attributes(x) # no attributes for a standard vector NULL &gt; &gt; m &lt;- matrix(1:6, nrow=2, ncol=3) &gt; attributes(m) $dim [1] 2 3 &gt; paint &lt;- factor(c(&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, + &quot;red&quot;)) &gt; attributes(paint) $levels [1] &quot;blue&quot; &quot;red&quot; &quot;white&quot; $class [1] &quot;factor&quot; 4.4.22 Names Names can be assigned to columns and rows of vectors, matrices, and data frames. This makes your code easier to write and read. &gt; names(x) &lt;- c(&quot;Princeton&quot;, &quot;Rutgers&quot;, &quot;Penn&quot;) &gt; x Princeton Rutgers Penn 1 2 3 &gt; &gt; colnames(m) &lt;- c(&quot;NJ&quot;, &quot;NY&quot;, &quot;PA&quot;) &gt; rownames(m) &lt;- c(&quot;East&quot;, &quot;West&quot;) &gt; m NJ NY PA East 1 3 5 West 2 4 6 &gt; colnames(m) [1] &quot;NJ&quot; &quot;NY&quot; &quot;PA&quot; 4.4.23 Accessing Names Displaying or assigning names to these three types of objects does not have consistent syntax. Object Column Names Row Names vector names() N/A data frame names() row.names() data frame colnames() rownames() matrix colnames() rownames() "],
["reproducible-data-analysis.html", "5 Reproducible Data Analysis 5.1 Definition and Motivation 5.2 Reproducible vs. Replicable 5.3 Steps to a Reproducible Analysis 5.4 Organizing Your Data Analysis 5.5 Common Mistakes 5.6 R Markdown", " 5 Reproducible Data Analysis 5.1 Definition and Motivation Reproducibility involves being able to recalculate the exact numbers in a data analysis using the code and raw data provided by the analyst. Reproducibility is often difficult to achieve and has slowed down the discovery of important data analytic errors. Reproducibility should not be confused with “correctness” of a data analysis. A data analysis can be fully reproducible and recreate all numbers in an analysis and still be misleading or incorrect. From Elements of Data Analytic Style, by Leek 5.2 Reproducible vs. Replicable Reproducible research is often used these days to indicate the ability to recalculate the exact numbers in a data analysis Replicable research results often refers to the ability to independently carry out a study (thereby collecting new data) and coming to equivalent conclusions as the original study These two terms are often confused, so it is important to clearly state the definition 5.3 Steps to a Reproducible Analysis Use a data analysis script – e.g., R Markdown (discussed next section!) or iPython Notebooks Record versions of software and paramaters – e.g., use sessionInfo() in R as in hw_1.Rmd Organize your data analysis Use version control – e.g., GitHub Set a random number generator seed – e.g., use set.seed() in R Have someone else run your analysis 5.4 Organizing Your Data Analysis Data raw data processed data (sometimes multiple stages for very large data sets) Figures Exploratory figures Final figures R code Raw or unused scripts Data processing scripts Analysis scripts Text README files explaining what all the components are Final data analysis products like presentations/writeups 5.5 Common Mistakes Failing to use a script for your analysis Not recording software and package version numbers or other settings used Not sharing your data and code Using reproducibility as a social weapon 5.6 R Markdown 5.6.1 R + Markdown + knitr R Markdown was developed by the RStudio team to allow one to write reproducible research documents using Markdown and knitr. This is contained in the rmarkdown package, but can easily be carried out in RStudio. Markdown was originally developed as a very simply text-to-html conversion tool. With Pandoc, Markdown is a very simply text-to-X conversion tool where X can be many different formats: html, LaTeX, PDF, Word, etc. 5.6.2 R Markdown Files R Markdown documents begin with a metadata section, the YAML header, that can include information on the title, author, and date as well as options for customizing output. title: &quot;QCB 508 -- Homework 1&quot; author: &quot;Your Name&quot; date: February 23, 2017 output: pdf_document Many options are available. See http://rmarkdown.rstudio.com for full documentation. 5.6.3 Markdown Headers: # Header 1 ## Header 2 ### Header 3 Emphasis: *italic* **bold** _italic_ __bold__ Tables: First Header | Second Header ------------- | ------------- Content Cell | Content Cell Content Cell | Content Cell Unordered list: - Item 1 - Item 2 - Item 2a - Item 2b Ordered list: 1. Item 1 2. Item 2 3. Item 3 - Item 3a - Item 3b Links: http://example.com [linked phrase](http://example.com) Blockquotes: Florence Nightingale once said: &gt; For the sick it is important &gt; to have the best. Plain code blocks: ``` This text is displayed verbatim with no formatting. ``` Inline Code: We use the `print()` function to print the contents of a variable in R. Additional documentation and examples can be found here and here. 5.6.4 LaTeX LaTeX is a markup language for technical writing, especially for mathematics. It can be include in R Markdown files. For example, $y = a + bx + \\epsilon$ produces \\(y = a + bx + \\epsilon\\) Here is an introduction to LaTeX and here is a primer on LaTeX for R Markdown. 5.6.5 knitr The knitr R package allows one to execute R code within a document, and to display the code itself and its output (if desired). This is particularly easy to do in the R Markdown setting. For example… Placing the following text in an R Markdown file The sum of 2 and 2 is `r 2+2`. produces in the output file The sum of 2 and 2 is 4. 5.6.6 knitr Chunks Chunks of R code separated from the text. In R Markdown: ```{r} x &lt;- 2 x + 1 print(x) ``` Output in file: &gt; x &lt;- 2 &gt; x + 1 [1] 3 &gt; print(x) [1] 2 5.6.7 Chunk Option: echo In R Markdown: ```{r, echo=FALSE} x &lt;- 2 x + 1 print(x) ``` Output in file: [1] 3 [1] 2 5.6.8 Chunk Option: results In R Markdown: ```{r, results=&quot;hide&quot;} x &lt;- 2 x + 1 print(x) ``` Output in file: &gt; x &lt;- 2 &gt; x + 1 &gt; print(x) 5.6.9 Chunk Option: include In R Markdown: ```{r, include=FALSE} x &lt;- 2 x + 1 print(x) ``` Output in file: (nothing) 5.6.10 Chunk Option: eval In R Markdown: ```{r, eval=FALSE} x &lt;- 2 x + 1 print(x) ``` Output in file: &gt; x &lt;- 2 &gt; x + 1 &gt; print(x) 5.6.11 Chunk Names Naming your chunks can be useful for identifying them in your file and during the execution, and also to denote dependencies among chunks. ```{r my_first_chunk} x &lt;- 2 x + 1 print(x) ``` 5.6.12 knitr Option: cache Sometimes you don’t want to run chunks over and over, especially for large calculations. You can “cache” them. ```{r chunk1, cache=TRUE, include=FALSE} x &lt;- 2 ``` ```{r chunk2, cache=TRUE, dependson=&quot;chunk1&quot;} y &lt;- 3 z &lt;- x + y ``` This creates a directory called cache in your working directory that stores the objects created or modified in these chunks. When chunk1 is modified, it is re-run. Since chunk2 depends on chunk1, it will also be re-run. 5.6.13 knitr Options: figures You can add chunk options regarding the placement and size of figures. Examples include: fig.width fig.height fig.align 5.6.14 Changing Default Chunk Settings If you will be using the same options on most chunks, you can set default options for the entire document. Run something like this at the beginning of your document with your desired chunk options. ```{r my_opts, cache=FALSE, echo=FALSE} library(&quot;knitr&quot;) opts_chunk$set(fig.align=&quot;center&quot;, fig.height=4, fig.width=6) ``` 5.6.15 Documentation and Examples http://yihui.name/knitr/ http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html https://github.com/jdstorey/asdslectures "],
["r-programming.html", "6 R Programming 6.1 Control Structures 6.2 Vectorized Operations 6.3 Subsetting R Objects 6.4 Functions 6.5 Environment 6.6 Packages 6.7 Organizing Your Code", " 6 R Programming 6.1 Control Structures 6.1.1 Rationale Control structures in R allow you to control the flow of execution of a series of R expressions They allow you to put some logic into your R code, rather than just always executing the same R code every time Control structures also allow you to respond to inputs or to features of the data and execute different R expressions accordingly Paraphrased from R Programming for Data Science, by Peng 6.1.2 Common Control Structures if and else: testing a condition and acting on it for: execute a loop a fixed number of times while: execute a loop while a condition is true repeat: execute an infinite loop (must break out of it to stop) break: break the execution of a loop next: skip an interation of a loop From R Programming for Data Science, by Peng 6.1.3 Some Boolean Logic R has built-in functions that produce TRUE or FALSE such as is.vector or is.na. You can also do the following: x == y : does x equal y? x &gt; y : is x greater than y? (also &lt; less than) x &gt;= y : is x greater than or equal to y? x &amp;&amp; y : are both x and y true? x || y : is either x or y true? !is.vector(x) : this is TRUE if x is not a vector 6.1.4 if Idea: if(&lt;condition&gt;) { ## do something } ### Continue with rest of code Example: &gt; x &lt;- c(1,2,3) &gt; if(is.numeric(x)) { + x+2 + } [1] 3 4 5 6.1.5 if-else Idea: if(&lt;condition&gt;) { ## do something } else { ## do something else } Example: &gt; x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) &gt; if(is.numeric(x)) { + print(x+2) + } else { + class(x) + } [1] &quot;character&quot; 6.1.6 for Loops Example: &gt; for(i in 1:10) { + print(i) + } [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 [1] 6 [1] 7 [1] 8 [1] 9 [1] 10 Examples: &gt; x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) &gt; &gt; for(i in 1:4) { + print(x[i]) + } [1] &quot;a&quot; [1] &quot;b&quot; [1] &quot;c&quot; [1] &quot;d&quot; &gt; &gt; for(i in seq_along(x)) { + print(x[i]) + } [1] &quot;a&quot; [1] &quot;b&quot; [1] &quot;c&quot; [1] &quot;d&quot; 6.1.7 Nested for Loops Example: &gt; m &lt;- matrix(1:6, nrow=2, ncol=3, byrow=TRUE) &gt; &gt; for(i in seq_len(nrow(m))) { + for(j in seq_len(ncol(m))) { + print(m[i,j]) + } + } [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 [1] 6 6.1.8 while Example: &gt; x &lt;- 1:10 &gt; idx &lt;- 1 &gt; &gt; while(x[idx] &lt; 4) { + print(x[idx]) + idx &lt;- idx + 1 + } [1] 1 [1] 2 [1] 3 &gt; &gt; idx [1] 4 Repeats the loop until while the condition is TRUE. 6.1.9 repeat Example: &gt; x &lt;- 1:10 &gt; idx &lt;- 1 &gt; &gt; repeat { + print(x[idx]) + idx &lt;- idx + 1 + if(idx &gt;= 4) { + break + } + } [1] 1 [1] 2 [1] 3 &gt; &gt; idx [1] 4 Repeats the loop until break is executed. 6.1.10 break and next break ends the loop. next skips the rest of the current loop iteration. Example: &gt; x &lt;- 1:1000 &gt; for(idx in 1:1000) { + # %% calculates division remainder + if((x[idx] %% 2) &gt; 0) { + next + } else if(x[idx] &gt; 10) { # an else-if!! + break + } else { + print(x[idx]) + } + } [1] 2 [1] 4 [1] 6 [1] 8 [1] 10 6.2 Vectorized Operations 6.2.1 Calculations on Vectors R is usually smart about doing calculations with vectors. Examples: &gt; &gt; x &lt;- 1:3 &gt; y &lt;- 4:6 &gt; &gt; 2*x # same as c(2*x[1], 2*x[2], 2*x[3]) [1] 2 4 6 &gt; x + 1 # same as c(x[1]+1, x[2]+1, x[3]+1) [1] 2 3 4 &gt; x + y # same as c(x[1]+y[1], x[2]+y[2], x[3]+y[3]) [1] 5 7 9 &gt; x*y # same as c(x[1]*y[1], x[2]*y[2], x[3]*y[3]) [1] 4 10 18 6.2.2 A Caveat If two vectors are of different lengths, R tries to find a solution for you (and doesn’t always tell you). &gt; x &lt;- 1:5 &gt; y &lt;- 1:2 &gt; x+y Warning in x + y: longer object length is not a multiple of shorter object length [1] 2 4 4 6 6 What happened here? 6.2.3 Vectorized Matrix Operations Operations on matrices are also vectorized. Example: &gt; x &lt;- matrix(1:4, nrow=2, ncol=2, byrow=TRUE) &gt; y &lt;- matrix(1:4, nrow=2, ncol=2) &gt; &gt; x+y [,1] [,2] [1,] 2 5 [2,] 5 8 &gt; &gt; x*y [,1] [,2] [1,] 1 6 [2,] 6 16 6.2.4 Mixing Vectors and Matrices What happens when we do calculations involving a vector and a matrix? Example: &gt; x &lt;- matrix(1:6, nrow=2, ncol=3, byrow=TRUE) &gt; z &lt;- 1:2 &gt; &gt; x + z [,1] [,2] [,3] [1,] 2 3 4 [2,] 6 7 8 &gt; &gt; x * z [,1] [,2] [,3] [1,] 1 2 3 [2,] 8 10 12 6.2.5 Mixing Vectors and Matrices Another example: &gt; x &lt;- matrix(1:6, nrow=2, ncol=3, byrow=TRUE) &gt; z &lt;- 1:3 &gt; &gt; x + z [,1] [,2] [,3] [1,] 2 5 5 [2,] 6 6 9 &gt; &gt; x * z [,1] [,2] [,3] [1,] 1 6 6 [2,] 8 5 18 What happened this time? 6.2.6 Vectorized Boolean Logic We saw &amp;&amp; and || applied to pairs of logical values. We can also vectorize these operations. &gt; a &lt;- c(TRUE, TRUE, FALSE) &gt; b &lt;- c(FALSE, TRUE, FALSE) &gt; &gt; a | b [1] TRUE TRUE FALSE &gt; a &amp; b [1] FALSE TRUE FALSE 6.3 Subsetting R Objects 6.3.1 Subsetting Vectors &gt; x &lt;- 1:8 &gt; &gt; x[1] # extract the first element [1] 1 &gt; x[2] # extract the second element [1] 2 &gt; &gt; x[1:4] # extract the first 4 elements [1] 1 2 3 4 &gt; &gt; x[c(1, 3, 4)] # extract elements 1, 3, and 4 [1] 1 3 4 &gt; x[-c(1, 3, 4)] # extract all elements EXCEPT 1, 3, and 4 [1] 2 5 6 7 8 6.3.2 Subsetting Vectors &gt; names(x) &lt;- letters[1:8] &gt; x a b c d e f g h 1 2 3 4 5 6 7 8 &gt; &gt; x[c(&quot;a&quot;, &quot;b&quot;, &quot;f&quot;)] a b f 1 2 6 &gt; &gt; s &lt;- x &gt; 3 &gt; s a b c d e f g h FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE &gt; x[s] d e f g h 4 5 6 7 8 6.3.3 Subsettng Matrices &gt; x &lt;- matrix(1:6, nrow=2, ncol=3, byrow=TRUE) &gt; x [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 &gt; &gt; x[1,2] [1] 2 &gt; x[1, ] [1] 1 2 3 &gt; x[ ,2] [1] 2 5 6.3.4 Subsettng Matrices &gt; colnames(x) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) &gt; &gt; x[ , c(&quot;B&quot;, &quot;C&quot;)] B C [1,] 2 3 [2,] 5 6 &gt; &gt; x[c(FALSE, TRUE), c(&quot;B&quot;, &quot;C&quot;)] B C 5 6 &gt; &gt; x[2, c(&quot;B&quot;, &quot;C&quot;)] B C 5 6 6.3.5 Subsettng Matrices &gt; s &lt;- (x %% 2) == 0 &gt; s A B C [1,] FALSE TRUE FALSE [2,] TRUE FALSE TRUE &gt; &gt; x[s] [1] 4 2 6 &gt; &gt; x[c(2, 3, 6)] [1] 4 2 6 6.3.6 Subsetting Lists &gt; x &lt;- list(my=1:3, favorite=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), + course=c(FALSE, TRUE, NA)) &gt; &gt; x[[1]] [1] 1 2 3 &gt; x[[&quot;my&quot;]] [1] 1 2 3 &gt; x$my [1] 1 2 3 &gt; x[[c(3,1)]] [1] FALSE &gt; x[[3]][1] [1] FALSE &gt; x[c(3,1)] $course [1] FALSE TRUE NA $my [1] 1 2 3 6.3.7 Subsetting Data Frames &gt; x &lt;- data.frame(my=1:3, favorite=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), + course=c(FALSE, TRUE, NA)) &gt; &gt; x[[1]] [1] 1 2 3 &gt; x[[&quot;my&quot;]] [1] 1 2 3 &gt; x$my [1] 1 2 3 &gt; x[[c(3,1)]] [1] FALSE &gt; x[[3]][1] [1] FALSE &gt; x[c(3,1)] course my 1 FALSE 1 2 TRUE 2 3 NA 3 6.3.8 Subsetting Data Frames &gt; x &lt;- data.frame(my=1:3, favorite=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), + course=c(FALSE, TRUE, NA)) &gt; &gt; x[1, ] my favorite course 1 1 a FALSE &gt; x[ ,3] [1] FALSE TRUE NA &gt; x[ ,&quot;favorite&quot;] [1] a b c Levels: a b c &gt; x[1:2, ] my favorite course 1 1 a FALSE 2 2 b TRUE &gt; x[ ,2:3] favorite course 1 a FALSE 2 b TRUE 3 c NA 6.3.9 Note on Data Frames R often converts character strings to factors unless you specify otherwise. In the previous slide, we saw it converted the “favorite” column to factors. Let’s fix that… &gt; x &lt;- data.frame(my=1:3, favorite=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), + course=c(FALSE, TRUE, NA), + stringsAsFactors=FALSE) &gt; &gt; x[ ,&quot;favorite&quot;] [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &gt; class(x[ ,&quot;favorite&quot;]) [1] &quot;character&quot; 6.3.10 Missing Values &gt; data(&quot;airquality&quot;, package=&quot;datasets&quot;) &gt; head(airquality) Ozone Solar.R Wind Temp Month Day 1 41 190 7.4 67 5 1 2 36 118 8.0 72 5 2 3 12 149 12.6 74 5 3 4 18 313 11.5 62 5 4 5 NA NA 14.3 56 5 5 6 28 NA 14.9 66 5 6 &gt; dim(airquality) [1] 153 6 &gt; which(is.na(airquality$Ozone)) [1] 5 10 25 26 27 32 33 34 35 36 37 39 42 43 45 46 52 [18] 53 54 55 56 57 58 59 60 61 65 72 75 83 84 102 103 107 [35] 115 119 150 &gt; sum(is.na(airquality$Ozone)) [1] 37 6.3.11 Subsetting by Matching &gt; letters [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; [18] &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; &gt; vowels &lt;- c(&quot;a&quot;, &quot;e&quot;, &quot;i&quot;, &quot;o&quot;, &quot;u&quot;) &gt; &gt; letters %in% vowels [1] TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE [12] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [23] FALSE FALSE FALSE FALSE &gt; which(letters %in% vowels) [1] 1 5 9 15 21 &gt; &gt; letters[which(letters %in% vowels)] [1] &quot;a&quot; &quot;e&quot; &quot;i&quot; &quot;o&quot; &quot;u&quot; 6.3.12 Advanced Subsetting The R Programming for Data Science chapter titled “Subsetting R Objects” contains additional material on subsetting that you should know. The Advanced R website contains more detailed information on subsetting that you may find useful. 6.4 Functions 6.4.1 Rationale Writing functions is a core activity of an R programmer. It represents the key step of the transition from a mere user to a developer who creates new functionality for R. Functions are often used to encapsulate a sequence of expressions that need to be executed numerous times, perhaps under slightly different conditions. Functions are also often written when code must be shared with others or the public. From R Programming for Data Science, by Peng 6.4.2 Defining a New Function Functions are defined using the function() directive They are stored as variables, so they can be passed to other functions and assigned to new variables Arguments and a final return object are defined 6.4.3 Example 1 &gt; my_square &lt;- function(x) { + x*x # can also do return(x*x) + } &gt; &gt; my_square(x=2) [1] 4 &gt; &gt; my_fun2 &lt;- my_square &gt; my_fun2(x=3) [1] 9 6.4.4 Example 2 &gt; my_square_ext &lt;- function(x) { + y &lt;- x*x + return(list(x_original=x, x_squared=y)) + } &gt; &gt; my_square_ext(x=2) $x_original [1] 2 $x_squared [1] 4 &gt; &gt; z &lt;- my_square_ext(x=2) 6.4.5 Example 3 &gt; my_power &lt;- function(x, e, say_hello) { + if(say_hello) { + cat(&quot;Hello World!&quot;) + } + x^e + } &gt; &gt; my_power(x=2, e=3, say_hello=TRUE) Hello World! [1] 8 &gt; &gt; z &lt;- my_power(x=2, e=3, say_hello=TRUE) Hello World! &gt; z [1] 8 6.4.6 Default Function Argument Values Some functions have default values for their arguments: &gt; str(matrix) function (data = NA, nrow = 1, ncol = 1, byrow = FALSE, dimnames = NULL) You can define a function with default values by the following: f &lt;- function(x, y=2) { x + y } If the user types f(x=1) then it defaults to y=2, but if the user types f(x=1, y=3), then it executes with these assignments. 6.4.7 The Ellipsis Argument You will encounter functions that include as a possible argument the ellipsis: ... This basically holds arguments that can be passed to functions called within a function. Example: &gt; double_log &lt;- function(x, ...) { + log((2*x), ...) + } &gt; &gt; double_log(x=1, base=2) [1] 1 &gt; double_log(x=1, base=10) [1] 0.30103 6.4.8 Argument Matching R tries to automatically deal with function calls when the arguments are not defined explicity. For example: x &lt;- matrix(1:6, nrow=2, ncol=3, byrow=TRUE) # versus x &lt;- matrix(1:6, 2, 3, TRUE) I strongly recommend that you define arguments explcitly. For example, I can never remember which comes first in matrix(), nrow or ncol. 6.5 Environment 6.5.1 Loading .RData Files An .RData file is a binary file containing R objects. These can be saved from your current R session and also loaded into your current session. &gt; # generally... &gt; # to load: &gt; load(file=&quot;path/to/file_name.RData&quot;) &gt; # to save: &gt; save(file=&quot;path/to/file_name.RData&quot;) &gt; ## assumes file in working directory &gt; load(file=&quot;project_1_R_basics.RData&quot;) &gt; ## loads from our GitHub repository &gt; load(file=url(&quot;https://github.com/SML201/project1/raw/ + master/project_1_R_basics.RData&quot;)) 6.5.2 Listing Objects The objects in your current R session can be listed. An environment can also be specificied in case you have objects stored in different environments. &gt; ls() [1] &quot;num_people_in_precept&quot; &quot;SML201_grade_distribution&quot; [3] &quot;some_ORFE_profs&quot; &gt; &gt; ls(name=globalenv()) [1] &quot;num_people_in_precept&quot; &quot;SML201_grade_distribution&quot; [3] &quot;some_ORFE_profs&quot; &gt; &gt; ## see help file for other options &gt; ?ls 6.5.3 Removing Objects You can remove specific objects or all objects from your R environment of choice. &gt; rm(&quot;some_ORFE_profs&quot;) # removes variable some_ORFE_profs &gt; &gt; rm(list=ls()) # Removes all variables from environment 6.5.4 Advanced The R environment is there to connect object names to object values. The R Programming for Data Science chapter titled “Scoping Rules of R” discussed environments and object names in more detail than we need for this course. A useful discussion about environments can also be found on the Advanced R web site. 6.6 Packages 6.6.1 Rationale “In R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others. As of January 2015, there were over 6,000 packages available on the Comprehensive R Archive Network, or CRAN, the public clearing house for R packages. This huge variety of packages is one of the reasons that R is so successful: the chances are that someone has already solved a problem that you’re working on, and you can benefit from their work by downloading their package.” From http://r-pkgs.had.co.nz/intro.html by Hadley Wickham 6.6.2 Contents of a Package R functions R data objects Help documents for using the package Information on the authors, dependencies, etc. Information to make sure it “plays well” with R and other packages 6.6.3 Installing Packages From CRAN: install.packages(&quot;dplyr&quot;) From GitHub (for advanced users): library(&quot;devtools&quot;) install_github(&quot;hadley/dplyr&quot;) From Bioconductor (basically CRAN for biology): library(&quot;BiocManager&quot;) BiocManager::install(&quot;qvalue&quot;) Be very careful about dependencies when installing from GitHub. Multiple packages: install.packages(c(&quot;dplyr&quot;, &quot;ggplot2&quot;)) Install all dependencies: install.packages(c(&quot;dplyr&quot;, &quot;ggplot2&quot;), dependencies=TRUE) Updating packages: update.packages() 6.6.4 Loading Packages Two ways to load a package: library(&quot;dplyr&quot;) library(dplyr) I prefer the former. 6.6.5 Getting Started with a Package When you install a new package and load it, what’s next? I like to look at the help files and see what functions and data sets a package has. library(&quot;dplyr&quot;) help(package=&quot;dplyr&quot;) 6.6.6 Specifying a Function within a Package You can call a function from a specific package. Suppose you are in a setting where you have two packages loaded that have functions with the same name. dplyr::arrange(mtcars, cyl, disp) This calls the arrange functin specifically from dplyr. The package plyr also has an arrange function. 6.6.7 More on Packages We will be covering several highly used R packages in depth this semester, so we will continue to learn about packages, how they are organized, and how they are used. You can download the “source” of a package from R and take a look at the contents if you want to dig deeper. There are also many good tutorials on creating packages, such as http://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/. 6.7 Organizing Your Code 6.7.1 Suggestions RStudio conveniently tries to automatically format your R code. We suggest the following in general. 1. No more than 80 characters per line (or fewer depending on how R Markdown compiles): really_long_line &lt;- my_function(x=20, y=30, z=TRUE, a=&quot;Joe&quot;, b=3.8) 2. Indent 2 or more characters for nested commands: for(i in 1:10) { if(i &gt; 4) { print(i) } } 3. Generously comment your code. ## a for-loop that prints the index ## whenever it is greater than 4 for(i in 1:10) { if(i &gt; 4) { print(i) } } ## a good way to get partial credit ## if something goes wrong :-) 4. Do not hesitate to write functions to organize tasks. These help to break up your code into more undertsandable pieces, and functions can often be used several times. 6.7.2 Where to Put Files See the Elements of Data Analytic Style chapter titled “Reproducibility” for suggestions on how to organize your files. In this course, we will keep this relatively simple. We will try to provide you with some organization when distributing the projects. "],
["getting-data-in-and-out-of-r.html", "7 Getting Data In and Out of R 7.1 .RData Files 7.2 readr Package 7.3 Scraping from the Web 7.4 APIs", " 7 Getting Data In and Out of R 7.1 .RData Files R objects can be saved to binary .RData files and loaded with the save (or save.image) and load functions, respectively. This is the easiest way to get data into R. 7.2 readr Package There are a number of R packages that provide more sophisticated tools for getting data in and out of R, especially as data sets have become larger and larger. One of those packages is readr for text files. It reads and writes data quickly, provides a useful status bar for large files, and does a good job at determining data types. readr is organized similarly to the base R functions. For example, there are functions read_table, read_csv, write_tsv, and write_csv. See also fread and fwrite from the data.table package. 7.3 Scraping from the Web There are several packages that facilitate “scraping” data from the web, including rvest demonstrated here. &gt; library(&quot;rvest&quot;) &gt; schedule &lt;- read_html(&quot;http://jdstorey.github.io/asdscourse/schedule/&quot;) &gt; first_table &lt;- html_table(schedule)[[1]] &gt; names(first_table) &lt;- c(&quot;week&quot;, &quot;topics&quot;, &quot;reading&quot;) &gt; first_table[2,&quot;week&quot;] &gt; first_table[2,&quot;topics&quot;] %&gt;% strsplit(split=&quot; &quot;) &gt; first_table[2,&quot;reading&quot;] %&gt;% strsplit(split=&quot; &quot;) &gt; grep(&quot;R4DS&quot;, first_table$reading) # which rows (weeks) have R4DS The rvest documentation recommends SelectorGadget, which is “a javascript bookmarklet that allows you to interactively figure out what css selector you need to extract desired components from a page.” &gt; usg_url &lt;- &quot;https://princetonusg.com/senate/&quot; &gt; usg &lt;- read_html(usg_url) &gt; officers &lt;- html_nodes(usg, &quot;.team-member-name&quot;) %&gt;% + html_text &gt; head(officers, n=20) 7.4 APIs API stands for “application programming interface” which is a set of routines, protocols, and tools for building software and applications. A specific website may provide an API for scraping data from that website. There are R packages that provide an interface with specific APIs, such as the twitteR package. "],
["data-wrangling-chapter.html", "8 Data Wrangling 8.1 Definition 8.2 Wrangling Challenges", " 8 Data Wrangling 8.1 Definition Data wrangling is loosely defined as the process of manually converting or mapping data from one “raw” form into another format that allows for more convenient consumption of the data with the help of semi-automated tools. It typically follows a set of general steps which begin with extracting the data in a raw form from the data source, “wrangling” the raw data using algorithms (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use. https://en.wikipedia.org/wiki/Data_wrangling 8.2 Wrangling Challenges Some of the challenges encountered in data wrangling are: Importing files Organizing data sets Transforming data Combining data sets Dealing with various data types (e.g., dates) Identifying errors "],
["tidy-data.html", "9 Tidy Data 9.1 Motivation 9.2 Definition 9.3 Example: Titanic Data 9.4 Rules of Thumb", " 9 Tidy Data 9.1 Motivation “Happy families are all alike; every unhappy family is unhappy in its own way.” – Leo Tolstoy “Tidy datasets are all alike, but every messy dataset is messy in its own way.” – Hadley Wickham From R for Data Science. 9.2 Definition Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. From Wickham (2014), “Tidy Data”, Journal of Statistical Software A dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative). Values are organized in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes. From: Wickham H (2014), “Tidy Data”, Journal of Statistical Software 9.3 Example: Titanic Data According to the Titanic data from the datasets package: 367 males survived, 1364 males perished, 344 females survived, and 126 females perished. How should we organize these data? 9.3.1 Intuitive Format Survived Perished Male 367 1364 Female 344 126 9.3.2 Tidy Format fate sex number perished male 1364 perished female 126 survived male 367 survived female 344 9.4 Rules of Thumb Something is a value if it represents different forms of a common object and it changes throughout the data set. Something is a value if the data can be arranged so that it appears across rows within a column and this makes sense. For example, fate and sex do not satisfy these criteria in the Titanic data, but perished/survived and female/male do. "],
["tidyverse.html", "10 Tidyverse 10.1 Idea 10.2 Packages 10.3 Primary Packages 10.4 Tidying Data 10.5 Reshaping Data", " 10 Tidyverse 10.1 Idea When the data are in tidy format, one can design functions around this format to consistently and intuitively perform data wrangling and analysis operations. The packages containing these are called the “tidyverse.” Note: The idea of tidy data was first proposed by Hadley Wickham and he created several of the core packages, so this used to be called (semi-seriously) the “hadleyverse.” 10.2 Packages The tidyverse is a set of packages that work in harmony because they share common data representations and API design. The tidyverse package is designed to make it easy to install and load core packages from the tidyverse in a single command. https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/ 10.3 Primary Packages dplyr: data manipulation ggplot2: data visualization purrr: functional programming readr: data import tibble: modernization of data frames tidyr: data tidying Loading tidyverse: &gt; library(tidyverse) 10.4 Tidying Data 10.4.1 tidyr Package This package provides a variety of functions that allow one to tidy data. Importantly, it solves two common ways that data come as untidy. gather(): Gathers a variable distributed across two or more columns into a single column. spread(): Spreads a column containing two or more variables into one column per variable. 10.4.2 Untidy Titanic Data This does not satisfy the definition of tidy data because a variable’s observations are distributed as column names. &gt; df &lt;- tibble(sex=c(&quot;male&quot;, &quot;female&quot;), + survived=c(367, 344), + perished=c(1364, 126)) &gt; df # A tibble: 2 x 3 sex survived perished &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 male 367 1364 2 female 344 126 10.4.3 gather() We apply the gather() function to make a column containing the survived and perished observations. &gt; df &lt;- gather(df, survived, perished, + key=&quot;fate&quot;, value=&quot;number&quot;) &gt; df # A tibble: 4 x 3 sex fate number &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 male survived 367 2 female survived 344 3 male perished 1364 4 female perished 126 10.4.4 spread() This example is here to show that spread() does the opposite operation as gather(). It isn’t used appropriately here because we revert the data back to untidy format. &gt; spread(df, key=fate, value=number) # A tibble: 2 x 3 sex perished survived &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 female 126 344 2 male 1364 367 10.4.5 Tidy with spread() Median cost of home and median income per city are two variables included in a single column. This means we need to use spread(). &gt; df # A tibble: 4 x 3 city median_value dollars &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 Boston home 527300 2 Boston income 71738 3 Raleigh home 215700 4 Raleigh income 65778 &gt; spread(df, key=median_value, value=dollars) # A tibble: 2 x 3 city home income &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Boston 527300 71738 2 Raleigh 215700 65778 10.5 Reshaping Data 10.5.1 Wide vs. Long Format Tidy data are in “wide format” in that they have a column for each variable and there is one observed unit per row. However, sometimes it’s useful to transform to “long format.” The simplest long format data have two columns. The first column contains the variable names and the second colum contains the values for the variables. There are “wider” long format data that have additional columns that identify connections between observations. Wide format data is useful for some analyses and long format for others. 10.5.2 reshape2 Package The reshape2 package has three important functions: melt, dcast, and acast. It allows one to move between wide and long tidy data formats. &gt; library(&quot;reshape2&quot;) &gt; library(&quot;datasets&quot;) &gt; data(airquality, package=&quot;datasets&quot;) &gt; names(airquality) [1] &quot;Ozone&quot; &quot;Solar.R&quot; &quot;Wind&quot; &quot;Temp&quot; &quot;Month&quot; &quot;Day&quot; &gt; dim(airquality) [1] 153 6 &gt; airquality &lt;- as_tibble(airquality) 10.5.3 Air Quality Data Set &gt; head(airquality) # A tibble: 6 x 6 Ozone Solar.R Wind Temp Month Day &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 41 190 7.4 67 5 1 2 36 118 8 72 5 2 3 12 149 12.6 74 5 3 4 18 313 11.5 62 5 4 5 NA NA 14.3 56 5 5 6 28 NA 14.9 66 5 6 &gt; tail(airquality) # A tibble: 6 x 6 Ozone Solar.R Wind Temp Month Day &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 14 20 16.6 63 9 25 2 30 193 6.9 70 9 26 3 NA 145 13.2 77 9 27 4 14 191 14.3 75 9 28 5 18 131 8 76 9 29 6 20 223 11.5 68 9 30 10.5.4 Melt Melting can be thought of as melting a piece of solid metal (wide data), so it drips into long format. &gt; aql &lt;- melt(airquality) No id variables; using all as measure variables &gt; head(aql) variable value 1 Ozone 41 2 Ozone 36 3 Ozone 12 4 Ozone 18 5 Ozone NA 6 Ozone 28 &gt; tail(aql) variable value 913 Day 25 914 Day 26 915 Day 27 916 Day 28 917 Day 29 918 Day 30 10.5.5 Guided Melt In the previous example, we lose the fact that a set of measurements occurred on a particular day and month, so we can do a guided melt to keep this information. &gt; aql &lt;- melt(airquality, id.vars = c(&quot;Month&quot;, &quot;Day&quot;)) &gt; head(aql) Month Day variable value 1 5 1 Ozone 41 2 5 2 Ozone 36 3 5 3 Ozone 12 4 5 4 Ozone 18 5 5 5 Ozone NA 6 5 6 Ozone 28 &gt; tail(aql) Month Day variable value 607 9 25 Temp 63 608 9 26 Temp 70 609 9 27 Temp 77 610 9 28 Temp 75 611 9 29 Temp 76 612 9 30 Temp 68 10.5.6 Casting Casting allows us to go from long format to wide format data. It can be visualized as pouring molten metal (long format) into a cast to create a solid piece of metal (wide format). Casting is more difficult because choices have to be made to determine how the wide format will be organized. It often takes some thought and experimentation for new users. Let’s do an example with dcast, which is casting for data frames. 10.5.7 dcast() &gt; aqw &lt;- dcast(aql, Month + Day ~ variable) &gt; head(aqw) Month Day Ozone Solar.R Wind Temp 1 5 1 41 190 7.4 67 2 5 2 36 118 8.0 72 3 5 3 12 149 12.6 74 4 5 4 18 313 11.5 62 5 5 5 NA NA 14.3 56 6 5 6 28 NA 14.9 66 &gt; tail(aqw) Month Day Ozone Solar.R Wind Temp 148 9 25 14 20 16.6 63 149 9 26 30 193 6.9 70 150 9 27 NA 145 13.2 77 151 9 28 14 191 14.3 75 152 9 29 18 131 8.0 76 153 9 30 20 223 11.5 68 "],
["transforming-data.html", "11 Transforming Data 11.1 dplyr Package 11.2 Grammar of dplyr 11.3 Baby Names Data Set 11.4 %&gt;% Operator 11.5 filter() 11.6 arrange() 11.7 rename() 11.8 select() 11.9 mutate() 11.10 distinct() 11.11 summarize() 11.12 group_by() 11.13 Chaining Verbs Together", " 11 Transforming Data 11.1 dplyr Package dplyr is a package with the following description: A fast, consistent tool for working with data frame like objects, both in memory and out of memory. This package offers a “grammar” for manipulating data frames. Everything that dplyr does can also be done using basic R commands – however, it tends to be much faster and easier to use dplyr. 11.2 Grammar of dplyr Verbs: filter: extract a subset of rows from a data frame based on logical conditions arrange: reorder rows of a data frame rename: rename variables in a data frame select: return a subset of the columns of a data frame, using a flexible notation mutate: add new variables/columns or transform existing variables distinct: returns only the unique values in a table summarize: generate summary statistics of different variables in the data frame, possibly within strata group_by: breaks down a dataset into specified groups of rows Partially based on R Programming for Data Science 11.3 Baby Names Data Set &gt; library(&quot;dplyr&quot;, verbose=FALSE) &gt; library(&quot;babynames&quot;) &gt; ls() character(0) &gt; babynames &lt;- as_tibble(babynames::babynames) &gt; ls() [1] &quot;babynames&quot; The babynames Object &gt; class(babynames) [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; &gt; dim(babynames) [1] 1924665 5 &gt; babynames # A tibble: 1,924,665 x 5 year sex name n prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 1880 F Mary 7065 0.0724 2 1880 F Anna 2604 0.0267 3 1880 F Emma 2003 0.0205 4 1880 F Elizabeth 1939 0.0199 5 1880 F Minnie 1746 0.0179 6 1880 F Margaret 1578 0.0162 7 1880 F Ida 1472 0.0151 8 1880 F Alice 1414 0.0145 9 1880 F Bertha 1320 0.0135 10 1880 F Sarah 1288 0.0132 # … with 1,924,655 more rows Peek at the Data &gt; set.seed(201) &gt; sample_n(babynames, 10) # A tibble: 10 x 5 year sex name n prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 1994 F Avigayil 10 0.00000513 2 1934 M Durand 5 0.00000471 3 1970 F Starlette 8 0.00000437 4 1906 M Joy 7 0.0000486 5 1995 F Markela 7 0.00000364 6 1929 F Forrest 17 0.0000147 7 1909 F Levada 8 0.0000217 8 1970 M Drew 405 0.000213 9 1922 F Myrtle 3649 0.00292 10 1958 F Tayna 9 0.00000436 &gt; ## try also sample_frac(babynames, 6e-6) 11.4 %&gt;% Operator Originally from R package magrittr. Provides a mechanism for chaining commands with a forward-pipe operator, %&gt;%. &gt; x &lt;- 1:10 &gt; &gt; x %&gt;% log(base=10) %&gt;% sum() [1] 6.559763 &gt; &gt; sum(log(x,base=10)) [1] 6.559763 &gt; babynames %&gt;% sample_n(5) # A tibble: 5 x 5 year sex name n prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 1982 F Jewell 51 0.0000281 2 1999 F Precious 687 0.000353 3 1993 M Albin 14 0.00000678 4 1982 F Georgeanna 14 0.00000772 5 1987 F Danyale 29 0.0000155 11.5 filter() &gt; filter(babynames, year==1880, sex==&quot;F&quot;) # A tibble: 942 x 5 year sex name n prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 1880 F Mary 7065 0.0724 2 1880 F Anna 2604 0.0267 3 1880 F Emma 2003 0.0205 4 1880 F Elizabeth 1939 0.0199 5 1880 F Minnie 1746 0.0179 6 1880 F Margaret 1578 0.0162 7 1880 F Ida 1472 0.0151 8 1880 F Alice 1414 0.0145 9 1880 F Bertha 1320 0.0135 10 1880 F Sarah 1288 0.0132 # … with 932 more rows &gt; ## same as filter(babynames, year==1880 &amp; sex==&quot;F&quot;) &gt; filter(babynames, year==1880, sex==&quot;F&quot;, n &gt; 5000) # A tibble: 1 x 5 year sex name n prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 1880 F Mary 7065 0.0724 11.6 arrange() &gt; arrange(babynames, name, year, sex) # A tibble: 1,924,665 x 5 year sex name n prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 2007 M Aaban 5 0.00000226 2 2009 M Aaban 6 0.00000283 3 2010 M Aaban 9 0.00000439 4 2011 M Aaban 11 0.00000542 5 2012 M Aaban 11 0.00000543 6 2013 M Aaban 14 0.00000694 7 2014 M Aaban 16 0.00000783 8 2015 M Aaban 15 0.00000736 9 2016 M Aaban 9 0.00000446 10 2017 M Aaban 11 0.0000056 # … with 1,924,655 more rows &gt; arrange(babynames, desc(name), desc(year), sex) # A tibble: 1,924,665 x 5 year sex name n prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 2010 M Zzyzx 5 0.00000244 2 2014 M Zyyon 6 0.00000293 3 2010 F Zyyanna 6 0.00000306 4 2015 M Zyvon 7 0.00000343 5 2009 M Zyvion 5 0.00000236 6 2017 F Zyva 9 0.0000048 7 2016 F Zyva 8 0.00000415 8 2015 M Zyus 5 0.00000245 9 2010 M Zytavious 6 0.00000292 10 2009 M Zytavious 7 0.0000033 # … with 1,924,655 more rows 11.7 rename() &gt; rename(babynames, number=n) # A tibble: 1,924,665 x 5 year sex name number prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 1880 F Mary 7065 0.0724 2 1880 F Anna 2604 0.0267 3 1880 F Emma 2003 0.0205 4 1880 F Elizabeth 1939 0.0199 5 1880 F Minnie 1746 0.0179 6 1880 F Margaret 1578 0.0162 7 1880 F Ida 1472 0.0151 8 1880 F Alice 1414 0.0145 9 1880 F Bertha 1320 0.0135 10 1880 F Sarah 1288 0.0132 # … with 1,924,655 more rows 11.8 select() &gt; select(babynames, sex, name, n) # A tibble: 1,924,665 x 3 sex name n &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 F Mary 7065 2 F Anna 2604 3 F Emma 2003 4 F Elizabeth 1939 5 F Minnie 1746 6 F Margaret 1578 7 F Ida 1472 8 F Alice 1414 9 F Bertha 1320 10 F Sarah 1288 # … with 1,924,655 more rows &gt; ## same as select(babynames, sex:n) Renaming with select(): &gt; select(babynames, sex, name, number=n) # A tibble: 1,924,665 x 3 sex name number &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 F Mary 7065 2 F Anna 2604 3 F Emma 2003 4 F Elizabeth 1939 5 F Minnie 1746 6 F Margaret 1578 7 F Ida 1472 8 F Alice 1414 9 F Bertha 1320 10 F Sarah 1288 # … with 1,924,655 more rows 11.9 mutate() &gt; mutate(babynames, total_by_year=round(n/prop)) # A tibble: 1,924,665 x 6 year sex name n prop total_by_year &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1880 F Mary 7065 0.0724 97605 2 1880 F Anna 2604 0.0267 97605 3 1880 F Emma 2003 0.0205 97605 4 1880 F Elizabeth 1939 0.0199 97605 5 1880 F Minnie 1746 0.0179 97605 6 1880 F Margaret 1578 0.0162 97605 7 1880 F Ida 1472 0.0151 97605 8 1880 F Alice 1414 0.0145 97605 9 1880 F Bertha 1320 0.0135 97605 10 1880 F Sarah 1288 0.0132 97605 # … with 1,924,655 more rows &gt; ## see also transmutate 11.10 distinct() Let’s put a few things together now adding the function distinct()… &gt; babynames %&gt;% mutate(total_by_year=round(n/prop)) %&gt;% + select(sex, year, total_by_year) %&gt;% distinct() # A tibble: 36,099 x 3 sex year total_by_year &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 F 1880 97605 2 F 1880 97604 3 F 1880 97606 4 F 1880 97603 5 F 1880 97607 6 F 1880 97602 7 F 1880 97609 8 F 1880 97599 9 M 1880 118400 10 M 1880 118399 # … with 36,089 more rows 11.11 summarize() &gt; summarize(babynames, mean_n = mean(n), median_n = median(n), + number_sex = n_distinct(sex), + distinct_names = n_distinct(name)) # A tibble: 1 x 4 mean_n median_n number_sex distinct_names &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 181. 12 2 97310 11.12 group_by() &gt; babynames %&gt;% group_by(year, sex) # A tibble: 1,924,665 x 5 # Groups: year, sex [276] year sex name n prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 1880 F Mary 7065 0.0724 2 1880 F Anna 2604 0.0267 3 1880 F Emma 2003 0.0205 4 1880 F Elizabeth 1939 0.0199 5 1880 F Minnie 1746 0.0179 6 1880 F Margaret 1578 0.0162 7 1880 F Ida 1472 0.0151 8 1880 F Alice 1414 0.0145 9 1880 F Bertha 1320 0.0135 10 1880 F Sarah 1288 0.0132 # … with 1,924,655 more rows 11.13 Chaining Verbs Together No. Individuals by Year and Sex &gt; babynames %&gt;% group_by(year, sex) %&gt;% + summarize(total_by_year=sum(n)) # A tibble: 276 x 3 # Groups: year [?] year sex total_by_year &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; 1 1880 F 90993 2 1880 M 110491 3 1881 F 91953 4 1881 M 100743 5 1882 F 107847 6 1882 M 113686 7 1883 F 112319 8 1883 M 104627 9 1884 F 129020 10 1884 M 114442 # … with 266 more rows How Many Distinct Names? &gt; babynames %&gt;% group_by(sex) %&gt;% + summarize(mean_n = mean(n), + distinct_names_sex = n_distinct(name)) # A tibble: 2 x 3 sex mean_n distinct_names_sex &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; 1 F 151. 67046 2 M 223. 40927 Most Popular Names by Year &gt; top_names &lt;- babynames %&gt;% group_by(year, sex) %&gt;% + summarize(top_name = name[which.max(n)]) &gt; &gt; head(top_names) # A tibble: 6 x 3 # Groups: year [3] year sex top_name &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1880 F Mary 2 1880 M John 3 1881 F Mary 4 1881 M John 5 1882 F Mary 6 1882 M John Most Popular Names in Recent Years &gt; tail(top_names, n=10) # A tibble: 10 x 3 # Groups: year [5] year sex top_name &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 2013 F Sophia 2 2013 M Noah 3 2014 F Emma 4 2014 M Noah 5 2015 F Emma 6 2015 M Noah 7 2016 F Emma 8 2016 M Noah 9 2017 F Emma 10 2017 M Liam Most Popular Female Names in the 1990s &gt; top_names %&gt;% filter(year &gt;= 1990 &amp; year &lt; 2000, sex==&quot;F&quot;) # A tibble: 10 x 3 # Groups: year [10] year sex top_name &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1990 F Jessica 2 1991 F Ashley 3 1992 F Ashley 4 1993 F Jessica 5 1994 F Jessica 6 1995 F Jessica 7 1996 F Emily 8 1997 F Emily 9 1998 F Emily 10 1999 F Emily Most Popular Male Names in the 1990s &gt; top_names %&gt;% filter(year &gt;= 1990 &amp; year &lt; 2000, sex==&quot;M&quot;) # A tibble: 10 x 3 # Groups: year [10] year sex top_name &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1990 M Michael 2 1991 M Michael 3 1992 M Michael 4 1993 M Michael 5 1994 M Michael 6 1995 M Michael 7 1996 M Michael 8 1997 M Michael 9 1998 M Michael 10 1999 M Jacob Analyzing the name ‘John’ &gt; john &lt;- babynames %&gt;% filter(sex==&quot;M&quot;, name==&quot;John&quot;) &gt; plot(john$year, john$prop, type=&quot;l&quot;) Analyzing the name ‘Bella’ &gt; bella &lt;- babynames %&gt;% filter(sex==&quot;F&quot;, name==&quot;Bella&quot;) &gt; plot(bella$year, bella$prop, type=&quot;l&quot;) "],
["relational-data.html", "12 Relational Data 12.1 Multiple Data Sets 12.2 Toy Example 12.3 Verbs 12.4 inner_join() 12.5 left_join() 12.6 right_join() 12.7 full_join() 12.8 anti_join() 12.9 semi_join() 12.10 Repeated Key Values 12.11 Set Operations", " 12 Relational Data 12.1 Multiple Data Sets In many data analyses you will have multiple tables of related data that must be combined in order to carry out your analysis. The dplyr package includes a number of tools to facilitate this. 12.2 Toy Example Here are two data frames that are related through a common variable called key. &gt; x &lt;- tibble(key = c(1, 2, 3), x_val = c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)) &gt; y &lt;- tibble(key = c(1, 2, 4), y_val = c(&quot;y1&quot;, &quot;y2&quot;, &quot;y4&quot;)) &gt; x # A tibble: 3 x 2 key x_val &lt;dbl&gt; &lt;chr&gt; 1 1 x1 2 2 x2 3 3 x3 &gt; y # A tibble: 3 x 2 key y_val &lt;dbl&gt; &lt;chr&gt; 1 1 y1 2 2 y2 3 4 y4 12.3 Verbs To work with relational data you need verbs that work with pairs of tables. There are three families of verbs designed to work with relational data. Mutating joins add new variables to one data frame from matching observations in another. Filtering joins filter observations from one data frame based on whether or not they match an observation in the other table. Set operations treat observations as if they were set elements. From R for Data Science 12.4 inner_join() An inner-join matches pairs of observations when their keys are equal. &gt; inner_join(x, y, key=&quot;key&quot;) Joining, by = &quot;key&quot; # A tibble: 2 x 3 key x_val y_val &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1 x1 y1 2 2 x2 y2 12.5 left_join() A left-join keeps all observations in the first argument, x. &gt; left_join(x, y, key=&quot;key&quot;) Joining, by = &quot;key&quot; # A tibble: 3 x 3 key x_val y_val &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1 x1 y1 2 2 x2 y2 3 3 x3 &lt;NA&gt; &gt; x %&gt;% left_join(y, key=&quot;key&quot;) Joining, by = &quot;key&quot; # A tibble: 3 x 3 key x_val y_val &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1 x1 y1 2 2 x2 y2 3 3 x3 &lt;NA&gt; 12.6 right_join() A right-join keeps all observations in the second argument, y. &gt; right_join(x, y) Joining, by = &quot;key&quot; # A tibble: 3 x 3 key x_val y_val &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1 x1 y1 2 2 x2 y2 3 4 &lt;NA&gt; y4 12.7 full_join() A full-join keeps all observations in either argument, x or y. &gt; full_join(x, y, key=&quot;key&quot;) Joining, by = &quot;key&quot; # A tibble: 4 x 3 key x_val y_val &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1 x1 y1 2 2 x2 y2 3 3 x3 &lt;NA&gt; 4 4 &lt;NA&gt; y4 12.8 anti_join() An anti-join removes all observations in the first argument, x, that appear in the second argument, y. &gt; anti_join(x, y, key=&quot;key&quot;) Joining, by = &quot;key&quot; # A tibble: 1 x 2 key x_val &lt;dbl&gt; &lt;chr&gt; 1 3 x3 12.9 semi_join() A semi-join keeps all observations in the first argument, x, that have a match in the second argument, y. &gt; semi_join(x, y, key=&quot;key&quot;) Joining, by = &quot;key&quot; # A tibble: 2 x 2 key x_val &lt;dbl&gt; &lt;chr&gt; 1 1 x1 2 2 x2 12.10 Repeated Key Values When one of the two data frames has repeated key values, the observations are repeated in the other data frame. &gt; y2 # A tibble: 4 x 2 key y_val &lt;dbl&gt; &lt;chr&gt; 1 1 y1 2 2 y2a 3 2 y2b 4 4 y4 &gt; x %&gt;% left_join(y2, key=&quot;key&quot;) Joining, by = &quot;key&quot; # A tibble: 4 x 3 key x_val y_val &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1 x1 y1 2 2 x2 y2a 3 2 x2 y2b 4 3 x3 &lt;NA&gt; 12.11 Set Operations One can perform traditional set operations on the rows of data frames. intersect(x, y): return only observations in both x and y union(x, y): return unique observations in x and y setdiff(x, y): return observations in x, but not in y From R for Data Science Example setdiff() &gt; df1 # A tibble: 2 x 2 x y &lt;dbl&gt; &lt;dbl&gt; 1 1 1 2 2 1 &gt; df2 # A tibble: 2 x 2 x y &lt;dbl&gt; &lt;dbl&gt; 1 1 1 2 1 2 &gt; setdiff(df1, df2) # A tibble: 1 x 2 x y &lt;dbl&gt; &lt;dbl&gt; 1 2 1 "],
["case-study-in-data-wrangling.html", "13 Case Study in Data Wrangling 13.1 Yeast Genomics", " 13 Case Study in Data Wrangling 13.1 Yeast Genomics Smith and Kruglyak (2008) is a study that measured 2820 genotypes in 109 yeast F1 segregants from a cross between parental lines BY and RM. They also measured gene expression on 4482 genes in each of these segregants when growing in two different Carbon sources, glucose and ethanol. 13.1.1 Load Data The data was distributed as a collection of matrices in R. &gt; rm(list=ls()) &gt; load(&quot;./data/smith_kruglyak.RData&quot;) &gt; ls() [1] &quot;exp.e&quot; &quot;exp.g&quot; &quot;exp.pos&quot; &quot;marker&quot; &quot;marker.pos&quot; &gt; eapply(env=.GlobalEnv, dim) $exp.e [1] 4482 109 $exp.g [1] 4482 109 $marker [1] 2820 109 $exp.pos [1] 4482 3 $marker.pos [1] 2820 2 13.1.2 Gene Expression Matrices &gt; exp.g %&gt;% cbind(rownames(exp.g), .) %&gt;% as_tibble() %&gt;% + print() Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`. This warning is displayed once per session. # A tibble: 4,482 x 110 V1 X100g.20_4_c.gl… X101g.21_1_d.gl… X102g.21_2_d.gl… &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 YJR1… 0.22 0.18 0.05 2 YPL2… -0.29 -0.2 -0.19 3 YDR5… 0.72 0.04 0.26 4 YDR2… 0.23 0.31 0.12 5 YHR0… 0.4 -0.04 0.36 6 YFR0… -0.36 0.35 -0.26 7 YPL1… 0.23 -0.21 -0.25 8 YDR0… -0.09 0.57 0.24 9 YLR3… -0.23 0.13 -0.17 10 YCR0… -0.25 -0.98 -0.3 # … with 4,472 more rows, and 106 more variables: # X103g.21_3_d.glucose &lt;chr&gt;, X104g.21_4_d.glucose &lt;chr&gt;, # X105g.21_5_c.glucose &lt;chr&gt;, X106g.22_2_d.glucose &lt;chr&gt;, # X107g.22_3_b.glucose &lt;chr&gt;, X109g.22_5_d.glucose &lt;chr&gt;, # X10g.2_5_d.glucose &lt;chr&gt;, X110g.23_3_d.glucose &lt;chr&gt;, # X111g.23_5_d.glucose &lt;chr&gt;, X112g.24_1_d.glucose &lt;chr&gt;, # X113g.25_1_d.glucose &lt;chr&gt;, X114g.25_3_d.glucose &lt;chr&gt;, # X115g.25_4_d.glucose &lt;chr&gt;, X116g.26_1_d.glucose &lt;chr&gt;, # X117g.26_2_d.glucose &lt;chr&gt;, X11g.2_6_d.glucose &lt;chr&gt;, # X12g.2_7_a.glucose &lt;chr&gt;, X13g.3_1_d.glucose &lt;chr&gt;, # X15g.3_3_d.glucose &lt;chr&gt;, X16g.3_4_d.glucose &lt;chr&gt;, # X17g.3_5_d.glucose &lt;chr&gt;, X18g.4_1_c.glucose &lt;chr&gt;, # X1g.1_1_d.glucose &lt;chr&gt;, X20g.4_3_d.glucose &lt;chr&gt;, # X21g.4_4_d.glucose &lt;chr&gt;, X22g.5_1_d.glucose &lt;chr&gt;, # X23g.5_2_d.glucose &lt;chr&gt;, X24g.5_3_d.glucose &lt;chr&gt;, # X25g.5_4_d.glucose &lt;chr&gt;, X26g.5_5_d.glucose &lt;chr&gt;, # X27g.6_1_d.glucose &lt;chr&gt;, X28g.6_2_b.glucose &lt;chr&gt;, # X29g.6_3_c.glucose &lt;chr&gt;, X30g.6_4_d.glucose &lt;chr&gt;, # X31g.6_5_d.glucose &lt;chr&gt;, X32g.6_6_d.glucose &lt;chr&gt;, # X33g.6_7_d.glucose &lt;chr&gt;, X34g.7_1_d.glucose &lt;chr&gt;, # X35g.7_2_c.glucose &lt;chr&gt;, X36g.7_3_d.glucose &lt;chr&gt;, # X37g.7_4_c.glucose &lt;chr&gt;, X38g.7_5_d.glucose &lt;chr&gt;, # X39g.7_6_c.glucose &lt;chr&gt;, X3g.1_3_d.glucose &lt;chr&gt;, # X40g.7_7_c.glucose &lt;chr&gt;, X41g.7_8_d.glucose &lt;chr&gt;, # X42g.8_1_a.glucose &lt;chr&gt;, X43g.8_2_d.glucose &lt;chr&gt;, # X44g.8_3_a.glucose &lt;chr&gt;, X45g.8_4_c.glucose &lt;chr&gt;, # X46g.8_5_b.glucose &lt;chr&gt;, X47g.8_6_c.glucose &lt;chr&gt;, # X48g.8_7_b.glucose &lt;chr&gt;, X49g.9_1_d.glucose &lt;chr&gt;, # X4g.1_4_d.glucose &lt;chr&gt;, X50g.9_2_d.glucose &lt;chr&gt;, # X51g.9_3_d.glucose &lt;chr&gt;, X52g.9_4_d.glucose &lt;chr&gt;, # X53g.9_5_d.glucose &lt;chr&gt;, X54g.9_6_d.glucose &lt;chr&gt;, # X55g.9_7_d.glucose &lt;chr&gt;, X56g.10_1_c.glucose &lt;chr&gt;, # X57g.10_2_d.glucose &lt;chr&gt;, X58g.10_3_c.glucose &lt;chr&gt;, # X59g.10_4_d.glucose &lt;chr&gt;, X5g.1_5_c.glucose &lt;chr&gt;, # X60g.11_1_a.glucose &lt;chr&gt;, X61g.11_2_d.glucose &lt;chr&gt;, # X62g.11_3_b.glucose &lt;chr&gt;, X63g.12_1_d.glucose &lt;chr&gt;, # X64g.12_2_b.glucose &lt;chr&gt;, X65g.13_1_a.glucose &lt;chr&gt;, # X66g.13_2_c.glucose &lt;chr&gt;, X67g.13_3_b.glucose &lt;chr&gt;, # X68g.13_4_a.glucose &lt;chr&gt;, X69g.13_5_c.glucose &lt;chr&gt;, # X70g.14_1_b.glucose &lt;chr&gt;, X71g.14_2_c.glucose &lt;chr&gt;, # X73g.14_4_a.glucose &lt;chr&gt;, X74g.14_5_b.glucose &lt;chr&gt;, # X75g.14_6_d.glucose &lt;chr&gt;, X76g.14_7_c.glucose &lt;chr&gt;, # X77g.15_2_d.glucose &lt;chr&gt;, X78g.15_3_b.glucose &lt;chr&gt;, # X79g.15_4_d.glucose &lt;chr&gt;, X7g.2_2_d.glucose &lt;chr&gt;, # X80g.15_5_b.glucose &lt;chr&gt;, X82g.16_1_d.glucose &lt;chr&gt;, # X83g.17_1_a.glucose &lt;chr&gt;, X84g.17_2_d.glucose &lt;chr&gt;, # X85g.17_4_a.glucose &lt;chr&gt;, X86g.17_5_b.glucose &lt;chr&gt;, # X87g.18_1_d.glucose &lt;chr&gt;, X88g.18_2_d.glucose &lt;chr&gt;, # X89g.18_3_d.glucose &lt;chr&gt;, X8g.2_3_d.glucose &lt;chr&gt;, # X90g.18_4_c.glucose &lt;chr&gt;, X92g.19_1_c.glucose &lt;chr&gt;, # X93g.19_2_c.glucose &lt;chr&gt;, X94g.19_3_c.glucose &lt;chr&gt;, … 13.1.3 Gene Position Matrix &gt; exp.pos %&gt;% cbind(rownames(exp.pos), .) %&gt;% as_tibble() %&gt;% + print() # A tibble: 4,482 x 4 V1 Chromsome Start_coord End_coord &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 YJR107W 10 627333 628319 2 YPL270W 16 30482 32803 3 YDR518W 4 1478600 1480153 4 YDR233C 4 930353 929466 5 YHR098C 8 301937 299148 6 YFR029W 6 210925 212961 7 YPL198W 16 173151 174701 8 YDR001C 4 452472 450217 9 YLR394W 12 907950 909398 10 YCR079W 3 252842 254170 # … with 4,472 more rows 13.1.4 Row Names The gene names are contained in the row names. &gt; head(rownames(exp.g)) [1] &quot;YJR107W&quot; &quot;YPL270W&quot; &quot;YDR518W&quot; &quot;YDR233C&quot; &quot;YHR098C&quot; &quot;YFR029W&quot; &gt; head(rownames(exp.e)) [1] &quot;YJR107W&quot; &quot;YPL270W&quot; &quot;YDR518W&quot; &quot;YDR233C&quot; &quot;YHR098C&quot; &quot;YFR029W&quot; &gt; head(rownames(exp.pos)) [1] &quot;YJR107W&quot; &quot;YPL270W&quot; &quot;YDR518W&quot; &quot;YDR233C&quot; &quot;YHR098C&quot; &quot;YFR029W&quot; &gt; all.equal(rownames(exp.g), rownames(exp.e)) [1] TRUE &gt; all.equal(rownames(exp.g), rownames(exp.pos)) [1] TRUE 13.1.5 Unify Column Names The segregants are column names, and they are inconsistent across matrices. &gt; head(colnames(exp.g)) [1] &quot;X100g.20_4_c.glucose&quot; &quot;X101g.21_1_d.glucose&quot; &quot;X102g.21_2_d.glucose&quot; [4] &quot;X103g.21_3_d.glucose&quot; &quot;X104g.21_4_d.glucose&quot; &quot;X105g.21_5_c.glucose&quot; &gt; head(colnames(marker)) [1] &quot;20_4_c&quot; &quot;21_1_d&quot; &quot;21_2_d&quot; &quot;21_3_d&quot; &quot;21_4_d&quot; &quot;21_5_c&quot; &gt; &gt; ##fix column names with gsub &gt; colnames(exp.g) %&lt;&gt;% strsplit(split=&quot;.&quot;, fixed=TRUE) %&gt;% + lapply(function(x) {x[2]}) &gt; colnames(exp.e) %&lt;&gt;% strsplit(split=&quot;.&quot;, fixed=TRUE) %&gt;% + lapply(function(x) {x[2]}) &gt; head(colnames(exp.g)) [1] &quot;20_4_c&quot; &quot;21_1_d&quot; &quot;21_2_d&quot; &quot;21_3_d&quot; &quot;21_4_d&quot; &quot;21_5_c&quot; 13.1.6 Gene Positions Let’s first pull out rownames of exp.pos and make them a column in the data frame. &gt; gene_pos &lt;- exp.pos %&gt;% as_tibble() %&gt;% + mutate(gene = rownames(exp.pos)) %&gt;% + dplyr::select(gene, chr = Chromsome, start = Start_coord, + end = End_coord) &gt; print(gene_pos, n=7) # A tibble: 4,482 x 4 gene chr start end &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 YJR107W 10 627333 628319 2 YPL270W 16 30482 32803 3 YDR518W 4 1478600 1480153 4 YDR233C 4 930353 929466 5 YHR098C 8 301937 299148 6 YFR029W 6 210925 212961 7 YPL198W 16 173151 174701 # … with 4,475 more rows 13.1.7 Tidy Each Expression Matrix We melt the expression matrices and bind them together into one big tidy data frame. &gt; exp_g &lt;- melt(exp.g) %&gt;% as_tibble() %&gt;% + dplyr::select(gene = Var1, segregant = Var2, + expression = value) %&gt;% + mutate(condition = &quot;glucose&quot;) &gt; exp_e &lt;- melt(exp.e) %&gt;% as_tibble() %&gt;% + dplyr::select(gene = Var1, segregant = Var2, + expression = value) %&gt;% + mutate(condition = &quot;ethanol&quot;) &gt; print(exp_e, n=4) # A tibble: 488,538 x 4 gene segregant expression condition &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; 1 YJR107W 20_4_c 0.06 ethanol 2 YPL270W 20_4_c -0.13 ethanol 3 YDR518W 20_4_c -0.94 ethanol 4 YDR233C 20_4_c 0.04 ethanol # … with 4.885e+05 more rows 13.1.8 Combine Into Single Data Frame Combine gene expression data from two conditions into a single data frame. &gt; exp_all &lt;- bind_rows(exp_g, exp_e) &gt; sample_n(exp_all, size=10) # A tibble: 10 x 4 gene segregant expression condition &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; 1 YBL087C 21_4_d -0.72 ethanol 2 YDR524C 21_2_d -0.17 glucose 3 YGR067C 9_1_d -3.92 glucose 4 YHR207C 26_1_d -0.43 ethanol 5 YDR329C 20_2_d -0.06 glucose 6 YGL121C 8_7_b 1 ethanol 7 YJR044C 3_3_d -0.12 ethanol 8 YIL088C 2_7_a 0.1 ethanol 9 YML127W 5_1_d -0.08 ethanol 10 YMR304W 6_1_d 0.2 ethanol 13.1.9 Join Gene Positions Now we want to join the gene positions with the expression data. &gt; exp_all &lt;- exp_all %&gt;% + mutate(gene = as.character(gene), + segregant = as.character(segregant)) &gt; sk_tidy &lt;- exp_all %&gt;% + left_join(gene_pos, by = &quot;gene&quot;) &gt; sample_n(sk_tidy, size=7) # A tibble: 7 x 7 gene segregant expression condition chr start end &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 YGL189C 1_3_d -0.26 ethanol 7 148594 148235 2 YBR257W 13_2_c 0.02 ethanol 2 728880 729719 3 YER098W 21_1_d 0.46 ethanol 5 355462 357726 4 YCR035C 9_1_d 0.07 glucose 3 193014 191830 5 YBR097W 17_5_b -0.03 glucose 2 436945 441309 6 YBR235W 8_4_c -0.18 ethanol 2 686896 690258 7 YJL094C 14_6_d 0 glucose 10 254437 251816 13.1.10 Apply dplyr Functions Now that we have the data made tidy in the data frame sk_tidy, let’s apply some dplyr operations… Does each gene have the same number of observations? &gt; sk_tidy %&gt;% group_by(gene) %&gt;% + summarize(value = n()) %&gt;% + summary() gene value Length:4478 Min. :218.0 Class :character 1st Qu.:218.0 Mode :character Median :218.0 Mean :218.6 3rd Qu.:218.0 Max. :872.0 No, so let’s see which genes have more than one set of observations. &gt; sk_tidy %&gt;% group_by(gene) %&gt;% + summarize(value = n()) %&gt;% + filter(value &gt; median(value)) # A tibble: 4 x 2 gene value &lt;chr&gt; &lt;int&gt; 1 YFR024C-A 872 2 YJL012C 872 3 YKL198C 872 4 YPR089W 872 Let’s remove replicated measurements for these genes. &gt; sk_tidy %&lt;&gt;% distinct(gene, segregant, condition, + .keep_all = TRUE) &gt; &gt; sk_tidy %&gt;% group_by(gene) %&gt;% + summarize(value = n()) %&gt;% + summary() gene value Length:4478 Min. :218 Class :character 1st Qu.:218 Mode :character Median :218 Mean :218 3rd Qu.:218 Max. :218 As an exercise, think about how you would use dplyr to replace the replicated gene expression values with a single averaged expression value for these genes. Get the mean and standard deviation expression per chromosome. &gt; sk_tidy %&gt;% + group_by(chr) %&gt;% + summarize(mean = mean(expression), sd=sd(expression)) # A tibble: 16 x 3 chr mean sd &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 -0.0762 0.826 2 2 -0.0447 0.632 3 3 -0.0230 0.682 4 4 -0.0233 0.537 5 5 -0.0579 0.610 6 6 -0.0772 0.660 7 7 -0.0441 0.617 8 8 -0.0474 0.638 9 9 -0.0430 0.614 10 10 -0.0299 0.570 11 11 -0.0396 0.613 12 12 -0.0515 0.643 13 13 -0.0265 0.584 14 14 -0.0294 0.642 15 15 -0.0130 0.554 16 16 -0.0368 0.604 Get the mean and standard deviation expression per chromosome in each condition. &gt; sk_tidy %&gt;% + group_by(chr, condition) %&gt;% + summarize(mean = mean(expression), sd=sd(expression)) # A tibble: 32 x 4 # Groups: chr [?] chr condition mean sd &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 ethanol 0.0260 0.480 2 1 glucose -0.178 1.05 3 2 ethanol 0.0132 0.479 4 2 glucose -0.103 0.750 5 3 ethanol 0.000164 0.536 6 3 glucose -0.0461 0.800 7 4 ethanol 0.00187 0.482 8 4 glucose -0.0484 0.586 9 5 ethanol -0.0297 0.479 10 5 glucose -0.0862 0.716 # … with 22 more rows Count the number of genes per chromosome. &gt; sk_tidy %&gt;% + filter(condition == &quot;glucose&quot;, segregant == &quot;20_4_c&quot;) %&gt;% + group_by(chr) %&gt;% + summarize(num.genes = n()) # A tibble: 16 x 2 chr num.genes &lt;int&gt; &lt;int&gt; 1 1 60 2 2 298 3 3 125 4 4 629 5 5 207 6 6 79 7 7 395 8 8 209 9 9 152 10 10 256 11 11 241 12 12 387 13 13 367 14 14 319 15 15 388 16 16 366 Filter for the first gene on every chromosome. &gt; sk_tidy %&gt;% + filter(condition == &quot;glucose&quot;, segregant == &quot;20_4_c&quot;) %&gt;% + group_by(chr) %&gt;% + filter(start == min(start)) # A tibble: 16 x 7 # Groups: chr [16] gene segregant expression condition chr start end &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 YHL040C 20_4_c -2.79 glucose 8 20968 19085 2 YNL334C 20_4_c -0.9 glucose 14 12876 12208 3 YOL157C 20_4_c -1.06 glucose 15 24293 22524 4 YKL222C 20_4_c 0.09 glucose 11 5621 3504 5 YIL168W 20_4_c -1.14 glucose 9 29032 29415 6 YJL213W 20_4_c 0.84 glucose 10 32163 33158 7 YPL272C 20_4_c -0.18 glucose 16 28164 26611 8 YLL063C 20_4_c -0.66 glucose 12 16072 14648 9 YFL048C 20_4_c -0.09 glucose 6 40180 38843 10 YML132W 20_4_c -0.21 glucose 13 7244 8383 11 YGL261C 20_4_c -0.14 glucose 7 6652 6290 12 YBL107C 20_4_c 0.290 glucose 2 10551 9961 13 YDL248W 20_4_c -0.68 glucose 4 1802 2953 14 YEL073C 20_4_c -0.02 glucose 5 7553 7230 15 YAL062W 20_4_c -5.64 glucose 1 31568 32941 16 YCL068C 20_4_c 0.47 glucose 3 12285 11503 To plot expression in glucose versus ethanol we first need to use dcast(). &gt; sk_tidy %&gt;% dcast(gene + segregant ~ condition, + value.var = &quot;expression&quot;) %&gt;% + as_tibble() # A tibble: 488,102 x 4 gene segregant ethanol glucose &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 YAL002W 1_1_d 0.37 -0.01 2 YAL002W 1_3_d 0.23 0.03 3 YAL002W 1_4_d 0.08 0.07 4 YAL002W 1_5_c -0.12 0.13 5 YAL002W 10_1_c 0.12 -0.1 6 YAL002W 10_2_d 0.1 -0.2 7 YAL002W 10_3_c 0.07 -0.15 8 YAL002W 10_4_d 0.06 -0.04 9 YAL002W 11_1_a 0.07 -0.07 10 YAL002W 11_2_d 0.3 0.1 # … with 488,092 more rows &gt; sk_tidy %&gt;% dcast(gene + segregant ~ condition, + value.var = &quot;expression&quot;) %&gt;% + filter(gene == &quot;YAL002W&quot;) %&gt;% + ggplot(aes(x = glucose, y = ethanol)) + + geom_point() + theme_bw() + + theme(legend.position = &quot;none&quot;) "],
["further-reading.html", "14 Further Reading 14.1 Additional Examples 14.2 Additional dplyr Features", " 14 Further Reading 14.1 Additional Examples You should study additional tutorials of dplyr that utilize other data sets: Read the dplyr introductory vignette Read the examples given in the R for Data Science assigned reading 14.2 Additional dplyr Features We’ve only scratched the surface – many interesting demos of dplyr can be found online dplyr can work with other data frame backends such as SQL databases There is an SQL interface for relational databases via the DBI package dplyr can be integrated with the data.table package for large fast tables There is a healthy rivalry between dplyr and data.table "],
["exploratory-data-analysis-1.html", "15 Exploratory Data Analysis 15.1 What is EDA? 15.2 Descriptive Statistics Examples 15.3 Components of EDA 15.4 Data Sets", " 15 Exploratory Data Analysis 15.1 What is EDA? Exploratory data analysis (EDA) is the process of analzying data to uncover their key features. John Tukey pioneered this framework, writing a seminal book on the topic (called Exploratory Data Analysis). EDA involves calculating numerical summaries of data, visualizing data in a variety of ways, and considering interesting data points. Before any model fitting is done to data, some exploratory data analysis should always be performed. Data science seems to focus much more on EDA than traditional statistics. 15.2 Descriptive Statistics Examples Facebook’s Visualizing Friendships (side note: a discussion) Hans Rosling: Debunking third-world myths with the best stats you’ve ever seen Flowing Data’s A Day in the Life of Americans 15.3 Components of EDA EDA involves calculating quantities and visualizing data for: Checking the n’s Checking for missing data Characterizing the distributional properties of the data Characterizing relationships among variables and observations Dimension reduction Model formulation Hypothesis generation … and there are possible many more activities one can do. 15.4 Data Sets For the majority of this chapter, we will use some simple data sets to demonstrate the ideas. 15.4.1 Data mtcars Load the mtcars data set: &gt; library(&quot;tidyverse&quot;) # why load tidyverse? &gt; data(&quot;mtcars&quot;, package=&quot;datasets&quot;) &gt; mtcars &lt;- as_tibble(mtcars) &gt; head(mtcars) # A tibble: 6 x 11 mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 15.4.2 Data mpg Load the mpg data set: &gt; data(&quot;mpg&quot;, package=&quot;ggplot2&quot;) &gt; head(mpg) # A tibble: 6 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 audi a4 1.8 1999 4 auto(… f 18 29 p comp… 2 audi a4 1.8 1999 4 manua… f 21 29 p comp… 3 audi a4 2 2008 4 manua… f 20 31 p comp… 4 audi a4 2 2008 4 auto(… f 21 30 p comp… 5 audi a4 2.8 1999 6 auto(… f 16 26 p comp… 6 audi a4 2.8 1999 6 manua… f 18 26 p comp… 15.4.3 Data diamonds Load the diamonds data set: &gt; data(&quot;diamonds&quot;, package=&quot;ggplot2&quot;) &gt; head(diamonds) # A tibble: 6 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 15.4.4 Data gapminder Load the gapminder data set: &gt; library(&quot;gapminder&quot;) &gt; data(&quot;gapminder&quot;, package=&quot;gapminder&quot;) &gt; gapminder &lt;- as_tibble(gapminder) &gt; head(gapminder) # A tibble: 6 x 6 country continent year lifeExp pop gdpPercap &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 2 Afghanistan Asia 1957 30.3 9240934 821. 3 Afghanistan Asia 1962 32.0 10267083 853. 4 Afghanistan Asia 1967 34.0 11537966 836. 5 Afghanistan Asia 1972 36.1 13079460 740. 6 Afghanistan Asia 1977 38.4 14880372 786. "],
["numerical-summaries-of-data.html", "16 Numerical Summaries of Data 16.1 Useful Summaries 16.2 Measures of Center 16.3 Mean, Median, and Mode in R 16.4 Quantiles and Percentiles 16.5 Five Number Summary 16.6 Measures of Spread 16.7 Variance, SD, and IQR in R 16.8 Identifying Outliers 16.9 Application to mtcars Data 16.10 Measuring Symmetry 16.11 skewness() Function 16.12 Measuring Tails 16.13 Excess Kurtosis 16.14 kurtosis() Function 16.15 Visualizing Skewness and Kurtosis 16.16 Covariance and Correlation", " 16 Numerical Summaries of Data 16.1 Useful Summaries Center: mean, median, mode Quantiles: percentiles, five number summaries Spread: standard deviation, variance, interquartile range Outliers Shape: skewness, kurtosis Concordance: correlation, quantile-quantile plots 16.2 Measures of Center Suppose we have data points \\(x_1, x_2, \\ldots, x_n\\). Mean: \\[\\overline{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\] Median: Order the points \\(x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}\\). The median is the middle value: - \\(x_{((n+1)/2)}\\) if \\(n\\) is odd - \\((x_{(n/2)} + x_{(n/2+1)})/2\\) if \\(n\\) is even Mode: The most frequently repeated value among the data (if any). If there are ties, then there is more than one mode. 16.3 Mean, Median, and Mode in R Let’s calculate these quantities in R. &gt; mean(mtcars$mpg) [1] 20.09062 &gt; median(mtcars$mpg) [1] 19.2 &gt; &gt; sample_mode &lt;- function(x) { + as.numeric(names(which(table(x) == max(table(x))))) + } &gt; &gt; sample_mode(round(mtcars$mpg)) [1] 15 21 It appears there is no R base function for calculating the mode. 16.4 Quantiles and Percentiles The \\(p\\)th percentile of \\(x_1, x_2, \\ldots, x_n\\) is a number such that \\(p\\)% of the data are less than this number. The 25th, 50th, and 75th percentiles are called 1st, 2nd, and 3rd “quartiles”, respectively. These are sometimes denoted as Q1, Q2, and Q3. The median is the 50th percentile aka the 2nd quartile aka Q2. In general, \\(q\\)-quantiles are cut points that divide the data into \\(q\\) approximately equally sized groups. The cut points are the percentiles \\(1/q, 2/q, \\ldots, (q-1)/q.\\) 16.5 Five Number Summary The “five number summary” is the minimum, the three quartiles, and the maximum. This can be calculated via fivenum() and summary(). They can produce different values. Finally, quantile() extracts any set of percentiles. &gt; fivenum(mtcars$mpg) [1] 10.40 15.35 19.20 22.80 33.90 &gt; summary(mtcars$mpg) Min. 1st Qu. Median Mean 3rd Qu. Max. 10.40 15.43 19.20 20.09 22.80 33.90 &gt; &gt; quantile(mtcars$mpg, prob=seq(0, 1, 0.25)) 0% 25% 50% 75% 100% 10.400 15.425 19.200 22.800 33.900 16.6 Measures of Spread The variance, standard deviation (SD), and interquartile range (IQR) measure the “spread” of the data. Variance: \\[s^2 = \\frac{\\sum_{i=1}^n \\left(x_i - \\overline{x}\\right)^2}{n-1}\\] Standard Deviation: \\(s = \\sqrt{s^2}\\) Iterquartile Range: IQR \\(=\\) Q3 \\(-\\) Q1 The SD and IQR have the same units as the observed data, but the variance is in squared units. 16.7 Variance, SD, and IQR in R Variance: &gt; var(mtcars$mpg) [1] 36.3241 Standard deviation: &gt; sd(mtcars$mpg) [1] 6.026948 Interquartile range: &gt; IQR(mtcars$mpg) [1] 7.375 &gt; diff(fivenum(mtcars$mpg)[c(2,4)]) [1] 7.45 16.8 Identifying Outliers An outlier is an unusual data point. Outliers can be perfectly valid but they can also be due to errors (as can non-outliers). One must define what is meant by an outlier. One definition is a data point that less than Q1 or greater than Q3 by 1.5 \\(\\times\\) IQR or more. Another definition is a data point whose difference from the mean is greater than 3 \\(\\times\\) SD or more. For Normal distributed data (bell curve shaped), the probability of this is less than 0.27%. 16.9 Application to mtcars Data &gt; sd_units &lt;- abs(mtcars$wt - mean(mtcars$wt))/sd(mtcars$wt) &gt; sum(sd_units &gt; 3) [1] 0 &gt; max(sd_units) [1] 2.255336 &gt; &gt; iqr_outlier_cuts &lt;- fivenum(mtcars$wt)[c(2,4)] + + c(-1.5, 1.5)*diff(fivenum(mtcars$wt)[c(2,4)]) &gt; sum(mtcars$wt &lt; iqr_outlier_cuts[1] | + mtcars$wt &gt; iqr_outlier_cuts[2]) [1] 2 16.10 Measuring Symmetry The skewness statistic measures symmetry of the data. It is calculated by: \\[ \\gamma = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^3/n}{s^3} \\] A negative number is left-skewed, and a positive number is right-skewed. Note: Use of \\(n\\) vs. \\(n-1\\) may vary – check the code. 16.11 skewness() Function In R, there is a function call skewness() from the moments package for calculating this statistic on data. &gt; library(moments) &gt; gapminder %&gt;% filter(year==2007) %&gt;% select(gdpPercap) %&gt;% + skewness() gdpPercap 1.211228 &gt; gapminder %&gt;% filter(year==2007) %&gt;% select(gdpPercap) %&gt;% + log() %&gt;% skewness() gdpPercap -0.1524203 &gt; rnorm(10000) %&gt;% skewness() [1] 0.005799917 16.12 Measuring Tails The tails of a distribution are often described as being heavy or light depending on how slowly they descend. This can be measured through statistic called kurtosis: \\[ \\kappa = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^4/n}{s^4} \\] As with skewness \\(\\gamma\\), use of \\(n\\) vs \\(n-1\\) may vary. 16.13 Excess Kurtosis For a standard Normal distribution (mean 0 and standard deviation 1), the kurtosis is on average 3. Therefore, a measure called “excess kurtosis” is defined to be \\(\\kappa - 3\\). A positive value implies heavier tails and a negative value implies lighter tails. 16.14 kurtosis() Function In R, there is a function call kurtosis() from the moments package for calculating this statistic on data. &gt; library(moments) &gt; gapminder %&gt;% filter(year==2007) %&gt;% select(gdpPercap) %&gt;% + kurtosis() gdpPercap 3.29593 &gt; gapminder %&gt;% filter(year==2007) %&gt;% select(gdpPercap) %&gt;% + log() %&gt;% kurtosis() gdpPercap 1.871608 &gt; rnorm(10000) %&gt;% kurtosis() [1] 2.955853 16.15 Visualizing Skewness and Kurtosis 16.16 Covariance and Correlation It is often the case that two or more quantitative variables are measured on each unit of observation (such as an individual). We are then often interested in characterizing how pairs of variables are associated or how they vary together. Two common measures for this are called “covariance” and “correlation”, both of which are most well suited for measuring linear associations 16.16.1 Covariance Suppose we observe \\(n\\) pairs of data \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\). Their sample covariance is \\[ {\\operatorname{cov}}_{xy} = \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{(n-1)}, \\] which meausers how the two variables “covary” about their respective means. Large positive numbers indicate concordance of deviations from the mean, and large negative numbers indicated discordance (so opposite sides of the mean). 16.16.2 Pearson Correlation Pearson correlation is sample covariance scaled by the variables’ standard deviations, meaning correlation is a unitless measure of variation about the mean. It is defined by \\[\\begin{eqnarray} r_{xy} &amp; = &amp; \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2 \\sum_{i=1}^n (y_i - \\overline{y})^2}} \\\\ \\ &amp; = &amp; \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{(n-1) s_x s_y} \\\\ \\ &amp; = &amp; \\frac{ \\operatorname{cov}_{xy}}{s_x s_y} \\end{eqnarray}\\] where \\(s_x\\) and \\(s_y\\) are the sample standard deviations of each measured variable. Note that \\(-1 \\leq r_{xy} \\leq 1\\). 16.16.3 Spearman Correlation There are other ways to measure correlation that are less reliant on linear trends in covariation and are also more robust to outliers. Specifically, one can convert each measured variable to ranks by size (1 for the smallest, \\(n\\) for the largest) and then use a formula for correlation designed for these ranks. One popular measure of rank-based correlation is the Spearman correlation. &gt; x &lt;- rnorm(500) &gt; y &lt;- x + rnorm(500) &gt; cor(x, y, method=&quot;pearson&quot;) [1] 0.7542651 &gt; cor(x, y, method=&quot;spearman&quot;) [1] 0.7499555 &gt; x &lt;- rnorm(500) &gt; y &lt;- x + rnorm(500, sd=2) &gt; cor(x, y, method=&quot;pearson&quot;) [1] 0.5164903 &gt; cor(x, y, method=&quot;spearman&quot;) [1] 0.5093092 &gt; x &lt;- c(rnorm(499), 100) &gt; y &lt;- c(rnorm(499), 100) &gt; cor(x, y, method=&quot;pearson&quot;) [1] 0.9528564 &gt; cor(x, y, method=&quot;spearman&quot;) [1] -0.02133551 "],
["data-visualization-basics.html", "17 Data Visualization Basics 17.1 Plots 17.2 R Base Graphics 17.3 Read the Documentation 17.4 Barplot 17.5 Boxplot 17.6 Constructing Boxplots 17.7 Boxplot with Outliers 17.8 Histogram 17.9 Histogram with More Breaks 17.10 Density Plot 17.11 Boxplot (Side-By-Side) 17.12 Stacked Barplot 17.13 Scatterplot 17.14 Quantile-Quantile Plots", " 17 Data Visualization Basics 17.1 Plots Single variables: Barplot Boxplot Histogram Density plot Two or more variables: Side-by-Side Boxplots Stacked Barplot Scatterplot 17.2 R Base Graphics We’ll first plodding through “R base graphics”, which means graphics functions that come with R. By default they are very simple. However, they can be customized a lot, but it takes a lot of work. Also, the syntax varies significantly among plot types and some think the syntax is not user-friendly. We will consider a very highly used graphics package next week, called ggplot2 that provides a “grammar of graphics”. It hits a sweet spot of “flexibility vs. complexity” for many data scientists. 17.3 Read the Documentation For all of the plotting functions covered below, read the help files. &gt; ?barplot &gt; ?boxplot &gt; ?hist &gt; ?density &gt; ?plot &gt; ?legend 17.4 Barplot &gt; cyl_tbl &lt;- table(mtcars$cyl) &gt; barplot(cyl_tbl, xlab=&quot;Cylinders&quot;, ylab=&quot;Count&quot;) 17.5 Boxplot &gt; boxplot(mtcars$mpg, ylab=&quot;MPG&quot;, col=&quot;lightgray&quot;) 17.6 Constructing Boxplots The top of the box is Q3 The line through the middle of the box is the median The bottom of the box is Q1 The top whisker is the minimum of Q3 + 1.5 \\(\\times\\) IQR or the largest data point The bottom whisker is the maximum of Q1 - 1.5 \\(\\times\\) IQR or the smallest data point Outliers lie outside of (Q1 - 1.5 \\(\\times\\) IQR) or (Q3 + 1.5 \\(\\times\\) IQR), and they are shown as points Outliers are calculated using the fivenum() function 17.7 Boxplot with Outliers &gt; boxplot(mtcars$wt, ylab=&quot;Weight (1000 lbs)&quot;, + col=&quot;lightgray&quot;) 17.8 Histogram &gt; hist(mtcars$mpg, xlab=&quot;MPG&quot;, main=&quot;&quot;, col=&quot;lightgray&quot;) 17.9 Histogram with More Breaks &gt; hist(mtcars$mpg, breaks=12, xlab=&quot;MPG&quot;, main=&quot;&quot;, col=&quot;lightgray&quot;) 17.10 Density Plot &gt; plot(density(mtcars$mpg), xlab=&quot;MPG&quot;, main=&quot;&quot;) &gt; polygon(density(mtcars$mpg), col=&quot;lightgray&quot;, border=&quot;black&quot;) 17.11 Boxplot (Side-By-Side) &gt; boxplot(mpg ~ cyl, data=mtcars, xlab=&quot;Cylinders&quot;, + ylab=&quot;MPG&quot;, col=&quot;lightgray&quot;) 17.12 Stacked Barplot &gt; counts &lt;- table(mtcars$cyl, mtcars$gear) &gt; counts 3 4 5 4 1 8 2 6 2 4 1 8 12 0 2 &gt; barplot(counts, main=&quot;Number of Gears and Cylinders&quot;, + xlab=&quot;Gears&quot;, col=c(&quot;blue&quot;,&quot;red&quot;, &quot;lightgray&quot;)) &gt; legend(x=&quot;topright&quot;, title=&quot;Cyl&quot;, + legend = rownames(counts), + fill = c(&quot;blue&quot;,&quot;red&quot;, &quot;lightgray&quot;)) 17.13 Scatterplot &gt; plot(mtcars$wt, mtcars$mpg, xlab=&quot;Weight (1000 lbs)&quot;, + ylab=&quot;MPG&quot;) 17.14 Quantile-Quantile Plots Quantile-quantile plots display the quantiles of: two samples of data a sample of data vs a theoretical distribution The first type allows one to assess how similar the distributions are of two samples of data. The second allows one to assess how similar a sample of data is to a theoretical distribution (often Normal with mean 0 and standard deviation 1). &gt; qqnorm(mtcars$mpg, main=&quot; &quot;) &gt; qqline(mtcars$mpg) # line through Q1 and Q3 &gt; before1980 &lt;- gapminder %&gt;% filter(year &lt; 1980) %&gt;% + select(lifeExp) %&gt;% unlist() &gt; after1980 &lt;- gapminder %&gt;% filter(year &gt; 1980) %&gt;% + select(lifeExp) %&gt;% unlist() &gt; qqplot(before1980, after1980); abline(0,1) &gt; ggplot(mtcars) + stat_qq(aes(sample = mpg)) &gt; ggplot(gapminder) + stat_qq(aes(sample=lifeExp)) &gt; ggplot(gapminder) + + stat_qq(aes(sample=lifeExp, color=continent)) "],
["a-grammar-of-graphics.html", "18 A Grammar of Graphics 18.1 Rationale 18.2 Package ggplot2 18.3 Pieces of the Grammar 18.4 Geometries 18.5 Call Format 18.6 Layers 18.7 Placement of the aes() Call 18.8 Original Publications 18.9 Documentation 18.10 Barplots 18.11 Boxplots and Violin Plots 18.12 Histograms and Density Plots 18.13 Line Plots 18.14 Scatterplots 18.15 Axis Scales 18.16 Scatterplot Smoothers 18.17 Overplotting 18.18 Labels and Legends 18.19 Facets 18.20 Colors 18.21 Saving Plots 18.22 Dynamic Visualization 18.23 Themes", " 18 A Grammar of Graphics 18.1 Rationale A grammar for communicating data visualization: Data: the data set we are plotting Aesthetics: the variation or relationships in the data we want to visualize Geometries: the geometric object by which we render the aesthetics Coordinates: the coordinate system used (not covered here) Facets: the layout of plots required to visualize the data Other Options: any other customizations we wish to make, such as changing the color scheme or labels These are strung together like words in a sentence. 18.2 Package ggplot2 The R package ggplot2 implements a grammar of graphics along these lines. First, let’s load ggplot2: &gt; library(ggplot2) Now let’s set a theme (more on this later): &gt; theme_set(theme_bw()) 18.3 Pieces of the Grammar ggplot() aes() geom_*() facet_*() scale_*() theme() labs() The * is a placeholder for a variety of terms that we will consider. 18.4 Geometries Perhaps the most important aspect of ggplot2 is to understand the “geoms”. We will cover the following: geom_bar() geom_boxplot() geom_violin() geom_histogram() geom_density() geom_line() geom_point() geom_smooth() geom_hex() 18.5 Call Format The most basic ggplot2 plot is made with something like: ggplot(data = &lt;DATA FRAME&gt;) + geom_*(mapping = aes(x = &lt;VAR X&gt;, y = &lt;VAR Y&gt;)) where &lt;DATA FRAME&gt; is a data frame and &lt;VAR X&gt; and &lt;VAR Y&gt; are variables (i.e., columns) from this data frame. Recall geom_* is a placeholder for a geometry such as geom_boxplot. 18.6 Layers There’s a complex “layers” construct occurring in the ggplot2 package. However, for our purposes, it suffices to note that the different parts of the plots are layered together through the + operator: &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy, color=drv)) + + geom_smooth(mapping = aes(x = displ, y = hwy, color=drv)) + + scale_color_brewer(palette = &quot;Set1&quot;, name = &quot;Drivetrain&quot;) + + labs(title = &quot;Highway MPG By Drivetrain and Displacement&quot;, + x = &quot;Displacement&quot;, y = &quot;Highway MPG&quot;) 18.7 Placement of the aes() Call In the previous slide, we saw that the same aes() call was made for two geom’s. When this is the case, we may more simply call aes() from within ggplot(): &gt; ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color=drv)) + + geom_point() + + geom_smooth() + + scale_color_brewer(palette = &quot;Set1&quot;, name = &quot;Drivetrain&quot;) + + labs(title = &quot;Highway MPG By Drivetrain and Displacement&quot;, + x = &quot;Displacement&quot;, y = &quot;Highway MPG&quot;) There may be cases where different geom’s are layered and require different aes() calls. This is something to keep in mind as we go through the specifics of the ggplot2 package. 18.8 Original Publications Wickham, H. (2010) A Layered Grammar of Graphics. Journal of Computational and Graphical Statistics, 19 (1): 3–28. This paper designs an implementation of The Grammar of Graphics by Leland Wilkinson (published in 2005). 18.9 Documentation In R: help(package=&quot;ggplot2&quot;) http://docs.ggplot2.org/current/ http://www.cookbook-r.com/Graphs/ ggplot2: Elegant Graphics for Data Analysis (somewhat outdated, but gives clear rationale) 18.10 Barplots The geom_bar() layer forms a barplot and only requires an x assignment in the aes() call: &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut)) Color in the bars by assigning fill in geom_bar(), but outside of aes(): &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut), fill = &quot;tomato&quot;) Color within the bars according to a variable by assigning fill in geom_bar() inside of aes(): &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut, fill = cut)) When we use fill = clarity within aes(), we see that it shows the proportion of each clarity value within each cut value: &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut, fill = clarity)) By setting position = &quot;dodge&quot; outside of aes(), it shows bar charts for the clarity values within each cut value: &gt; ggplot(data = diamonds) + + geom_bar(mapping= aes(x = cut, fill = clarity), + position = &quot;dodge&quot;) By setting position = &quot;fill&quot;, it shows the proportion of clarity values within each cut value and no longer shows the cut values: &gt; ggplot(data = diamonds) + + geom_bar(mapping=aes(x = cut, fill = clarity), + position = &quot;fill&quot;) + + labs(x = &quot;cut&quot;, y = &quot;relative proporition within cut&quot;) 18.11 Boxplots and Violin Plots The geom_boxplot() layer forms a boxplot and requires both x and y assignments in the aes() call, even when plotting a single boxplot: &gt; ggplot(data = mpg) + + geom_boxplot(mapping = aes(x = 1, y = hwy)) Color in the boxes by assigning fill in geom_boxplot(), but outside of aes(): &gt; ggplot(data = mpg) + + geom_boxplot(mapping = aes(x = 1, y = hwy), + fill=&quot;lightblue&quot;) + + labs(x=NULL) Show a boxplot for the y values occurring within each x factor level by making these assignments in aes(): &gt; ggplot(data = mpg) + + geom_boxplot(mapping = aes(x = factor(cyl), y = hwy)) By assigning the fill argument within aes(), we can color each boxplot according to the x-axis factor variable: &gt; ggplot(data = mpg) + + geom_boxplot(mapping = aes(x = factor(cyl), y = hwy, + fill = factor(cyl))) The geom_jitter() function plots the data points and randomly jitters them so we can better see all of the points: &gt; ggplot(data = mpg, mapping = aes(x=factor(cyl), y=hwy)) + + geom_boxplot(fill = &quot;lightblue&quot;) + + geom_jitter(width = 0.2) A violin plot, called via geom_violin(), is similar to a boxplot, except shows a density plot turned on its side and reflected across its vertical axis: &gt; ggplot(data = mpg) + + geom_violin(mapping = aes(x = drv, y = hwy)) Add a geom_jitter() to see how the original data points relate to the violin plots: &gt; ggplot(data = mpg, mapping = aes(x = drv, y = hwy)) + + geom_violin(adjust=1.2) + + geom_jitter(width=0.2, alpha=0.5) Boxplot example on the gapminder data: &gt; ggplot(gapminder, aes(x = continent, y = lifeExp)) + + geom_boxplot(outlier.colour = &quot;red&quot;) + + geom_jitter(width = 0.1, alpha = 0.25) Analogous violin plot example on the gapminder data: &gt; ggplot(gapminder, aes(x = continent, y = lifeExp)) + + geom_violin() + + geom_jitter(width = 0.1, alpha = 0.25) 18.12 Histograms and Density Plots We can create a histogram using the geom_histogram() layer, which requires an x argument only in the aes() call: &gt; ggplot(gapminder) + + geom_histogram(mapping = aes(x=lifeExp)) We can change the bin width directly in the histogram, which is an intuitive parameter to change based on visual inspection: &gt; ggplot(gapminder) + + geom_histogram(mapping = aes(x=lifeExp), binwidth=5) The bins are sometimes centered in an unexpected manner in ggplot2: &gt; ggplot(diamonds) + + geom_histogram(mapping = aes(x=price), binwidth = 1000) Let’s fix how the bins are centered (make center half of binwidth). &gt; ggplot(diamonds) + + geom_histogram(mapping = aes(x=price), binwidth = 1000, + center=500) Instead of counts on the y-axis, we may instead want the area of the bars to sum to 1, like a probability density: &gt; ggplot(gapminder) + + geom_histogram(mapping = aes(x=lifeExp, y=..density..), + binwidth=5) When we use fill = continent within aes(), we see that it shows the counts of each continent value within each lifeExp bin: &gt; ggplot(gapminder) + + geom_histogram(mapping = aes(x=lifeExp, fill = continent), + binwidth = 5) Display a density plot using the geom_density() layer: &gt; ggplot(gapminder) + + geom_density(mapping = aes(x=lifeExp)) Employ the arguments color=&quot;blue&quot; and fill=&quot;lightblue&quot; outside of the aes() call to include some colors: &gt; ggplot(gapminder) + + geom_density(mapping = aes(x=lifeExp), color=&quot;blue&quot;, + fill=&quot;lightblue&quot;) By utilizing color=as.factor(year) we plot a density of lifeExp stratified by each year value: &gt; ggplot(gapminder) + + geom_density(aes(x=lifeExp, color=as.factor(year)), + size=1.2) Overlay a density plot and a histogram together: &gt; ggplot(gapminder, mapping = aes(x=lifeExp)) + + geom_histogram(aes(y=..density..), color=&quot;black&quot;, + fill=&quot;white&quot;) + + geom_density(fill=&quot;lightblue&quot;, alpha=.5) 18.13 Line Plots babynames Revisited Let’s first create a data frame that captures the number of times “John” is registered in males per year: &gt; library(&quot;babynames&quot;) &gt; john &lt;- babynames %&gt;% filter(sex==&quot;M&quot;, name==&quot;John&quot;) &gt; head(john) # A tibble: 6 x 5 year sex name n prop &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 1880 M John 9655 0.0815 2 1881 M John 8769 0.0810 3 1882 M John 9557 0.0783 4 1883 M John 8894 0.0791 5 1884 M John 9388 0.0765 6 1885 M John 8756 0.0755 We can geom_lines() to plot a line showing the popularity of “John” over time: &gt; ggplot(data = john) + + geom_line(mapping = aes(x=year, y=prop), size=1.5) Now let’s look at a name that occurs nontrivially in males and females: &gt; kelly &lt;- babynames %&gt;% filter(name==&quot;Kelly&quot;) &gt; ggplot(data = kelly) + + geom_line(mapping = aes(x=year, y=prop, color=sex), + size=1.5) 18.14 Scatterplots The layer geom_point() produces a scatterplot, and the aes() call requires x and y assignment: &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy)) Give the points a color: &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy), + color = &quot;blue&quot;) Color the points according to a factor variable by including color = class within the aes() call: &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy, + color = class)) Increase the size of points with size=2 outside of the aes() call: &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy, + color = class), size=2) Vary the size of the points according to the pop variable: &gt; gapminder %&gt;% filter(year==2007) %&gt;% ggplot() + + geom_point(aes(x = log(gdpPercap), y = lifeExp, + size = pop)) Vary the transparency of the points according to the class factor variable by setting alpha=class within the aes() call: &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy, + alpha = class)) Warning: Using alpha for a discrete variable is not advised. Vary the shape of the points according to the class factor variable by setting alpha=class within the aes() call (maximum 6 possible shapes – oops!): &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy, + shape = class)) Color the points according to the cut variable by setting color=cut within the aes() call: &gt; ggplot(data = diamonds) + + geom_point(mapping = aes(x=carat, y=price, color=cut), + alpha=0.7) Color the points according to the clarity variable by setting color=clarity within the aes() call: &gt; ggplot(data = diamonds) + + geom_point(mapping=aes(x=carat, y=price, color=clarity), + alpha=0.3) Override the alpha=0.3 in the legend: &gt; ggplot(data=diamonds) + + geom_point(mapping=aes(x=carat, y=price, color=clarity), + alpha=0.3) + + guides(color=guide_legend(override.aes = list(alpha = 1))) 18.15 Axis Scales A different way to take the log of gdpPercap: &gt; gapminder %&gt;% filter(year==2007) %&gt;% ggplot() + + geom_point(aes(x = gdpPercap, y = lifeExp, + size = pop)) + + scale_x_log10() The price variable seems to be significantly right-skewed: &gt; ggplot(diamonds) + + geom_boxplot(aes(x=color, y=price)) We can try to reduce this skewness by rescaling the variables. We first try to take the log(base=10) of the price variable via scale_y_log10(): &gt; ggplot(diamonds) + + geom_boxplot(aes(x=color, y=price)) + + scale_y_log10() Let’s repeat this on the analogous violing plots: &gt; ggplot(diamonds) + + geom_violin(aes(x=color, y=price)) + + scale_y_log10() The relationship between carat and price is nonlinear. Let’s explore different transformations to find an approximately linear relationship. &gt; ggplot(data = diamonds) + + geom_point(mapping=aes(x=carat, y=price, color=clarity), + alpha=0.3) First try to take the squareroot of the the price variable: &gt; ggplot(data = diamonds) + + geom_point(aes(x=carat, y=price, color=clarity), + alpha=0.3) + + scale_y_sqrt() Now let’s try to take log(base=10) on both the carat and price variables: &gt; ggplot(data = diamonds) + + geom_point(aes(x=carat, y=price, color=clarity), alpha=0.3) + + scale_y_log10(breaks=c(1000,5000,10000)) + + scale_x_log10(breaks=1:5) Forming a violin plot of price stratified by clarity and transforming the price variable yields an interesting relationship in this data set: &gt; ggplot(diamonds) + + geom_violin(aes(x=clarity, y=price, fill=clarity), + adjust=1.5) + + scale_y_log10() 18.16 Scatterplot Smoothers Fitting “Smoothers” and Other Models to Scatterplots Later this semester, we will spend several weeks learning how to explain or predict an outcome variable in terms of predictor variables We will briefly show here how to plot some simple model fits to scatterplots You may want to return to these slides later in the semester once we cover modeling in more detail Recall the scatterplot showing the relationship between highway mpg and displacement. How can we plot a smoothed relationship between these two variables? &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy)) Plot a smoother with geom_smooth() using the default settings (other than removing the error bands): &gt; ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + + geom_point() + + geom_smooth(se=FALSE) The default smoother here is a “loess” smoother. Let’s compare that to the least squares regresson line: &gt; ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + + geom_point() + + geom_smooth(aes(colour = &quot;loess&quot;), method = &quot;loess&quot;, se = FALSE) + + geom_smooth(aes(colour = &quot;lm&quot;), method = &quot;lm&quot;, se = FALSE) Now let’s plot a smoother to the points stratified by the drv variable: &gt; ggplot(data=mpg, mapping = aes(x = displ, y = hwy, + linetype = drv)) + + geom_point() + + geom_smooth(se=FALSE) Instead of different line types, let’s instead differentiate them by line color: &gt; ggplot(data = mpg, mapping = aes(x = displ, y = hwy, + color=drv)) + + geom_point() + + geom_smooth(se=FALSE) 18.17 Overplotting Definition Overplotting occurs when there are many observations, resulting in many objects being plotted on top of each other For example, the diamonds data set has 53940 observations per variable Let’s explore some ways to deal with overplotting Here is an example of an overplotted scatterplot: &gt; ggplot(data = diamonds, mapping = aes(x=carat, y=price)) + + geom_point() Let’s reduce the alpha of the points: &gt; ggplot(data = diamonds, mapping = aes(x=carat, y=price)) + + geom_point(alpha=0.1) Let’s further reduce the alpha: &gt; ggplot(data = diamonds, mapping = aes(x=carat, y=price)) + + geom_point(alpha=0.01) We can bin the points into hexagons, and report how many points fall within each bin. We use the geom_hex() layer to do this: &gt; ggplot(data = diamonds, mapping = aes(x=carat, y=price)) + + geom_hex() Let’s try to improve the color scheme: &gt; ggplot(data = diamonds, mapping = aes(x=carat, y=price)) + + geom_hex() + + scale_fill_gradient2(low=&quot;lightblue&quot;, mid=&quot;purple&quot;, high=&quot;black&quot;, + midpoint=3000) We can combine the scale transformation used earlier with the “hexbin” plotting method: &gt; ggplot(data = diamonds, mapping = aes(x=carat, y=price)) + + geom_hex(bins=20) + + scale_x_log10(breaks=1:5) + scale_y_log10(breaks=c(1000,5000,10000)) 18.18 Labels and Legends Here’s how you can change the axis labels and give the plot a title: &gt; ggplot(data = mpg) + + geom_boxplot(mapping = aes(x = factor(cyl), y = hwy)) + + labs(title=&quot;Highway MPG by Cylinders&quot;,x=&quot;Cylinders&quot;, + y=&quot;Highway MPG&quot;) You can remove the legend to a plot by the following: &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut, fill = cut)) + + theme(legend.position=&quot;none&quot;) The legend can be placed on the “top”, “bottom”, “left”, or “right”: &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut, fill = cut)) + + theme(legend.position=&quot;bottom&quot;) The legend can be moved inside the plot itself: &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut, fill = cut)) + + theme(legend.position=c(0.15,0.75)) Change the name of the legend: &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut, fill = cut)) + + scale_fill_discrete(name=&quot;Diamond\\nCut&quot;) Change the labels within the legend: &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut, fill = cut)) + + scale_fill_discrete(labels=c(&quot;F&quot;, &quot;G&quot;, &quot;VG&quot;, &quot;P&quot;, &quot;I&quot;)) 18.19 Facets Here is the histogram of the displ variable from the mpg data set: &gt; ggplot(mpg) + geom_histogram(mapping=aes(x=displ), + binwidth=0.25) The facet_wrap() layer allows us to stratify the displ variable according to cyl, and show the histograms for the strata in an organized fashion: &gt; ggplot(mpg) + + geom_histogram(mapping=aes(x=displ), binwidth=0.25) + + facet_wrap(~ cyl) Here is facet_wrap() applied to displ startified by the drv variable: &gt; ggplot(mpg) + + geom_histogram(mapping=aes(x=displ), binwidth=0.25) + + facet_wrap(~ drv) We can stratify by two variable simultaneously by using the facet_grid() layer: &gt; ggplot(mpg) + + geom_histogram(mapping=aes(x=displ), binwidth=0.25) + + facet_grid(drv ~ cyl) Let’s carry out a similar faceting on the diamonds data over the next four plots: &gt; ggplot(diamonds) + + geom_histogram(mapping=aes(x=price), binwidth=500) Stratify price by clarity: &gt; ggplot(diamonds) + + geom_histogram(mapping=aes(x=price), binwidth=500) + + facet_wrap(~ clarity) Stratify price by clarity, but allow each y-axis range to be different by including the scale=&quot;free_y&quot; argument: &gt; ggplot(diamonds) + + geom_histogram(mapping=aes(x=price), binwidth=500) + + facet_wrap(~ clarity, scale=&quot;free_y&quot;) Jointly stratify price by cut and clarify: &gt; ggplot(diamonds) + + geom_histogram(mapping=aes(x=price), binwidth=500) + + facet_grid(cut ~ clarity) + + scale_x_continuous(breaks=9000) 18.20 Colors 18.20.1 Finding Colors A list of named colors in R (e.g., “lightblue”) RColorBrewer package The Crayola crayon colors from the broman package – use brocolors(set=&quot;crayons&quot;) Color blind palette: &gt; cbPalette &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, + &quot;#D55E00&quot;, &quot;#CC79A7&quot;) 18.20.2 Some Useful Layers scale_fill_manual() scale_color_manual() scale_fill_gradient() scale_color_gradient() Manually determine colors to fill the barplot using the color blind palette defined above, cbPalette: &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut, fill = cut)) + + scale_fill_manual(values=cbPalette) Manually determine point colors using the color blind palette defined above, cbPalette: &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy, color = class), size=2) + + scale_color_manual(values=cbPalette) Fill the histogram bars using a color gradient by their counts, where we determine the endpoint colors: &gt; ggplot(data = mpg) + + geom_histogram(aes(x=hwy, fill=..count..)) + + scale_fill_gradient(low=&quot;blue&quot;, high=&quot;red&quot;) Color the points based on a gradient formed from the quantitative variable, displ, where we we determine the endpoint colors: &gt; ggplot(data = mpg) + + geom_point(aes(x=hwy, y=cty, color=displ), size=2) + + scale_color_gradient(low=&quot;blue&quot;, high=&quot;red&quot;) An example of using the palette “Set1” from the RColorBrewer package, included in ggplot2: &gt; ggplot(diamonds) + + geom_density(mapping = aes(x=price, color=clarity)) + + scale_color_brewer(palette = &quot;Set1&quot;) Another example of using the palette “Set1” from the RColorBrewer package, included in ggplot2: &gt; ggplot(data = mpg) + + geom_point(mapping = aes(x = displ, y = hwy, color = class)) + + scale_color_brewer(palette = &quot;Set1&quot;) The gapminder package comes with its own set of colors, country_colors. &gt; ggplot(subset(gapminder, continent != &quot;Oceania&quot;), + aes(x = year, y = lifeExp, group = country, + color = country)) + + geom_line(show.legend = FALSE) + facet_wrap(~ continent) + + scale_color_manual(values = country_colors) 18.21 Saving Plots 18.21.1 Saving Plots as Variables Pieces of the plots can be saved as variables, which is particular useful to explortatory data analysis. These all produce the same plot: &gt; ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color=drv)) + + geom_point() + + geom_smooth(se=FALSE) &gt; p &lt;- ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color=drv)) + + geom_point() &gt; p + geom_smooth(se=FALSE) &gt; p &lt;- ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color=drv)) &gt; p + geom_point() + geom_smooth(se=FALSE) Try it yourself! 18.21.2 Saving Plots to Files Plots can be saved to many formats using the ggsave() function. Here are some examples: &gt; p &lt;- ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color=drv)) + + geom_point() + + geom_smooth(se=FALSE) &gt; ggsave(filename=&quot;my_plot.pdf&quot;, plot=p) # saves PDF file &gt; ggsave(filename=&quot;my_plot.png&quot;, plot=p) # saves PNG file Here are the arguments that ggsave() takes: &gt; str(ggsave) function (filename, plot = last_plot(), device = NULL, path = NULL, scale = 1, width = NA, height = NA, units = c(&quot;in&quot;, &quot;cm&quot;, &quot;mm&quot;), dpi = 300, limitsize = TRUE, ...) 18.22 Dynamic Visualization 18.22.1 Examples Tools to dynamically interact with data visualizations (and calculations) are becoming increasingly common and straightforward to implement. Here are several examples: Shiny (see also, example from my lab) plotly ggvis animation gganimate &gt; p &lt;- ggplot(gapminder) + + geom_point(aes(x=gdpPercap, y=lifeExp, size = pop, + color = continent, frame = year)) + + scale_x_log10() &gt; gganimate(p, &quot;animation_ex1.gif&quot;, ani.height=400, ani.width=500) The resulting file can be viewed here: https://github.com/jdstorey/asdslectures/blob/master/docs/images/animation_ex1.gif &gt; p &lt;- ggplot(gapminder) + + geom_density(aes(x=lifeExp, color=as.factor(year), + frame=year), + size=1.2) + + scale_color_discrete(name=&quot;year&quot;) &gt; gganimate(p, &quot;animation_ex2.gif&quot;, ani.height=400, ani.width=500) The resulting file can be viewed here: https://github.com/jdstorey/asdslectures/blob/master/docs/images/animation_ex2.gif &gt; p &lt;- ggplot(gapminder) + + geom_density(aes(x=lifeExp, color=as.factor(year), + frame=year, cumulative = TRUE), + size=1.2) + + scale_color_discrete(name=&quot;year&quot;) &gt; gganimate(p, &quot;animation_ex3.gif&quot;, ani.height=400, ani.width=500) The resulting file can be viewed here: https://github.com/jdstorey/asdslectures/blob/master/docs/images/animation_ex3.gif 18.23 Themes 18.23.1 Available Themes See https://r4ds.had.co.nz/graphics-for-communication.html#themes for an explanation of the ggplot2 themes. See also the ggthemes package for additional themes. 18.23.2 Setting a Theme Globally: &gt; theme_set(theme_minimal()) Locally: &gt; ggplot(data = diamonds) + + geom_bar(mapping = aes(x = cut)) + + theme_minimal() "],
["eda-of-high-dimensional-data.html", "19 EDA of High-Dimensional Data 19.1 Definition 19.2 Examples 19.3 Big Data vs HD Data 19.4 Definition of HD Data 19.5 Rationale", " 19 EDA of High-Dimensional Data 19.1 Definition High-dimensional data (HD data) typically refers to data sets where many variables are simultaneously measured on any number of observations. The number of variables is often represented by \\(p\\) and the number of observations by \\(n\\). HD data are collected into a \\(p \\times n\\) or \\(n \\times p\\) matrix. Many methods exist for “large \\(p\\), small \\(n\\)” data sets. 19.2 Examples Clinical studies Genomics (e.g., gene expression) Neuroimaging (e.g., fMRI) Finance (e.g., time series) Environmental studies Internet data (e.g., Netflix movie ratings) 19.3 Big Data vs HD Data “Big data” are data sets that cannot fit into a standard computer’s memory. HD data were defined above. They are not necessarily equivalent. 19.4 Definition of HD Data High-dimesional data is a data set where the number of variables measured is many. Large same size data is a data set where few variables are measured, but many observations are measured. Big data is a data set where there are so many data points that it cannot be managed straightforwardly in memory, but must rather be stored and accessed elsewhere. Big data can be high-dimensional, large sample size, or both. We will abbreviate high-dimensional with HD. 19.5 Rationale Exploratory data analysis (EDA) of high-dimensional data adds the additional challenge that many variables must be examined simultaneously. Therefore, in addition to the EDA methods we discussed earlier, methods are often employed to organize, visualize, or numerically capture high-dimensional data into lower dimensions. Examples of EDA approaches applied to HD data include: Traditional EDA methods covered earlier Cluster analysis Dimensionality reduction "],
["cluster-analysis.html", "20 Cluster Analysis 20.1 Definition 20.2 Types of Clustering 20.3 Top-Down vs Bottom-Up 20.4 Challenges 20.5 Illustrative Data Sets 20.6 Distance Measures 20.7 Hierarchical Clustering 20.8 K-Means Clustering", " 20 Cluster Analysis 20.1 Definition Cluster analysis is the process of grouping objects (variables or observations) into groups based on measures of similarity. Similar objects are placed in the same cluster, and dissimilar objects are placed in different clusters. Cluster analysis methods are typically described by algorithms (rather than models or formulas). 20.2 Types of Clustering Clustering can be categorized in various ways: Hard vs. soft Top-down vs bottom-up Partitioning vs. hierarchical agglomerative 20.3 Top-Down vs Bottom-Up We will discuss two of the major clustering methods – hierarchical clustering and K-means clustering. Hierarchical clustering is an example of bottom-up clustering in that the process begings with each object being its own cluster and then objects are joined in a hierarchical manner into larger and larger clusters. \\(K\\)-means clustering is an example of top-down clustering in that the number of clusters is chosen beforehand and then object are assigned to one of the \\(K\\) clusters. 20.4 Challenges Cluster analysis method Distance measure Number of clusters Convergence issues 20.5 Illustrative Data Sets 20.5.1 Simulated data1 20.5.2 “True” Clusters data1 20.5.3 Simulated data2 20.5.4 “True” Clusters data2 20.6 Distance Measures 20.6.1 Objects Most clustering methods require calculating a “distance” between two objects. Let \\(\\pmb{a} = (a_1, a_2, \\ldots, a_n)\\) be one object and \\(\\pmb{b} = (b_1, b_2, \\ldots, b_n)\\) be another object. We will assume both objects are composed of real numbers. 20.6.2 Euclidean Euclidean distance is the shortest spatial distance between two objects in Euclidean space. Euclidean distance is calculated as: \\[d(\\pmb{a}, \\pmb{b}) = \\sqrt{\\sum_{i=1}^n \\left(a_i - b_i \\right)^2}\\] 20.6.3 Manhattan Manhattan distance is sometimes called taxicab distance. If you picture two locations in a city, it is the distance a taxicab must travel to get from one location to the other. Manhattan distance is calculated as: \\[d(\\pmb{a}, \\pmb{b}) = \\sum_{i=1}^n \\left| a_i - b_i \\right|\\] 20.6.4 Euclidean vs Manhattan Green is Euclidean. All others are Manhattan (and equal). Figure from Exploratory Data Analysis with R. 20.6.5 dist() A distance matrix – which is the set of values resulting from a distance measure applied to all pairs of objects – can be obtained through the function dist(). Default arguments for dist(): &gt; str(dist) function (x, method = &quot;euclidean&quot;, diag = FALSE, upper = FALSE, p = 2) The key argument for us is method= which can take values method=&quot;euclidean&quot; and method=&quot;manhattan&quot; among others. See ?dist. 20.6.6 Distance Matrix data1 &gt; sub_data1 &lt;- data1[1:4, c(1,2)] &gt; sub_data1 x y 1 2.085818 2.248086 2 1.896636 1.369547 3 2.097729 2.386383 4 1.491026 2.029814 &gt; mydist &lt;- dist(sub_data1) &gt; print(mydist) 1 2 3 2 0.8986772 3 0.1388086 1.0365293 4 0.6335776 0.7749019 0.7037257 &gt; (sub_data1[1,] - sub_data1[2,])^2 %&gt;% sum() %&gt;% sqrt() [1] 0.8986772 20.7 Hierarchical Clustering 20.7.1 Strategy Hierarchical clustering is a hierarchical agglomerative, bottom-up clustering method that strategically joins objects into larger and larger clusters, until all objects are contained in a single cluster. Hierarchical clustering results are typically displayed as a dendrogram. The number of clusters does not necessarily need to be known or chosen by the analyst. 20.7.2 Example: Cancer Subtypes Figure from Alizadeh et al. (2000) Nature. 20.7.3 Algorithm The algorithm for hierarchical clustering works as follows. Start with each object assigned as its own cluster. Calculate a distance between all pairs of clusters. Join the two clusters with the smallest distance. Repeat steps 2–3 until there is only one cluster. At the very first iteration of the algorithm, all we need is some distance function (e.g., Euclidean or Manhattan) to determine the two objects that are closest. But once clusters with more than one object are present, how do we calculate the distance between two clusters? This is where a key choice called the linkage method or criterion is needed. 20.7.4 Linkage Criteria Suppose there are two clusters \\(A\\) and \\(B\\) and we have a distance function \\(d(\\pmb{a}, \\pmb{b})\\) for all objects \\(\\pmb{a} \\in A\\) and \\(\\pmb{b} \\in B\\). Here are three ways (among many) to calculate a distance between clusters \\(A\\) and \\(B\\): \\[\\begin{eqnarray} \\mbox{Complete: } &amp; \\max \\{d(\\pmb{a}, \\pmb{b}): \\pmb{a} \\in A, \\pmb{b} \\in B\\} \\\\ \\mbox{Single: } &amp; \\min \\{d(\\pmb{a}, \\pmb{b}): \\pmb{a} \\in A, \\pmb{b} \\in B\\} \\\\ \\mbox{Average: } &amp; \\frac{1}{|A| |B|} \\sum_{\\pmb{a} \\in A} \\sum_{\\pmb{b} \\in B} d(\\pmb{a}, \\pmb{b}) \\end{eqnarray}\\] 20.7.5 hclust() The hclust() function produces an R object that contains all of the information needed to create a complete hierarchical clustering. Default arguments for hclust(): &gt; str(hclust) function (d, method = &quot;complete&quot;, members = NULL) The primary input for hclust() is the d argument, which is a distance matrix (usually obtained from dist()). The method argument takes the linkage method, which includes method=&quot;complete&quot;, method=&quot;single&quot;, method=&quot;average&quot;, etc. See ?hclust. 20.7.6 Hierarchical Clustering of data1 20.7.7 Standard hclust() Usage &gt; mydist &lt;- dist(data1, method = &quot;euclidean&quot;) &gt; myhclust &lt;- hclust(mydist, method=&quot;complete&quot;) &gt; plot(myhclust) 20.7.8 as.dendrogram() &gt; plot(as.dendrogram(myhclust)) 20.7.9 Modify the Labels &gt; library(dendextend) &gt; dend1 &lt;- as.dendrogram(myhclust) &gt; labels(dend1) &lt;- data1$true_clusters &gt; labels_colors(dend1) &lt;- + c(&quot;red&quot;, &quot;blue&quot;, &quot;gray47&quot;)[as.numeric(data1$true_clusters)] &gt; plot(dend1, axes=FALSE, main=&quot; &quot;, xlab=&quot; &quot;) 20.7.10 Color the Branches &gt; dend2 &lt;- as.dendrogram(myhclust) &gt; labels(dend2) &lt;- rep(&quot; &quot;, nrow(data1)) &gt; dend2 &lt;- color_branches(dend2, k = 3, col=c(&quot;red&quot;, &quot;blue&quot;, &quot;gray47&quot;)) &gt; plot(dend2, axes=FALSE, main=&quot; &quot;, xlab=&quot; &quot;) 20.7.11 Cluster Assignments (\\(K = 3\\)) &gt; est_clusters &lt;- cutree(myhclust, k=3) &gt; est_clusters [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [71] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 [106] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [141] 3 3 3 3 3 3 3 3 3 3 &gt; est_clusters &lt;- factor(est_clusters) &gt; p &lt;- data1 %&gt;% + mutate(est_clusters=est_clusters) %&gt;% + ggplot() &gt; p + geom_point(aes(x=x, y=y, color=est_clusters)) 20.7.12 Cluster Assignments (\\(K = 3\\)) 20.7.13 Cluster Assignments (\\(K = 2\\)) &gt; (data1 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=2))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 20.7.14 Cluster Assignments (\\(K = 4\\)) &gt; (data1 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=4))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 20.7.15 Cluster Assignments (\\(K = 6\\)) &gt; (data1 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=6))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 20.7.16 Linkage: Complete (Default) &gt; data1 %&gt;% dist() %&gt;% hclust(method=&quot;complete&quot;) %&gt;% + as.dendrogram() %&gt;% plot(axes=FALSE) 20.7.17 Linkage: Average &gt; data1 %&gt;% dist() %&gt;% hclust(method=&quot;average&quot;) %&gt;% + as.dendrogram() %&gt;% plot(axes=FALSE) 20.7.18 Linkage: Single &gt; data1 %&gt;% dist() %&gt;% hclust(method=&quot;single&quot;) %&gt;% + as.dendrogram() %&gt;% plot(axes=FALSE) 20.7.19 Linkage: Ward &gt; data1 %&gt;% dist() %&gt;% hclust(method=&quot;ward.D&quot;) %&gt;% + as.dendrogram() %&gt;% plot(axes=FALSE) 20.7.20 Hierarchical Clustering of data2 20.7.21 as.dendrogram() &gt; mydist &lt;- dist(data2, method = &quot;euclidean&quot;) &gt; myhclust &lt;- hclust(mydist, method=&quot;complete&quot;) &gt; plot(as.dendrogram(myhclust)) 20.7.22 Modify the Labels &gt; library(dendextend) &gt; dend1 &lt;- as.dendrogram(myhclust) &gt; labels(dend1) &lt;- data2$true_clusters &gt; labels_colors(dend1) &lt;- + c(&quot;red&quot;, &quot;blue&quot;)[as.numeric(data2$true_clusters)] &gt; plot(dend1, axes=FALSE, main=&quot; &quot;, xlab=&quot; &quot;) 20.7.23 Color the Branches &gt; dend2 &lt;- as.dendrogram(myhclust) &gt; labels(dend2) &lt;- rep(&quot; &quot;, nrow(data2)) &gt; dend2 &lt;- color_branches(dend2, k = 2, col=c(&quot;red&quot;, &quot;blue&quot;)) &gt; plot(dend2, axes=FALSE, main=&quot; &quot;, xlab=&quot; &quot;) 20.7.24 Cluster Assignments (\\(K = 2\\)) &gt; (data2 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=2))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 20.7.25 Cluster Assignments (\\(K = 3\\)) &gt; (data2 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=3))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 20.7.26 Cluster Assignments (\\(K = 4\\)) &gt; (data2 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=4))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 20.7.27 Cluster Assignments (\\(K = 5\\)) &gt; (data2 %&gt;% + mutate(est_clusters=factor(cutree(myhclust, k=6))) %&gt;% + ggplot()) + geom_point(aes(x=x, y=y, color=est_clusters)) 20.8 K-Means Clustering 20.8.1 Strategy K-means clustering is a top-down, partitioning cluster analysis method that assigns each object to one of \\(K\\) clusters based on the distance between each object and the cluster centers, called centroids. This is an iterative algorithm with potential random initial values. The value of \\(K\\) is typically unknown and must be determined by the analyst. 20.8.2 Centroid A centroid is the coordinate-wise average of all objects in a cluster. Let \\(A\\) be a given cluster with objects \\(\\pmb{a} \\in A\\). Its centroid is: \\[\\overline{\\pmb{a}} = \\frac{1}{|A|} \\sum_{\\pmb{a} \\in A} \\pmb{a}\\] 20.8.3 Algorithm The number of clusters \\(K\\) must be chosen beforehand. Initialize \\(K\\) cluster centroids. Assign each object to a cluster by choosing the cluster with the smalllest distance (e.g., Euclidean) between the object and the cluster centroid. Calculate new centroids based on the cluster assignments from Step 2. Repeat Steps 2–3 until convergence. 20.8.4 Notes The initialization of the centroids is typically random, so often the algorithm is run several times with new, random initial centroids. Convergence is usually defined in terms of neglible changes in the centroids or no changes in the cluster assignments. 20.8.5 kmeans() K-means clustering can be accomplished through the following function: &gt; str(kmeans) function (x, centers, iter.max = 10L, nstart = 1L, algorithm = c(&quot;Hartigan-Wong&quot;, &quot;Lloyd&quot;, &quot;Forgy&quot;, &quot;MacQueen&quot;), trace = FALSE) x: the data to clusters, objects along rows centers: either the number of clusters \\(K\\) or a matrix giving initial centroids iter.max: the maximum number of iterations allowed nstart: how many random intial \\(K\\) centroids, where the best one is returned 20.8.6 fitted() The cluster centroids or assigments can be extracted through the function fitted(), which is applied to the output of kmeans(). The input of fitted() is the object returned by kmeans(). The key additional argument is called method. When method=&quot;centers&quot; it returns the centroids. When method=&quot;classes&quot; it returns the cluster assignments. 20.8.7 K-Means Clustering of data1 &gt; km1 &lt;- kmeans(x=data1[,-3], centers=3, iter.max=100, nstart=5) &gt; est_clusters &lt;- fitted(km1, method=&quot;classes&quot;) &gt; est_clusters [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [71] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 [106] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [141] 2 2 2 2 2 2 2 2 2 2 20.8.8 Centroids of data1 &gt; centroids1 &lt;- fitted(km1, method=&quot;centers&quot;) %&gt;% unique() &gt; centroids1 x y 1 1.943184 2.028062 3 2.042872 4.037987 2 4.015934 2.962279 &gt; est_clusters &lt;- fitted(km1, method=&quot;classes&quot;) &gt; data1 %&gt;% mutate(est_clusters = factor(est_clusters)) %&gt;% + group_by(est_clusters) %&gt;% summarize(mean(x), mean(y)) # A tibble: 3 x 3 est_clusters `mean(x)` `mean(y)` &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 1.94 2.03 2 2 4.02 2.96 3 3 2.04 4.04 20.8.9 Cluster Assignments (\\(K = 3\\)) &gt; est_clusters &lt;- factor(est_clusters) &gt; ggplot(data1) + geom_point(aes(x=x, y=y, color=est_clusters)) 20.8.10 Cluster Assignments (\\(K = 2\\)) 20.8.11 Cluster Assignments (\\(K = 6\\)) 20.8.12 K-Means Clustering of data2 &gt; km2 &lt;- kmeans(x=data2[,-3], centers=2, iter.max=100, nstart=5) &gt; est_clusters &lt;- fitted(km2, method=&quot;classes&quot;) &gt; est_clusters [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 [71] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [106] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 20.8.13 Cluster Assignments (\\(K = 2\\)) &gt; est_clusters &lt;- factor(est_clusters) &gt; ggplot(data2) + geom_point(aes(x=x, y=y, color=est_clusters)) 20.8.14 Cluster Assignments (\\(K = 3\\)) 20.8.15 Cluster Assignments (\\(K = 5\\)) "],
["principal-component-analysis.html", "21 Principal Component Analysis 21.1 Dimensionality Reduction 21.2 Goal of PCA 21.3 Defining the First PC 21.4 Calculating All PCs 21.5 Singular Value Decomposition 21.6 A Simple PCA Function 21.7 The Ubiquitous PCA Example 21.8 PC Biplots 21.9 PCA Examples", " 21 Principal Component Analysis 21.1 Dimensionality Reduction The goal of dimensionality reduction is to extract low dimensional representations of high dimensional data that are useful for visualization, exploration, inference, or prediction. The low dimensional representations should capture key sources of variation in the data. Some methods for dimensionality reduction include: Principal component analysis Singular value decomposition Latent variable modeling Vector quantization Self-organizing maps Multidimensional scaling We will focus on what is likely the most commonly applied dimensionality reduction tool, principal components analysis. 21.2 Goal of PCA For a given set of variables, principal component analysis (PCA) finds (constrained) weighted sums of the variables to produce variables (called principal components) that capture consectuive maximum levels of variation in the data. Specifically, the first principal component is the weighted sum of the variables that results in a component with the highest variation. This component is then “removed” from the data, and the second principal component is obtained on the resulting residuals. This process is repeated until there is no variation left in the data. 21.3 Defining the First PC Suppose we have \\(m\\) variables, each with \\(n\\) observations: \\[ \\begin{aligned} {\\boldsymbol{x}}_1 &amp; = (x_{11}, x_{12}, \\ldots, x_{1n}) \\\\ {\\boldsymbol{x}}_2 &amp; = (x_{21}, x_{22}, \\ldots, x_{2n}) \\\\ \\ &amp; \\vdots \\ \\\\ {\\boldsymbol{x}}_m &amp; = (x_{m1}, x_{m2}, \\ldots, x_{mn}) \\end{aligned} \\] We can organize these variables into an \\(m \\times n\\) matrix \\({\\boldsymbol{X}}\\) where row \\(i\\) is \\({\\boldsymbol{x}}_i\\). Consider all possible weighted sums of these variables \\[\\tilde{\\pmb{x}} = \\sum_{i=1}^{m} u_i \\pmb{x}_i\\] where we constrain \\(\\sum_{i=1}^{m} u_i^2 = 1\\). We wish to identify the vector \\(\\pmb{u} = \\{u_i\\}_{i=1}^{m}\\) under this constraint that maximizes the sample variance of \\(\\tilde{\\pmb{x}}\\). However, note that if we first mean center each variable, replacing \\(x_{ij}\\) with \\[x_{ij}^* = x_{ij} - \\frac{1}{n} \\sum_{k=1}^n x_{ik},\\] then the sample variance of \\(\\tilde{\\pmb{x}} = \\sum_{i=1}^{m} u_i \\pmb{x}_i\\) is equal to that of \\(\\tilde{\\pmb{x}}^* = \\sum_{i=1}^{m} u_i \\pmb{x}^*_i\\). PCA is a method concerned with decompositon variance and covariance, so we don’t wish to involve the mean of each indivdual variable. Therefore, unless the true population mean of each variable is known (in which case it would be subtracted from its respective variable), we will formulate PCA in terms of mean centered variables, \\(\\pmb{x}^*_i = (x^*_{i1}, x^*_{i2}, \\ldots, x^*_{in})\\), which we can collect into \\(m \\times n\\) matrix \\({\\boldsymbol{X}}^*\\). We therefore consider all possible weighted sums of variables: \\[\\tilde{\\pmb{x}}^* = \\sum_{i=1}^{m} u_i \\pmb{x}^*_i.\\] The first principal component of \\({\\boldsymbol{X}}^*\\) (and \\({\\boldsymbol{X}}\\)) is \\(\\tilde{\\pmb{x}}^*\\) with maximum sample variance \\[ s^2_{\\tilde{{\\boldsymbol{x}}}^*} = \\frac{\\sum_{j=1}^n \\tilde{x}^{*2}_j}{n-1} \\] The \\(\\pmb{u} = \\{u_i\\}_{i=1}^{m}\\) yielding this first principal component is called its loadings. Note that \\[ s^2_{\\tilde{{\\boldsymbol{x}}}} = \\frac{\\sum_{j=1}^n \\left(\\tilde{x}_j - \\frac{1}{n} \\sum_{k=1}^n \\tilde{x}_k \\right)^2}{n-1} = \\frac{\\sum_{j=1}^n \\tilde{x}^{*2}_j}{n-1} = s^2_{\\tilde{{\\boldsymbol{x}}}^*} \\ , \\] so the loadings can be found from either \\({\\boldsymbol{X}}\\) or \\({\\boldsymbol{X}}^*\\). However, it the technically correct first PC is \\(\\tilde{{\\boldsymbol{x}}}^*\\) rather than \\(\\tilde{{\\boldsymbol{x}}}\\). This first PC is then removed from the data, and the procedure is repeated until all possible sample PCs are constructed. This is accomplished by calculating the product of \\({\\boldsymbol{u}}_{m \\times 1}\\) and \\(\\tilde{\\pmb{x}}^*_{1 \\times n}\\), and subtracting it from \\({\\boldsymbol{X}}^*\\): \\[ {\\boldsymbol{X}}^* - {\\boldsymbol{u}}\\tilde{\\pmb{x}}^* \\ . \\] 21.4 Calculating All PCs All of the PCs can be calculated simultaneously. First, we construct the \\(m \\times m\\) sample covariance matrix \\({\\boldsymbol{S}}\\) with \\((i,j)\\) entry \\[ s_{ij} = \\frac{\\sum_{k=1}^n (x_{ik} - \\bar{x}_{i\\cdot})(x_{jk} - \\bar{x}_{j\\cdot})}{n-1}. \\] The sample covariance can also be calculated by \\[ {\\boldsymbol{S}}= \\frac{1}{n-1} {\\boldsymbol{X}}^{*} {\\boldsymbol{X}}^{*T}. \\] It can be shown that \\[ s^2_{\\tilde{{\\boldsymbol{x}}}^*} = {\\boldsymbol{u}}^T {\\boldsymbol{S}}{\\boldsymbol{u}}, \\] so identifying \\({\\boldsymbol{u}}\\) that maximizes \\(s^2_{\\tilde{{\\boldsymbol{x}}}}\\) also maximizes \\({\\boldsymbol{u}}^T {\\boldsymbol{S}}{\\boldsymbol{u}}\\). Using a Lagrange multiplier, we wish to maximize \\[ {\\boldsymbol{u}}^T {\\boldsymbol{S}}{\\boldsymbol{u}}+ \\lambda({\\boldsymbol{u}}^T {\\boldsymbol{u}}- 1). \\] Differentiating with respect to \\({\\boldsymbol{u}}\\) and setting this to \\({\\boldsymbol{0}}\\), we get \\({\\boldsymbol{S}}{\\boldsymbol{u}}- \\lambda {\\boldsymbol{u}}= 0\\) or \\[ {\\boldsymbol{S}}{\\boldsymbol{u}}= \\lambda {\\boldsymbol{u}}. \\] For any such \\({\\boldsymbol{u}}\\) and \\(\\lambda\\) where this holds, note that \\[ s_{ij} = \\frac{\\sum_{k=1}^n (x_{ik} - \\bar{x}_{i\\cdot})(x_{jk} - \\bar{x}_{j\\cdot})}{n-1} = {\\boldsymbol{u}}^T {\\boldsymbol{S}}{\\boldsymbol{u}}= \\lambda \\] so the PC’s variance is \\(\\lambda\\). The eigendecompositon of a matrix identifies all such solutions to \\({\\boldsymbol{S}}{\\boldsymbol{u}}= \\lambda {\\boldsymbol{u}}.\\) Specifically, it calculates the decompositon \\[ {\\boldsymbol{S}}= {\\boldsymbol{U}}{\\boldsymbol{\\Lambda}}{\\boldsymbol{U}}^T \\] where \\({\\boldsymbol{U}}\\) is an \\(m \\times m\\) orthogonal matrix and \\({\\boldsymbol{\\Lambda}}\\) is a diagonal matrix with entries \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_m \\geq 0\\). The fact that \\({\\boldsymbol{U}}\\) is orthogonal means \\({\\boldsymbol{U}}{\\boldsymbol{U}}^T = {\\boldsymbol{U}}^T {\\boldsymbol{U}}= {\\boldsymbol{I}}\\). The following therefore hold: For each column \\(j\\) of \\({\\boldsymbol{U}}\\), say \\({\\boldsymbol{u}}_j\\), it follows that \\({\\boldsymbol{S}}{\\boldsymbol{u}}_j = \\lambda_j {\\boldsymbol{u}}_j\\) \\(\\| {\\boldsymbol{u}}_j \\|^2_2 = 1\\) and \\({\\boldsymbol{u}}_j^T {\\boldsymbol{u}}_k = {\\boldsymbol{0}}\\) for \\(\\lambda_j \\not= \\lambda_k\\) \\({\\operatorname{Var}}({\\boldsymbol{u}}_j^T {\\boldsymbol{X}}) = \\lambda_j\\) \\({\\operatorname{Var}}({\\boldsymbol{u}}_1^T {\\boldsymbol{X}}) \\geq {\\operatorname{Var}}({\\boldsymbol{u}}_2^T {\\boldsymbol{X}}) \\geq \\cdots \\geq {\\operatorname{Var}}({\\boldsymbol{u}}_m^T {\\boldsymbol{X}})\\) \\({\\boldsymbol{S}}= \\sum_{j=1}^m \\lambda_j {\\boldsymbol{u}}_j {\\boldsymbol{u}}_j^T\\) For \\(\\lambda_j \\not= \\lambda_k\\), \\[{\\operatorname{Cov}}({\\boldsymbol{u}}_j^T {\\boldsymbol{X}}, {\\boldsymbol{u}}_k^T {\\boldsymbol{X}}) = {\\boldsymbol{u}}_j^T {\\boldsymbol{S}}{\\boldsymbol{u}}_k = \\lambda_k {\\boldsymbol{u}}_j^T {\\boldsymbol{u}}_k = {\\boldsymbol{0}}\\] To calculate the actual principal components, let \\(x^*_{ij} = x_{ij} - \\bar{x}_{i\\cdot}\\) be the mean-centered variables. Let \\({\\boldsymbol{X}}^*\\) be the matrix composed of these mean-centered variables. Also, let \\({\\boldsymbol{u}}_j\\) be column \\(j\\) of \\({\\boldsymbol{U}}\\) from \\({\\boldsymbol{S}}= {\\boldsymbol{U}}{\\boldsymbol{\\Lambda}}{\\boldsymbol{U}}^T\\). Sample principal component \\(j\\) is then \\[ \\tilde{{\\boldsymbol{x}}}_j = {\\boldsymbol{u}}_j^T {\\boldsymbol{X}}^* = \\sum_{i=1}^m u_{ij} {\\boldsymbol{x}}^*_i \\] for \\(j = 1, 2, \\ldots, \\min(m, n-1)\\). For \\(j &gt; \\min(m, n-1)\\), we have \\(\\lambda_j = 0\\), so these principal components are not necessary to calculate. The loadings corresponding to PC \\(j\\) are \\({\\boldsymbol{u}}_j\\). Note that the convention is that mean of PC \\(j\\) is zero, i.e., that \\[ \\frac{1}{n} \\sum_{k=1}^n \\tilde{x}_{jk} = 0, \\] but as mentioned above \\(\\sum_{i=1}^m u_{ij} {\\boldsymbol{x}}^*_i\\) and the uncentered \\(\\sum_{i=1}^m u_{ij} {\\boldsymbol{x}}_i\\) have the same sample variance. It can be calculated that the variance of PC \\(j\\) is \\[ s^2_{\\tilde{{\\boldsymbol{x}}}_j} = \\frac{\\sum_{k=1}^n \\tilde{x}_{jk}^2}{n-1} = \\lambda_j. \\] The proportion of variance explained by PC \\(j\\) is \\[ \\operatorname{PVE}_j = \\frac{\\lambda_j}{\\sum_{k=1}^m \\lambda_k}. \\] 21.5 Singular Value Decomposition One way in which PCA is performed is to carry out a singular value decomposition (SVD) of the data matrix \\({\\boldsymbol{X}}\\). Let \\(q = \\min(m, n)\\). Recalling that \\({\\boldsymbol{X}}^*\\) is the row-wise mean centered \\({\\boldsymbol{X}}\\), we can take the SVD of \\({\\boldsymbol{X}}^*/\\sqrt{n-1}\\) to obtain \\[ \\frac{1}{\\sqrt{n-1}} {\\boldsymbol{X}}^* = {\\boldsymbol{U}}{\\boldsymbol{D}}{\\boldsymbol{V}}^T \\] where \\({\\boldsymbol{U}}_{m \\times q}\\), \\({\\boldsymbol{V}}_{n \\times q}\\), and diagonal \\({\\boldsymbol{D}}_{q \\times q}\\). Also, we have the orthogonality properties \\({\\boldsymbol{V}}^T {\\boldsymbol{V}}= {\\boldsymbol{U}}^T {\\boldsymbol{U}}= {\\boldsymbol{I}}_{q}\\). Finally, \\({\\boldsymbol{D}}\\) is composed of diagonal elements \\(d_1 \\geq d_2 \\geq \\cdots \\geq d_q \\geq 0\\) where \\(d_q = 0\\) if \\(q = n\\). Note that \\[ {\\boldsymbol{S}}= \\frac{1}{n-1} {\\boldsymbol{X}}^{*} {\\boldsymbol{X}}^{*T} = {\\boldsymbol{U}}{\\boldsymbol{D}}{\\boldsymbol{V}}^T \\left({\\boldsymbol{U}}{\\boldsymbol{D}}{\\boldsymbol{V}}^T\\right)^T = {\\boldsymbol{U}}{\\boldsymbol{D}}^2 {\\boldsymbol{U}}^T. \\] Therefore: The variance of PC \\(j\\) is \\(\\lambda_j = d_j^2\\) The loadings of PC \\(j\\) are contained in the columns of the left-hand matrix from the decomposition of \\({\\boldsymbol{S}}\\) or \\({\\boldsymbol{X}}^*\\) PC \\(j\\) is row \\(j\\) of \\({\\boldsymbol{D}}{\\boldsymbol{V}}^T\\) 21.6 A Simple PCA Function &gt; pca &lt;- function(x, space=c(&quot;rows&quot;, &quot;columns&quot;), + center=TRUE, scale=FALSE) { + space &lt;- match.arg(space) + if(space==&quot;columns&quot;) {x &lt;- t(x)} + x &lt;- t(scale(t(x), center=center, scale=scale)) + x &lt;- x/sqrt(nrow(x)-1) + s &lt;- svd(x) + loading &lt;- s$u + colnames(loading) &lt;- paste0(&quot;Loading&quot;, 1:ncol(loading)) + rownames(loading) &lt;- rownames(x) + pc &lt;- diag(s$d) %*% t(s$v) + rownames(pc) &lt;- paste0(&quot;PC&quot;, 1:nrow(pc)) + colnames(pc) &lt;- colnames(x) + pve &lt;- s$d^2 / sum(s$d^2) + if(space==&quot;columns&quot;) {pc &lt;- t(pc); loading &lt;- t(loading)} + return(list(pc=pc, loading=loading, pve=pve)) + } The input is as follows: x: a matrix of numerical values space: either &quot;rows&quot; or &quot;columns&quot;, denoting which dimension contains the variables center: if TRUE then the variables are mean centered before calculating PCs scale: if TRUE then the variables are std dev scaled before calculating PCs The output is a list with the following items: pc: a matrix of all possible PCs loading: the weights or “loadings” that determined each PC pve: the proportion of variation explained by each PC Note that the rows or columns of pc and loading have names to let you know on which dimension the values are organized. 21.7 The Ubiquitous PCA Example Here’s an example very frequently encountered to explain PCA, but it’s slightly complicated and conflates several ideas in PCA. I think it’s not a great example to motivate PCA, but it’s so common I want to carefully clarify what it’s displaying. &gt; set.seed(508) &gt; n &lt;- 70 &gt; z &lt;- sqrt(0.8) * rnorm(n) &gt; x1 &lt;- z + sqrt(0.2) * rnorm(n) &gt; x2 &lt;- z + sqrt(0.2) * rnorm(n) &gt; X &lt;- rbind(x1, x2) &gt; p &lt;- pca(x=X, space=&quot;rows&quot;) PCS is often explained by showing the following plot and stating, “The first PC finds the direction of maximal variance in the data…” The above figure was made with the following code: &gt; a1 &lt;- p$loading[1,1] * p$pc[1,] + mean(x1) &gt; a2 &lt;- p$loading[1,2] * p$pc[1,] + mean(x2) &gt; df &lt;- data.frame(x1=c(x1, a1), + x2=c(x2, a2), + legend=c(rep(&quot;data&quot;,n),rep(&quot;pc1_projection&quot;,n))) &gt; ggplot(df) + geom_point(aes(x=x1,y=x2,color=legend)) + + scale_color_manual(values=c(&quot;blue&quot;, &quot;red&quot;)) The red dots are therefore the projection of x1 and x2 onto the first PC, so they are neither the loadings nor the PC. This is rather complicated to understand before loadings and PCs are full understood. Note that there are several ways to calculate these projections. # all equivalent ways to get a1 p$loading[1,1] * p$pc[1,] outer(p$loading[,1], p$pc[1,])[1,] + mean(x1) lm(x1 ~ p$pc[1,])$fit # and # all equivalent ways to get a2 p$loading[2,2] * p$pc[2,] outer(p$loading[,1], p$pc[1,])[2,] + mean(x2) lm(x2 ~ p$pc[1,])$fit We haven’t seen the lm() function yet, but once we do this example will be useful to revisit to understand what is meant by “projection”. Here is PC1 vs PC2: &gt; data.frame(pc1=p$pc[1,], pc2=p$pc[2,]) %&gt;% + ggplot() + geom_point(aes(x=pc1,y=pc2)) + + theme(aspect.ratio=1) Here is PC1 vs x1: &gt; data.frame(pc1=p$pc[1,], x1=x1) %&gt;% + ggplot() + geom_point(aes(x=pc1,y=x1)) + + theme(aspect.ratio=1) Here is PC1 vs x2: &gt; data.frame(pc1=p$pc[1,], x2=x2) %&gt;% + ggplot() + geom_point(aes(x=pc1,y=x2)) + + theme(aspect.ratio=1) Here is PC1 vs z: &gt; data.frame(pc1=p$pc[1,], z=z) %&gt;% + ggplot() + geom_point(aes(x=pc1,y=z)) + + theme(aspect.ratio=1) 21.8 PC Biplots Sometimes it is informative to plot a PC versus another PC. This is called a PC biplot. It is possible that interesting subgroups or clusters of observations will emerge. 21.9 PCA Examples 21.9.1 Weather Data These daily temperature data (in tenths of degrees C) come from meteorogical observations for weather stations in the US for the year 2012 provided by NOAA (National Oceanic and Atmospheric Administration).: &gt; load(&quot;./data/weather_data.RData&quot;) &gt; dim(weather_data) [1] 2811 50 &gt; &gt; weather_data[1:5, 1:7] 11 16 18 19 27 30 31 AG000060611 138.0000 175.0000 173 164.0000 218 160 163.0000 AGM00060369 158.0000 162.0000 154 159.0000 165 125 171.0000 AGM00060425 272.7619 272.7619 152 163.0000 163 108 158.0000 AGM00060444 128.0000 102.0000 100 111.0000 125 33 125.0000 AGM00060468 105.0000 122.0000 97 263.5714 155 52 263.5714 This matrix contains temperature data on 50 days and 2811 stations that were randomly selected. First, we will convert temperatures to Fahrenheit: &gt; weather_data &lt;- 0.18*weather_data + 32 &gt; weather_data[1:5, 1:6] 11 16 18 19 27 30 AG000060611 56.84000 63.50000 63.14 61.52000 71.24 60.80 AGM00060369 60.44000 61.16000 59.72 60.62000 61.70 54.50 AGM00060425 81.09714 81.09714 59.36 61.34000 61.34 51.44 AGM00060444 55.04000 50.36000 50.00 51.98000 54.50 37.94 AGM00060468 50.90000 53.96000 49.46 79.44286 59.90 41.36 &gt; &gt; apply(weather_data, 1, median) %&gt;% + quantile(probs=seq(0,1,0.1)) 0% 10% 20% 30% 40% 50% 8.886744 49.010000 54.500000 58.460000 62.150000 65.930000 60% 70% 80% 90% 100% 69.679318 73.490000 77.990000 82.940000 140.000000 Let’s perform PCA on these data. &gt; mypca &lt;- pca(weather_data, space=&quot;rows&quot;) &gt; &gt; names(mypca) [1] &quot;pc&quot; &quot;loading&quot; &quot;pve&quot; &gt; dim(mypca$pc) [1] 50 50 &gt; dim(mypca$loading) [1] 2811 50 &gt; mypca$pc[1:3, 1:3] 11 16 18 PC1 19.5166741 25.441401 25.9023874 PC2 -2.6025225 -4.310673 0.9707207 PC3 -0.6681223 -1.240748 -3.7276658 &gt; mypca$loading[1:3, 1:3] Loading1 Loading2 Loading3 AG000060611 -0.015172744 0.013033849 -0.011273121 AGM00060369 -0.009439176 0.016884418 -0.004611284 AGM00060425 -0.015779138 0.007026312 -0.009907972 PC1 vs Time: &gt; day_of_the_year &lt;- as.numeric(colnames(weather_data)) &gt; data.frame(day=day_of_the_year, PC1=mypca$pc[1,]) %&gt;% + ggplot() + geom_point(aes(x=day, y=PC1), size=2) PC2 vs Time: &gt; data.frame(day=day_of_the_year, PC2=mypca$pc[2,]) %&gt;% + ggplot() + geom_point(aes(x=day, y=PC2), size=2) PC1 vs PC2 Biplot: This does not appear to be subgroups or clusters in the weather data set biplot of PC1 vs PC2. &gt; data.frame(PC1=mypca$pc[1,], PC2=mypca$pc[2,]) %&gt;% + ggplot() + geom_point(aes(x=PC1, y=PC2), size=2) Proportion of Variance Explained: &gt; data.frame(Component=1:length(mypca$pve), PVE=mypca$pve) %&gt;% + ggplot() + geom_point(aes(x=Component, y=PVE), size=2) We can multiple the loadings matrix by the PCs matrix to reproduce the data: &gt; # mean centered weather data &gt; weather_data_mc &lt;- weather_data - rowMeans(weather_data) &gt; &gt; # difference between the PC projections and the data &gt; # the small sum is just machine imprecision &gt; sum(abs(weather_data_mc/sqrt(nrow(weather_data_mc)-1) - + mypca$loading %*% mypca$pc)) [1] 1.329755e-10 The sum of squared weights – i.e., loadings – equals one for each component: &gt; sum(mypca$loading[,1]^2) [1] 1 &gt; &gt; apply(mypca$loading, 2, function(x) {sum(x^2)}) Loading1 Loading2 Loading3 Loading4 Loading5 Loading6 Loading7 1 1 1 1 1 1 1 Loading8 Loading9 Loading10 Loading11 Loading12 Loading13 Loading14 1 1 1 1 1 1 1 Loading15 Loading16 Loading17 Loading18 Loading19 Loading20 Loading21 1 1 1 1 1 1 1 Loading22 Loading23 Loading24 Loading25 Loading26 Loading27 Loading28 1 1 1 1 1 1 1 Loading29 Loading30 Loading31 Loading32 Loading33 Loading34 Loading35 1 1 1 1 1 1 1 Loading36 Loading37 Loading38 Loading39 Loading40 Loading41 Loading42 1 1 1 1 1 1 1 Loading43 Loading44 Loading45 Loading46 Loading47 Loading48 Loading49 1 1 1 1 1 1 1 Loading50 1 PCs by contruction have sample correlation equal to zero: &gt; cor(mypca$pc[1,], mypca$pc[2,]) [1] 3.135149e-17 &gt; cor(mypca$pc[1,], mypca$pc[3,]) [1] 2.273613e-16 &gt; cor(mypca$pc[1,], mypca$pc[12,]) [1] -1.231339e-16 &gt; cor(mypca$pc[5,], mypca$pc[27,]) [1] -2.099516e-17 &gt; # etc... I can transform the top PC back to the original units to display it at a scale that has a more direct interpretation. &gt; day_of_the_year &lt;- as.numeric(colnames(weather_data)) &gt; y &lt;- -mypca$pc[1,] + mean(weather_data) &gt; data.frame(day=day_of_the_year, max_temp=y) %&gt;% + ggplot() + geom_point(aes(x=day, y=max_temp)) 21.9.2 Yeast Gene Expression Yeast cells were synchronized so that they were on the same approximate cell cycle timing in Spellman et al. (1998). The goal was to understand how gene expression varies over the cell cycle from a genome-wide perspective. &gt; load(&quot;./data/spellman.RData&quot;) &gt; time [1] 0 30 60 90 120 150 180 210 240 270 330 360 390 &gt; dim(gene_expression) [1] 5981 13 &gt; gene_expression[1:6,1:5] 0 30 60 90 120 YAL001C 0.69542786 -0.4143538 3.2350520 1.6323737 -2.1091820 YAL002W -0.01210662 3.0465649 1.1062193 4.0591467 -0.1166399 YAL003W -2.78570526 -1.0156981 -2.1387564 1.9299681 0.7797033 YAL004W 0.55165887 0.6590093 0.5857847 0.3890409 -1.0009777 YAL005C -0.53191556 0.1577985 -1.2401448 0.8170350 -1.3520947 YAL007C -0.86693416 -1.1642322 -0.6359588 1.1179131 1.9587021 Proportion Variance Explained: &gt; p &lt;- pca(gene_expression, space=&quot;rows&quot;) &gt; ggplot(data.frame(pc=1:13, pve=p$pve)) + + geom_point(aes(x=pc,y=pve), size=2) PCs vs Time (with Smoothers): 21.9.3 HapMap Genotypes I curated a small data set that cleanly separates human subpopulations from the HapMap data. These include unrelated individuals from Yoruba people from Ibadan, Nigeria (YRI), Utah residents of northern and western European ancestry (CEU), Japanese individuals from Tokyo, Japan (JPT), and Han Chinese individuals from Beijing, China (CHB). &gt; hapmap &lt;- read.table(&quot;./data/hapmap_sample.txt&quot;) &gt; dim(hapmap) [1] 400 24 &gt; hapmap[1:6,1:6] NA18516 NA19138 NA19137 NA19223 NA19200 NA19131 rs2051075 0 1 2 1 1 1 rs765546 2 2 0 0 0 0 rs10019399 2 2 2 1 1 2 rs7055827 2 2 1 2 0 2 rs6943479 0 0 2 0 1 0 rs2095381 1 2 1 2 1 1 Proportion Variance Explained: &gt; p &lt;- pca(hapmap, space=&quot;rows&quot;) &gt; ggplot(data.frame(pc=(1:ncol(hapmap)), pve=p$pve)) + + geom_point(aes(x=pc,y=pve), size=2) PC1 vs PC2 Biplot: PC1 vs PC3 Biplot: PC2 vs PC3 Biplot: "],
["probability-and-statistics.html", "22 Probability and Statistics 22.1 Central Dogma of Inference 22.2 Data Analysis Without Probability", " 22 Probability and Statistics Probabilistic modeling and/or statistical inference are required when the goals include: Characterizing randomness or “noise” in the data Quantifying uncertainty in models we build or decisions we make from the data Predicting future observations or decisions in the face of uncertainty 22.1 Central Dogma of Inference Figure 22.1: Central Dogma of Statistical Inference 22.2 Data Analysis Without Probability It is possible to do data analysis without probability and formal statistical inference: Descriptive statistics can be reported without utilizing probability and statistical inference Exploratory data analysis and visualization tend to not involve probability or formal statistical inference Important problems in machine learning do not involve probability or statistical inference. "],
["probability-theory.html", "23 Probability Theory 23.1 Sample Space 23.2 Measure Theoretic Probabilty 23.3 Mathematical Probability 23.4 Union of Two Events 23.5 Conditional Probability 23.6 Independence 23.7 Bayes Theorem 23.8 Law of Total Probability", " 23 Probability Theory 23.1 Sample Space The sample space \\(\\Omega\\) is the set of all outcomes We are interested in calculating probabilities on relevant subsets of this space, called events: \\(A \\subseteq \\Omega\\) Examples — Two coin flips: \\(\\Omega =\\) {HH, HT, TH, TT} SNP genotypes: \\(\\Omega =\\) {AA, AT, TT} Amazon product rating: \\(\\Omega =\\) {1 star, 2 stars, …, 5 stars} Political survey: \\(\\Omega =\\) {agree, disagree} 23.2 Measure Theoretic Probabilty \\[(\\Omega, \\mathcal{F}, \\Pr)\\] \\(\\Omega\\) is the sample space \\(\\mathcal{F}\\) is the \\(\\sigma\\)-algebra of events where probability can be measured \\(\\Pr\\) is the probability measure 23.3 Mathematical Probability A proper mathematical formulation of a probability measure should include the following properties: The probability of any even \\(A\\) is such that \\(0 \\leq \\Pr(A) \\leq 1\\) If \\(\\Omega\\) is the sample space then \\(\\Pr(\\Omega)=1\\) Let \\(A^c\\) be all outcomes from \\(\\Omega\\) that are not in \\(A\\) (called the complement); then \\(\\Pr(A) + \\Pr(A^c) = 1\\) For any \\(n\\) events such that \\(A_i \\cap A_j = \\varnothing\\) for all \\(i \\not= j\\), then \\(\\Pr\\left( \\cup_{i=1}^n A_i \\right) = \\sum_{i=1}^n \\Pr(A_i)\\), where \\(\\varnothing\\) is the empty set 23.4 Union of Two Events The probability of two events are calculated by the following general relationship: \\[\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)\\] where we note that \\(\\Pr(A \\cap B)\\) gets counted twice in \\(\\Pr(A) + \\Pr(B)\\). 23.5 Conditional Probability An important calclation in probability and statistics is the conditional probability. We can consider the probability of an event \\(A\\), conditional on the fact that we are restricted to be within event \\(B\\). This is defined as: \\[\\Pr(A | B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}\\] 23.6 Independence Two events \\(A\\) and \\(B\\) by definition independent when: \\(\\Pr(A | B) = \\Pr(A)\\) \\(\\Pr(B | A) = \\Pr(B)\\) \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\) All three of these are equivalent. 23.7 Bayes Theorem A common approach in statistics is to obtain a conditional probability of two events through the opposite conditional probability and their marginal probability. This is called Bayes Theorem: \\[\\Pr(B | A) = \\frac{\\Pr(A | B)\\Pr(B)}{\\Pr(A)}\\] This forms the basis of Bayesian Inference but has more general use in carrying out probability calculations. 23.8 Law of Total Probability For events \\(A_1, \\ldots, A_n\\) such that \\(A_i \\cap A_j = \\varnothing\\) for all \\(i \\not= j\\) and \\(\\cup_{i=1}^n A_i = \\Omega\\), it follows that for any event \\(B\\): \\[\\Pr(B) = \\sum_{i=1}^n \\Pr(B | A_i) \\Pr(A_i).\\] "],
["random-variables.html", "24 Random Variables 24.1 Definition 24.2 Distributon of RV 24.3 Discrete Random Variables 24.4 Example: Discrete PMF 24.5 Example: Discrete CDF 24.6 Probabilities of Events Via Discrete CDF 24.7 Continuous Random Variables 24.8 Example: Continuous PDF 24.9 Example: Continuous CDF 24.10 Probabilities of Events Via Continuous CDF 24.11 Example: Continuous RV Event 24.12 Note on PMFs and PDFs 24.13 Note on CDFs 24.14 Sample Vs Population Statistics 24.15 Expected Value 24.16 Variance 24.17 Covariance 24.18 Correlation 24.19 Moment Generating Functions 24.20 Random Variables in R", " 24 Random Variables 24.1 Definition A random variable \\(X\\) is a function from \\(\\Omega\\) to the real numbers: \\[X: \\Omega \\rightarrow \\mathbb{R}\\] For any outcome in \\(\\Omega\\), the function \\(X(\\omega)\\) produces a real value. We will write the range of \\(X\\) as \\[\\mathcal{R} = \\{X(\\omega): \\omega \\in \\Omega\\}\\] where \\(\\mathcal{R} \\subseteq \\mathbb{R}\\). 24.2 Distributon of RV We define the probability distribution of a random variable through its probability mass function (pmf) for discrete rv’s or its probability density function (pdf) for continuous rv’s. We can also define the distribution through its cumulative distribution function (cdf). The pmf/pdf determines the cdf, and vice versa. 24.3 Discrete Random Variables A discrete rv \\(X\\) takes on a discrete set of values such as \\(\\{1, 2, \\ldots, n\\}\\) or \\(\\{0, 1, 2, 3, \\ldots \\}\\). Its distribution is characterized by its pmf \\[f(x) = \\Pr(X = x)\\] for \\(x \\in \\{X(\\omega): \\omega \\in \\Omega \\}\\) and \\(f(x) = 0\\) otherwise. Its cdf is \\[F(y) = \\Pr(X \\leq y) = \\sum_{x \\leq y} \\Pr(X = x)\\] for \\(y \\in \\mathbb{R}\\). 24.4 Example: Discrete PMF 24.5 Example: Discrete CDF 24.6 Probabilities of Events Via Discrete CDF Examples: Probability CDF PMF \\(\\Pr(X \\leq b)\\) \\(F(b)\\) \\(\\sum_{x \\leq b} f(x)\\) \\(\\Pr(X \\geq a)\\) \\(1-F(a-1)\\) \\(\\sum_{x \\geq a} f(x)\\) \\(\\Pr(X &gt; a)\\) \\(1-F(a)\\) \\(\\sum_{x &gt; a} f(x)\\) \\(\\Pr(a \\leq X \\leq b)\\) \\(F(b) - F(a-1)\\) \\(\\sum_{a \\leq x \\leq b} f(x)\\) \\(\\Pr(a &lt; X \\leq b)\\) \\(F(b) - F(a)\\) \\(\\sum_{a &lt; x \\leq b} f(x)\\) 24.7 Continuous Random Variables A continuous rv \\(X\\) takes on a continuous set of values such as \\([0, \\infty)\\) or \\(\\mathbb{R} = (-\\infty, \\infty)\\). The probability that \\(X\\) takes on any specific value is 0; but the probability it lies within an interval can be non-zero. Its pdf \\(f(x)\\) therefore gives an infinitesimal, local, relative probability. Its cdf is \\[F(y) = \\Pr(X \\leq y) = \\int_{-\\infty}^y f(x) dx\\] for \\(y \\in \\mathbb{R}\\). 24.8 Example: Continuous PDF 24.9 Example: Continuous CDF 24.10 Probabilities of Events Via Continuous CDF Examples: Probability CDF PDF \\(\\Pr(X \\leq b)\\) \\(F(b)\\) \\(\\int_{-\\infty}^{b} f(x) dx\\) \\(\\Pr(X \\geq a)\\) \\(1-F(a)\\) \\(\\int_{a}^{\\infty} f(x) dx\\) \\(\\Pr(X &gt; a)\\) \\(1-F(a)\\) \\(\\int_{a}^{\\infty} f(x) dx\\) \\(\\Pr(a \\leq X \\leq b)\\) \\(F(b) - F(a)\\) \\(\\int_{a}^{b} f(x) dx\\) \\(\\Pr(a &lt; X \\leq b)\\) \\(F(b) - F(a)\\) \\(\\int_{a}^{b} f(x) dx\\) 24.11 Example: Continuous RV Event 24.12 Note on PMFs and PDFs PMFs and PDFs are defined as \\(f(x)=0\\) outside of the range of \\(X\\), \\(\\mathcal{R} = \\{X(\\omega): \\omega \\in \\Omega\\}\\). That is: Also, they sum or integrate to 1: \\[\\sum_{x \\in \\mathcal{R}} f(x) = 1\\] \\[\\int_{x \\in \\mathcal{R}} f(x) dx = 1\\] Using measure theory, we can consider both types of rv’s in one framework, and we would write: \\[\\int_{-\\infty}^{\\infty} dF(x) = 1\\] 24.13 Note on CDFs Properties of all cdf’s, regardless of continuous or discrete underlying rv: They are right continuous with left limits \\(\\lim_{x \\rightarrow \\infty} F(x) = 1\\) \\(\\lim_{x \\rightarrow -\\infty} F(x) = 0\\) The right derivative of \\(F(x)\\) equals \\(f(x)\\) 24.14 Sample Vs Population Statistics We earlier discussed measures of center and spread for a set of data, such as the mean and the variance. Analogous measures exist for probability distributions. These are distinguished by calling those on data “sample” measures (e.g., sample mean) and those on probability distributions “population” measures (e.g., population mean). 24.15 Expected Value The expected value, also called the “population mean”, is a measure of center for a rv. It is calculated in a fashion analogous to the sample mean: \\[\\begin{align*} &amp; \\operatorname{E}[X] = \\sum_{x \\in \\mathcal{R}} x \\ f(x) &amp; \\mbox{(discrete)} \\\\ &amp; \\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x \\ f(x) \\ dx &amp; \\mbox{(continuous)} \\\\ &amp; \\operatorname{E}[X] = \\int_{-\\infty}^{\\infty} x \\ dF(x) &amp; \\mbox{(general)} \\end{align*}\\] 24.16 Variance The variance, also called the “population variance”, is a measure of spread for a rv. It is calculated in a fashion analogous to the sample variance: \\[{\\operatorname{Var}}(X) = {\\operatorname{E}}\\left[\\left(X-{\\operatorname{E}}[X]\\right)^2\\right] = {\\operatorname{E}}[X^2] - E[X]^2\\] \\[{\\rm SD}(X) = \\sqrt{{\\operatorname{Var}}(X)}\\] \\[{\\operatorname{Var}}(X) = \\sum_{x \\in \\mathcal{R}} \\left(x-{\\operatorname{E}}[X]\\right)^2 \\ f(x) \\ \\ \\ \\ \\mbox{(discrete)}\\] \\[{\\operatorname{Var}}(X) = \\int_{-\\infty}^{\\infty} \\left(x-{\\operatorname{E}}[X]\\right)^2 \\ f(x) \\ dx \\ \\ \\ \\ \\mbox{(continuous)}\\] 24.17 Covariance The covariance, also called the “population covariance”, measures how two rv’s covary. It is calculated in a fashion analogous to the sample covariance: \\[{\\operatorname{Cov}}(X, Y) = \\operatorname{E} \\left[ (X - \\operatorname{E}[X]) (Y - \\operatorname{E}[Y]) \\right]\\] Note that \\({\\operatorname{Cov}}(X, X) = {\\operatorname{Var}}(X)\\). 24.18 Correlation The population correlation is calculated analogously to the sample correlation: \\[\\operatorname{Cor}(X, Y) = \\frac{{\\operatorname{Cov}}(X, Y)}{\\operatorname{SD}(X)\\operatorname{SD}(Y)}\\] 24.19 Moment Generating Functions The moment generating function (mgf) of a rv is defined to be \\[m(t) = \\operatorname{E}\\left[e^{tX}\\right]\\] whenever this expecation exists. Under certain conditions, the moments of a rv can then be obtained by: \\[\\operatorname{E} \\left[ X^k \\right] = \\frac{d^k}{dt^k}m(0).\\] 24.20 Random Variables in R The pmf/pdf, cdf, quantile function, and random number generator for many important random variables are built into R. They all follow the form, where &lt;name&gt; is replaced with the name used in R for each specific distribution: d&lt;name&gt;: pmf or pdf p&lt;name&gt;: cdf q&lt;name&gt;: quantile function or inverse cdf r&lt;name&gt;: random number generator To see a list of random variables, type ?Distributions in R. "],
["discrete-rvs.html", "25 Discrete RVs 25.1 Uniform (Discrete) 25.2 Uniform (Discrete) PMF 25.3 Uniform (Discrete) in R 25.4 Bernoulli 25.5 Binomial 25.6 Binomial PMF 25.7 Binomial in R 25.8 Poisson 25.9 Poisson PMF 25.10 Poisson in R", " 25 Discrete RVs 25.1 Uniform (Discrete) This simple rv distribution assigns equal probabilities to a finite set of values: \\[X \\sim \\mbox{Uniform}\\{1, 2, \\ldots, n\\}\\] \\[\\mathcal{R} = \\{1, 2, \\ldots, n\\}\\] \\[f(x; n) = 1/n \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = \\frac{n+1}{2}, \\ {\\operatorname{Var}}(X) = \\frac{n^2-1}{12}\\] 25.2 Uniform (Discrete) PMF 25.3 Uniform (Discrete) in R There is no family of functions built into R for this distribution since it is so simple. However, it is possible to generate random values via the sample function: &gt; n &lt;- 20L &gt; sample(x=1:n, size=10, replace=TRUE) [1] 8 19 4 1 18 15 18 18 2 7 &gt; &gt; x &lt;- sample(x=1:n, size=1e6, replace=TRUE) &gt; mean(x) - (n+1)/2 [1] 0.006991 &gt; var(x) - (n^2-1)/12 [1] 0.0208284 25.4 Bernoulli A single success/failure event, such as heads/tails when flipping a coin or survival/death. \\[X \\sim \\mbox{Bernoulli}(p)\\] \\[\\mathcal{R} = \\{0, 1\\}\\] \\[f(x; p) = p^x (1-p)^{1-x} \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = p, \\ {\\operatorname{Var}}(X) = p(1-p)\\] 25.5 Binomial An extension of the Bernoulli distribution to simultaneously considering \\(n\\) independent success/failure trials and counting the number of successes. \\[X \\sim \\mbox{Binomial}(n, p)\\] \\[\\mathcal{R} = \\{0, 1, 2, \\ldots, n\\}\\] \\[f(x; p) = {n \\choose x} p^x (1-p)^{n-x} \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = np, \\ {\\operatorname{Var}}(X) = np(1-p)\\] Note that \\({n \\choose x} = \\frac{n!}{x! (n-x)!}\\) is the number of unique ways to choose \\(x\\) items from \\(n\\) without respect to order. 25.6 Binomial PMF 25.7 Binomial in R &gt; str(dbinom) function (x, size, prob, log = FALSE) &gt; str(pbinom) function (q, size, prob, lower.tail = TRUE, log.p = FALSE) &gt; str(qbinom) function (p, size, prob, lower.tail = TRUE, log.p = FALSE) &gt; str(rbinom) function (n, size, prob) 25.8 Poisson Models the number of occurrences of something within a defined time/space period, where the occurrences are independent. Examples: the number of lightning strikes on campus in a given year; the number of emails received on a given day. \\[X \\sim \\mbox{Poisson}(\\lambda)\\] \\[\\mathcal{R} = \\{0, 1, 2, 3, \\ldots \\}\\] \\[f(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = \\lambda, \\ {\\operatorname{Var}}(X) = \\lambda\\] 25.9 Poisson PMF 25.10 Poisson in R &gt; str(dpois) function (x, lambda, log = FALSE) &gt; str(ppois) function (q, lambda, lower.tail = TRUE, log.p = FALSE) &gt; str(qpois) function (p, lambda, lower.tail = TRUE, log.p = FALSE) &gt; str(rpois) function (n, lambda) "],
["continuous-rvs.html", "26 Continuous RVs 26.1 Uniform (Continuous) 26.2 Uniform (Continuous) PDF 26.3 Uniform (Continuous) in R 26.4 Exponential 26.5 Exponential PDF 26.6 Exponential in R 26.7 Beta 26.8 Beta PDF 26.9 Beta in R 26.10 Normal 26.11 Normal PDF 26.12 Normal in R", " 26 Continuous RVs 26.1 Uniform (Continuous) Models the scenario where all values in the unit interval [0,1] are equally likely. \\[X \\sim \\mbox{Uniform}(0,1)\\] \\[\\mathcal{R} = [0,1]\\] \\[f(x) = 1 \\mbox{ for } x \\in \\mathcal{R}\\] \\[F(y) = y \\mbox{ for } y \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = 1/2, \\ {\\operatorname{Var}}(X) = 1/12\\] 26.2 Uniform (Continuous) PDF 26.3 Uniform (Continuous) in R &gt; str(dunif) function (x, min = 0, max = 1, log = FALSE) &gt; str(punif) function (q, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(qunif) function (p, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(runif) function (n, min = 0, max = 1) 26.4 Exponential Models a time to failure and has a “memoryless property”. \\[X \\sim \\mbox{Exponential}(\\lambda)\\] \\[\\mathcal{R} = [0, \\infty)\\] \\[f(x; \\lambda) = \\lambda e^{-\\lambda x} \\mbox{ for } x \\in \\mathcal{R}\\] \\[F(y; \\lambda) = 1 - e^{-\\lambda y} \\mbox{ for } y \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = \\frac{1}{\\lambda}, \\ {\\operatorname{Var}}(X) = \\frac{1}{\\lambda^2}\\] 26.5 Exponential PDF 26.6 Exponential in R &gt; str(dexp) function (x, rate = 1, log = FALSE) &gt; str(pexp) function (q, rate = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(qexp) function (p, rate = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(rexp) function (n, rate = 1) 26.7 Beta Yields values in \\((0,1)\\), so often used to generate random probabilities, such as the \\(p\\) in Bernoulli\\((p)\\). \\[X \\sim \\mbox{Beta}(\\alpha,\\beta) \\mbox{ where } \\alpha, \\beta &gt; 0\\] \\[\\mathcal{R} = (0,1)\\] \\[f(x; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta - 1} \\mbox{ for } x \\in \\mathcal{R}\\] where \\(\\Gamma(z) = \\int_{0}^{\\infty} x^{z-1} e^{-x} dx\\). \\[{\\operatorname{E}}[X] = \\frac{\\alpha}{\\alpha + \\beta}, \\ {\\operatorname{Var}}(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\] 26.8 Beta PDF 26.9 Beta in R &gt; str(dbeta) #shape1=alpha, shape2=beta function (x, shape1, shape2, ncp = 0, log = FALSE) &gt; str(pbeta) function (q, shape1, shape2, ncp = 0, lower.tail = TRUE, log.p = FALSE) &gt; str(qbeta) function (p, shape1, shape2, ncp = 0, lower.tail = TRUE, log.p = FALSE) &gt; str(rbeta) function (n, shape1, shape2, ncp = 0) 26.10 Normal Due to the Central Limit Theorem (covered later), this “bell curve” distribution is often observed in properly normalized real data. \\[X \\sim \\mbox{Normal}(\\mu, \\sigma^2)\\] \\[\\mathcal{R} = (-\\infty, \\infty)\\] \\[f(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}} \\mbox{ for } x \\in \\mathcal{R}\\] \\[{\\operatorname{E}}[X] = \\mu, \\ {\\operatorname{Var}}(X) = \\sigma^2\\] 26.11 Normal PDF 26.12 Normal in R &gt; str(dnorm) #notice it requires the STANDARD DEVIATION, not the variance function (x, mean = 0, sd = 1, log = FALSE) &gt; str(pnorm) function (q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(qnorm) function (p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) &gt; str(rnorm) function (n, mean = 0, sd = 1) "],
["sums-of-random-variables.html", "27 Sums of Random Variables 27.1 Linear Transformation of a RV 27.2 Sums of Independent RVs 27.3 Sums of Dependent RVs 27.4 Means of Random Variables", " 27 Sums of Random Variables 27.1 Linear Transformation of a RV Suppose that \\(X\\) is a random variable and that \\(a\\) and \\(b\\) are constants. Then: \\[{\\operatorname{E}}\\left[a + bX \\right] = a + b {\\operatorname{E}}[X]\\] \\[{\\operatorname{Var}}\\left(a + bX \\right) = b^2 {\\operatorname{Var}}(X)\\] 27.2 Sums of Independent RVs If \\(X_1, X_2, \\ldots, X_n\\) are independent random variables, then: \\[{\\operatorname{E}}\\left[ \\sum_{i=1}^n X_i \\right] = \\sum_{i=1}^n {\\operatorname{E}}[X_i]\\] \\[{\\operatorname{Var}}\\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n {\\operatorname{Var}}(X_i)\\] 27.3 Sums of Dependent RVs If \\(X_1, X_2, \\ldots, X_n\\) are independent random variables, then: \\[{\\operatorname{E}}\\left[ \\sum_{i=1}^n X_i \\right] = \\sum_{i=1}^n {\\operatorname{E}}[X_i]\\] \\[{\\operatorname{Var}}\\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n {\\operatorname{Var}}(X_i) + \\sum_{i \\not= j} {\\operatorname{Cov}}(X_i, X_j)\\] 27.4 Means of Random Variables Suppose \\(X_1, X_2, \\ldots, X_n\\) are independent and identically distributed (iid) random variables. Let \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) be their sample mean. Then: \\[{\\operatorname{E}}\\left[\\overline{X}_n \\right] = {\\operatorname{E}}[X_i]\\] \\[{\\operatorname{Var}}\\left(\\overline{X}_n \\right) = \\frac{1}{n}{\\operatorname{Var}}(X_i)\\] "],
["convergence-of-random-variables.html", "28 Convergence of Random Variables 28.1 Sequence of RVs 28.2 Convergence in Distribution 28.3 Convergence in Probability 28.4 Almost Sure Convergence 28.5 Strong Law of Large Numbers 28.6 Central Limit Theorem 28.7 Example: Calculations 28.8 Example: Plot", " 28 Convergence of Random Variables 28.1 Sequence of RVs Let \\(Z_1, Z_2, \\ldots\\) be an infinite sequence of rv’s. An important example is \\[Z_n = \\overline{X}_n = \\frac{\\sum_{i=1}^n X_i}{n}.\\] It is useful to be able to determine a limiting value or distribution of \\(\\{Z_i\\}\\). 28.2 Convergence in Distribution \\(\\{Z_i\\}\\) converges in distribution to \\(Z\\), written \\[Z_n \\stackrel{D}{\\longrightarrow} Z\\] if \\[F_{Z_n}(y) = \\Pr(Z_n \\leq y) \\rightarrow \\Pr(Z \\leq y) = F_{Z}(y)\\] as \\(n \\rightarrow \\infty\\) for all \\(y \\in \\mathbb{R}\\). 28.3 Convergence in Probability \\(\\{Z_i\\}\\) converges in probability to \\(Z\\), written \\[Z_n \\stackrel{P}{\\longrightarrow} Z\\] if \\[\\Pr(|Z_n - Z| \\leq \\epsilon) \\rightarrow 1\\] as \\(n \\rightarrow \\infty\\) for all \\(\\epsilon &gt; 0\\). Note that it may also be the case that \\(Z_n \\stackrel{P}{\\longrightarrow} \\theta\\) for a fixed, nonrandom value \\(\\theta\\). 28.4 Almost Sure Convergence \\(\\{Z_i\\}\\) converges almost surely (or “with probability 1”) to \\(Z\\), written \\[Z_n \\stackrel{a.s.}{\\longrightarrow} Z\\] if \\[\\Pr\\left(\\{\\omega: |Z_n(\\omega) - Z(\\omega)| \\stackrel{n \\rightarrow \\infty}{\\longrightarrow} 0 \\}\\right) = 1.\\] Note that it may also be the case that \\(Z_n \\stackrel{a.s.}{\\longrightarrow} \\theta\\) for a fixed, nonrandom value \\(\\theta\\). 28.5 Strong Law of Large Numbers Suppose \\(X_1, X_2, \\ldots, X_n\\) are iid rv’s with population mean \\({\\operatorname{E}}[X_i] = \\mu\\) where \\({\\operatorname{E}}[|X_i|] &lt; \\infty\\). Then \\[\\overline{X}_n \\stackrel{a.s.}{\\longrightarrow} \\mu.\\] 28.6 Central Limit Theorem Suppose \\(X_1, X_2, \\ldots, X_n\\) are iid rv’s with population mean \\({\\operatorname{E}}[X_i] = \\mu\\) and variance \\({\\operatorname{Var}}(X_i) = \\sigma^2\\). Then as \\(n \\rightarrow \\infty\\), \\[\\sqrt{n}(\\overline{X}_n - \\mu) \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0, \\sigma^2)\\] \\[\\frac{\\overline{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0, 1)\\] 28.7 Example: Calculations Let \\(X_1, X_2, \\ldots, X_{40}\\) be iid Poisson(\\(\\lambda\\)) with \\(\\lambda=6\\). We will form \\(\\sqrt{40}(\\overline{X} - 6)\\) over 10,000 realizations and compare their distribution to a Normal(0, 6) distribution. &gt; x &lt;- replicate(n=1e4, expr=rpois(n=40, lambda=6), + simplify=&quot;matrix&quot;) &gt; x_bar &lt;- apply(x, 2, mean) &gt; clt &lt;- sqrt(40)*(x_bar - 6) &gt; &gt; df &lt;- data.frame(clt=clt, x = seq(-18,18,length.out=1e4), + y = dnorm(seq(-18,18,length.out=1e4), + sd=sqrt(6))) 28.8 Example: Plot &gt; ggplot(data=df) + + geom_histogram(aes(x=clt, y=..density..), color=&quot;blue&quot;, + fill=&quot;lightgray&quot;, binwidth=0.75) + + geom_line(aes(x=x, y=y), size=1.5) "],
["joint-distributions.html", "29 Joint Distributions 29.1 Bivariate Random Variables 29.2 Events for Bivariate RVs 29.3 Marginal Distributions 29.4 Independent Random Variables 29.5 Conditional Distributions 29.6 Conditional Moments 29.7 Law of Total Variance 29.8 Multivariate Distributions 29.9 MV Expected Value 29.10 MV Variance-Covariance Matrix 29.11 Population PCA", " 29 Joint Distributions 29.1 Bivariate Random Variables For a pair of rv’s \\(X\\) and \\(Y\\) defined on the same probability space, we can define their joint pmf or pdf. For the discrete case, \\[\\begin{align*} f(x, y) &amp; = \\Pr(\\{\\omega: X(\\omega) = x\\} \\cap \\{\\omega: Y(\\omega) = y\\}) \\\\ \\ &amp; = \\Pr(X=x, Y=y). \\end{align*}\\] The joint pdf is defined analogously for continuous rv’s. 29.2 Events for Bivariate RVs Let \\(A_x \\times A_y \\subseteq \\mathbb{R} \\times \\mathbb{R}\\) be an event. Then \\(\\Pr(X \\in A_x, Y \\in A_y)\\) is calculated by: \\[\\begin{align*} &amp; \\sum_{x \\in A_x} \\sum_{y \\in A_y} f(x, y) &amp; \\mbox{(discrete)} \\\\ &amp; \\int_{x \\in A_x} \\int_{y \\in A_y} f(x, y) dy dx &amp; \\mbox{(continuous)} \\\\ &amp; \\int_{x \\in A_x} \\int_{y \\in A_y} dF_Y(y) dF_{X}(x) &amp; \\mbox{(general)} \\end{align*}\\] 29.3 Marginal Distributions We can calculate the marginal distribution of \\(X\\) (or \\(Y\\)) from their joint distribution: \\[f(x) = \\sum_{y \\in \\mathcal{R}_y} f(x, y)\\] \\[f(x) = \\int_{-\\infty}^{\\infty} f(x, y) dy\\] 29.4 Independent Random Variables Two rv’s are independent when their joint pmf or pdf factor: \\[f(x,y) = f(x) f(y)\\] This means, for example, in the continuous case, \\[\\begin{align*} \\Pr(X \\in A_x, Y \\in A_y) &amp; = \\int_{x \\in A_x} \\int_{y \\in A_y} f(x, y) dy dx \\\\ \\ &amp; = \\int_{x \\in A_x} \\int_{y \\in A_y} f(x) f(y) dy dx \\\\ \\ &amp; = \\Pr(X \\in A_x) \\Pr(Y \\in A_y) \\end{align*}\\] 29.5 Conditional Distributions We can define the conditional distribution of \\(X\\) given \\(Y\\) as follows. The conditional rv \\(X | Y \\sim F_{X|Y}\\) with conditional pmf or pdf for \\(X | Y=y\\) given by \\[ f(x | y) = \\frac{f(x, y)}{f(y)}. \\] 29.6 Conditional Moments The \\(k\\)th conditional moment (when it exists) is calculated by: \\[{\\operatorname{E}}\\left[X^k | Y=y\\right] = \\sum_{x \\in \\mathcal{R}_x} x^k f(x | y)\\] \\[{\\operatorname{E}}\\left[X^k | Y=y\\right] = \\int_{-\\infty}^{\\infty} x^k f(x | y) dx\\] Note that \\({\\operatorname{E}}\\left[X^k | Y\\right]\\) is a random variable that is a function of \\(Y\\) whose distribution is determined by that of \\(Y\\). 29.7 Law of Total Variance We can partition the variance of \\(X\\) according to the following conditional calculations on \\(Y\\): \\[{\\operatorname{Var}}(X) = {\\operatorname{Var}}({\\operatorname{E}}[X | Y]) + {\\operatorname{E}}[{\\operatorname{Var}}(X | Y)].\\] This is a useful result for partitioning variation in modeling fitting. 29.8 Multivariate Distributions Let \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_n)^T\\) be a vector of \\(n\\) rv’s. We also let realiozed values be \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_n)^T\\). The joint pmf or pdf is written as \\[f(\\boldsymbol{x}) = f(x_1, x_2, \\ldots, x_n)\\] and if the rv’s are independent then \\[f(\\boldsymbol{x}) = \\prod_{i=1}^{n} f(x_i).\\] 29.9 MV Expected Value The expected value of \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_n)^T\\) is an \\(n\\)-vector: \\[{\\operatorname{E}}[\\boldsymbol{X}] = \\begin{bmatrix} {\\operatorname{E}}[X_1] \\\\ {\\operatorname{E}}[X_2] \\\\ \\vdots \\\\ {\\operatorname{E}}[X_n] \\end{bmatrix} \\] 29.10 MV Variance-Covariance Matrix The variance-covariance matrix of \\(\\boldsymbol{X}\\) is an \\(n \\times n\\) matrix with \\((i, j)\\) entry equal to \\({\\operatorname{Cov}}(X_i, X_j)\\). \\[{\\operatorname{Var}}(\\boldsymbol{X}) = \\begin{bmatrix} {\\operatorname{Var}}(X_1) &amp; {\\operatorname{Cov}}(X_1, X_2) &amp; \\cdots &amp; {\\operatorname{Cov}}(X_1, X_n) \\\\ {\\operatorname{Cov}}(X_2, X_1) &amp; {\\operatorname{Var}}(X_2) &amp; \\cdots &amp; \\vdots \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ {\\operatorname{Cov}}(X_n, X_1) &amp; \\cdots &amp; &amp; {\\operatorname{Var}}(X_n) \\end{bmatrix} \\] Often times \\(\\boldsymbol{\\Sigma}\\) is used as the matrix of population covariances in \\({\\operatorname{Var}}(\\boldsymbol{X})\\) so that \\(\\boldsymbol{\\Sigma}={\\operatorname{Var}}(\\boldsymbol{X})\\). 29.11 Population PCA Suppose we have \\(m\\) random variables \\(X_1, X_2, \\ldots, X_m\\). We wish to identify a set of weights \\(w_1, w_2, \\ldots, w_m\\) that maximizes \\[ {\\operatorname{Var}}\\left(w_1 X_1 + w_2 X_2 + \\cdots + w_m X_m \\right). \\] However, this is unbounded, so we need to constrain the weights. It turns out that constraining the weights so that \\[ \\| {\\boldsymbol{w}}\\|_2^2 = \\sum_{i=1}^m w_i^2 = 1 \\] is both interpretable and mathematically tractable. Therefore we wish to maximize \\[ {\\operatorname{Var}}\\left(w_1 X_1 + w_2 X_2 + \\cdots + w_m X_m \\right) \\] subject to \\(\\| {\\boldsymbol{w}}\\|_2^2 = 1\\). Let \\({\\boldsymbol{\\Sigma}}\\) be the \\(m \\times m\\) population covariance matrix of the random variables \\(X_1, X_2, \\ldots, X_m\\). It follows that \\[ {\\operatorname{Var}}\\left(w_1 X_1 + w_2 X_2 + \\cdots + w_m X_m \\right) = {\\boldsymbol{w}}^T {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}. \\] Using a Lagrange multiplier, we wish to maximize \\[ {\\boldsymbol{w}}^T {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}+ \\lambda({\\boldsymbol{w}}^T {\\boldsymbol{w}}- 1). \\] Differentiating with respect to \\({\\boldsymbol{w}}\\) and setting to \\({\\boldsymbol{0}}\\), we get \\({\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}- \\lambda {\\boldsymbol{w}}= 0\\) or \\[ {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}= \\lambda {\\boldsymbol{w}}. \\] For any such \\({\\boldsymbol{w}}\\) and \\(\\lambda\\) where this holds, note that \\[ {\\operatorname{Var}}\\left(w_1 X_1 + w_2 X_2 + \\cdots + w_m X_m \\right) = {\\boldsymbol{w}}^T {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}= \\lambda \\] so the variance is \\(\\lambda\\). The eigendecompositon of a matrix identifies all such solutions to \\({\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}= \\lambda {\\boldsymbol{w}}\\). Specifically, it calculates the decompositon \\[ {\\boldsymbol{\\Sigma}}= {\\boldsymbol{W}}{\\boldsymbol{\\Lambda}}{\\boldsymbol{W}}^T \\] where \\({\\boldsymbol{W}}\\) is an \\(m \\times m\\) orthogonal matrix and \\({\\boldsymbol{\\Lambda}}\\) is a diagonal matrix with entries \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_m \\geq 0\\). The fact that \\({\\boldsymbol{W}}\\) is orthogonal means \\({\\boldsymbol{W}}{\\boldsymbol{W}}^T = {\\boldsymbol{W}}^T {\\boldsymbol{W}}= {\\boldsymbol{I}}\\). The following therefore hold: For each column \\(j\\) of \\({\\boldsymbol{W}}\\), say \\({\\boldsymbol{w}}_j\\), it follows that \\({\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}_j = \\lambda_j {\\boldsymbol{w}}_j\\) \\(\\| {\\boldsymbol{w}}_j \\|^2_2 = 1\\) and \\({\\boldsymbol{w}}_j^T {\\boldsymbol{w}}_k = {\\boldsymbol{0}}\\) for \\(\\lambda_j \\not= \\lambda_k\\) \\({\\operatorname{Var}}({\\boldsymbol{w}}_j^T {\\boldsymbol{X}}) = \\lambda_j\\) \\({\\operatorname{Var}}({\\boldsymbol{w}}_1^T {\\boldsymbol{X}}) \\geq {\\operatorname{Var}}({\\boldsymbol{w}}_2^T {\\boldsymbol{X}}) \\geq \\cdots \\geq {\\operatorname{Var}}({\\boldsymbol{w}}_m^T {\\boldsymbol{X}})\\) \\({\\boldsymbol{\\Sigma}}= \\sum_{j=1}^m \\lambda_j {\\boldsymbol{w}}^j {\\boldsymbol{w}}_j^T\\) For \\(\\lambda_j \\not= \\lambda_k\\), \\[{\\operatorname{Cov}}({\\boldsymbol{w}}_j^T {\\boldsymbol{X}}, {\\boldsymbol{w}}_k^T {\\boldsymbol{X}}) = {\\boldsymbol{w}}_j^T {\\boldsymbol{\\Sigma}}{\\boldsymbol{w}}_k = \\lambda_k {\\boldsymbol{w}}_j^T {\\boldsymbol{w}}_k = {\\boldsymbol{0}}\\] The \\(j\\)th population principal component (PC) of \\(X_1, X_2, \\ldots, X_m\\) is \\[ {\\boldsymbol{w}}_j^T {\\boldsymbol{X}}= w_{1j} X_1 + w_{2j} X_2 + \\cdots + w_{mj} X_m \\] where \\({\\boldsymbol{w}}_j = (w_{1j}, w_{2j}, \\ldots, w_{mj})^T\\) is column \\(j\\) of \\({\\boldsymbol{W}}\\) from the eigendecomposition \\[ {\\boldsymbol{\\Sigma}}= {\\boldsymbol{W}}{\\boldsymbol{\\Lambda}}{\\boldsymbol{W}}^T. \\] The column \\({\\boldsymbol{w}}_j\\) are called the loadings of the \\(j\\)th principal component. The variance explained by the \\(j\\)th PC is \\(\\lambda_j\\), which is diagonal element \\(j\\) of \\({\\boldsymbol{\\Lambda}}\\). "],
["multivariate-rvs.html", "30 Multivariate RVs 30.1 Multinomial 30.2 Multivariate Normal 30.3 Dirichlet 30.4 In R", " 30 Multivariate RVs 30.1 Multinomial Suppose \\(\\boldsymbol{X}\\) (an \\(m\\)-vector) is \\(\\mbox{Multinomial}_m(n, \\boldsymbol{p})\\), where \\(\\boldsymbol{p}\\) is an \\(m\\)-vector such that \\(\\sum_{i=1}^m p_i = 1\\). It has pmf \\[ f(\\boldsymbol{x}; \\boldsymbol{p}) = {n \\choose x_1 \\ x_2 \\ \\cdots \\ x_m} p_1^{x_1} p_2^{x_2} \\cdots p_m^{x_m} \\] where \\[{n \\choose x_1 \\ x_2 \\ \\cdots \\ x_m} = \\frac{n!}{x_1! x_2! \\cdots x_m!}\\] and \\(\\sum_{i=1}^m x_i = n\\). The Multinomial distribution is a generalization of the Binomial distribution. It models \\(n\\) independent outcomes where each outcome has probability \\(p_i\\) of category \\(i\\) occurring (for \\(i=1, 2, \\ldots, m\\)). The counts per category are contained in the \\(X_i\\) random variables that are constrained so that \\(\\sum_{i=1}^m X_i = n\\). It can be calculated that \\[{\\operatorname{E}}[X_i] = np_i, \\quad {\\operatorname{Var}}(X_i) = n p_i (1-p_i),\\] \\[{\\operatorname{Cov}}(X_i, X_j) = -n p_i p_j \\quad (i \\not= j).\\] 30.2 Multivariate Normal The \\(n\\)-vector \\(\\boldsymbol{X}\\) has Multivariate Normal distribution when \\(\\boldsymbol{X} \\sim \\mbox{MVN}_n(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) where \\(\\boldsymbol{\\mu}\\) is the \\(n\\)-vector of population means and \\(\\boldsymbol{\\Sigma}\\) is the \\(n \\times n\\) variance-covariance matrix. Its pdf is \\[ f(\\boldsymbol{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{2 \\pi |\\boldsymbol{\\Sigma}|}} \\exp -\\left\\{ -\\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}) \\right\\}. \\] Fun fact: \\(\\boldsymbol{\\Sigma}^{-1/2} (\\boldsymbol{X}-\\boldsymbol{\\mu}) \\sim \\mbox{MVN}_n(\\boldsymbol{0}, \\boldsymbol{I})\\). 30.3 Dirichlet The Dirichlet distribution models an \\(m\\)-vector \\(\\boldsymbol{X}\\) so that \\(0 \\leq X_i \\leq 1\\) and \\(\\sum_{i=1}^m X_i = 1\\). It is a generalization of the Beta distribution. The rv \\(\\boldsymbol{X} \\sim \\mbox{Dirichlet}_m(\\boldsymbol{\\alpha})\\), where \\(\\boldsymbol{\\alpha}\\) is an \\(m\\)-vector, has pdf \\[ f(\\boldsymbol{x}; \\boldsymbol{\\alpha}) = \\frac{\\Gamma\\left( \\sum_{i=1}^m \\alpha_i \\right)}{\\prod_{i=1}^m \\Gamma(\\alpha_i)} \\prod_{i=1}^m x_i^{\\alpha_i-1}. \\] It can be calculated that \\[{\\operatorname{E}}[X_i] = \\frac{\\alpha_i}{\\alpha_0}, {\\operatorname{Var}}(X_i) = \\frac{\\alpha_i (\\alpha_0 - \\alpha_i)}{\\alpha_0^2 (\\alpha_0 + 1)}, {\\operatorname{Cov}}(X_i, X_j) = \\frac{- \\alpha_i \\alpha_j}{\\alpha_0^2 (\\alpha_0 + 1)}\\] where \\(\\alpha_0 = \\sum_{k=1}^m \\alpha_k\\) and \\(i \\not= j\\) in \\({\\operatorname{Cov}}(X_i, X_j)\\). 30.4 In R For the Multinomial, base R contains the functions dmultinom and rmultinom. For the Multivariate Normal, there are several packages that work with this distribution. One choice is the package mvtnorm, which contains the functions dmvnorm and rmvnorm. For the Dirichlet, there are several packages that work with this distribution. One choice is the package MCMCpack, which contains the functions ddirichlet and rdirichlet. "],
["from-probability-to-likelihood.html", "31 From Probability to Likelihood 31.1 Likelihood Function 31.2 Log-Likelihood Function 31.3 Sufficient Statistics 31.4 Factorization Theorem 31.5 Example: Normal 31.6 Likelihood Principle 31.7 Maximum Likelihood 31.8 Going Further", " 31 From Probability to Likelihood 31.1 Likelihood Function Suppose that we observe \\(x_1, x_2, \\ldots, x_n\\) according to the model \\(X_1, X_2, \\ldots, X_n \\sim F_{\\theta}\\). The joint pdf is \\(f(\\boldsymbol{x} ; \\theta)\\). We view the pdf as being a function of \\(\\boldsymbol{x}\\) for a fixed \\(\\theta\\). The likelihood function is obtained by reversing the arguments and viewing this as a function of \\(\\theta\\) for a fixed, observed \\(\\boldsymbol{x}\\): \\[L(\\theta ; \\boldsymbol{x}) = f(\\boldsymbol{x} ; \\theta).\\] 31.2 Log-Likelihood Function The log-likelihood function is \\[ \\ell(\\theta ; \\boldsymbol{x}) = \\log L(\\theta ; \\boldsymbol{x}).\\] When the data are iid, we have \\[ \\ell(\\theta ; \\boldsymbol{x}) = \\log \\prod_{i=1}^n f(x_i ; \\theta) = \\sum_{i=1}^n \\log f(x_i ; \\theta).\\] 31.3 Sufficient Statistics A statistic \\(T(\\boldsymbol{x})\\) is defined to be a function of the data. A sufficient statistic is a statistic where the distribution of data, conditional on this statistic, does not depend on \\(\\theta\\). That is, \\(\\boldsymbol{X} | T(\\boldsymbol{X})\\) does not depend on \\(\\theta\\). The interpretation is that the information in \\(\\boldsymbol{X}\\) about \\(\\theta\\) (the target of inference) is contained in \\(T(\\boldsymbol{X})\\). 31.4 Factorization Theorem The factorization theorem says that \\(T(\\boldsymbol{x})\\) is a sufficient statistic if and only if we can factor \\[f(\\boldsymbol{x} ; \\theta) = g(T(\\boldsymbol{x}), \\theta) h(\\boldsymbol{x}).\\] Therefore, if \\(T(\\boldsymbol{x})\\) is a sufficient statistic then \\[L(\\theta ; \\boldsymbol{x}) = g(T(\\boldsymbol{x}), \\theta) h(\\boldsymbol{x}) \\propto L(\\theta ; T(\\boldsymbol{x})).\\] This formalizes the idea that the information in \\(\\boldsymbol{X}\\) about \\(\\theta\\) (the target of inference) is contained in \\(T(\\boldsymbol{X})\\). 31.5 Example: Normal If \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu \\sigma^2)\\), then \\(\\overline{X}\\) is sufficient for \\(\\mu\\). As an exercise, show this via the factorization theorem. Hint: \\(\\sum_{i=1}^n (x_i - \\mu)^2 = \\sum_{i=1}^n (x_i - \\overline{x})^2 + n(\\overline{x} - \\mu)^2\\). 31.6 Likelihood Principle If \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) are two data sets so that \\[L(\\theta ; \\boldsymbol{x}) \\propto L(\\theta ; \\boldsymbol{y}),\\] \\[\\mbox{i.e., } L(\\theta ; \\boldsymbol{x}) = c(\\boldsymbol{x}, \\boldsymbol{y}) L(\\theta ; \\boldsymbol{y}),\\] then inferenece \\(\\theta\\) should be the same for \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\). 31.7 Maximum Likelihood A common starting point for inference is to calculate the maximum likelihood estimate. This is the value of \\(\\theta\\) that maximizes \\(L(\\theta ; \\boldsymbol{x})\\) for an observe data set \\(\\boldsymbol{x}\\). \\[\\begin{align*} \\hat{\\theta}_{{\\rm MLE}} &amp; = \\operatorname{argmax}_{\\theta} L(\\theta ; \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_{\\theta} \\ell (\\theta ; \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_{\\theta} L (\\theta ; T(\\boldsymbol{x})) \\end{align*}\\] where the last equality holds for sufficient statistics \\(T(\\boldsymbol{x})\\). 31.8 Going Further If this interests you, be sure to read about: Minimal sufficient statistics Complete sufficient statistics Ancillary statistics Basu’s theorem "],
["exponential-family-distributions.html", "32 Exponential Family Distributions 32.1 Rationale 32.2 Definition 32.3 Example: Bernoulli 32.4 Example: Normal 32.5 Natural Single Parameter EFD 32.6 Calculating Moments 32.7 Example: Normal 32.8 Maximum Likelihood 32.9 Table of Common EFDs", " 32 Exponential Family Distributions 32.1 Rationale Exponential family distributions (EFDs) provide a generalized parameterization and form of a very large class of distributions used in inference. For example, Binomia, Poisson, Exponential, Normal, Multinomial, MVN, and Dirichlet are all EFDs. The generalized form provides generally applicable formulas for moments, estimators, etc. EFDs also facilitate developing general algorithms for model fitting. 32.2 Definition If \\(X\\) follows an EFD then it has pdf of the form \\[f(x ; \\boldsymbol{\\theta}) = h(x) \\exp \\left\\{ \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(x) - A(\\boldsymbol{\\eta}) \\right\\} \\] where \\(\\boldsymbol{\\theta}\\) is a vector of parameters, \\(\\{T_k(x)\\}\\) are sufficient statistics, \\(A(\\boldsymbol{\\eta})\\) is the cumulant generating function. The functions \\(\\eta_k(\\boldsymbol{\\theta})\\) for \\(k=1, \\ldots, d\\) map the usual parameters to the “natural parameters”. \\(\\{T_k(x)\\}\\) are sufficient statistics for \\(\\{\\eta_k\\}\\) due to the factorization theorem. \\(A(\\boldsymbol{\\eta})\\) is sometimes called the “log normalizer” because \\[A(\\boldsymbol{\\eta}) = \\log \\int h(x) \\exp \\left\\{ \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(x) \\right\\}.\\] 32.3 Example: Bernoulli \\[\\begin{align*} f(x ; p) &amp; = p^x (1-p)^{1-x} \\\\ &amp; = \\exp \\left\\{ x \\log(p) + (1-x) \\log(1-p) \\right\\} \\\\ &amp; = \\exp \\left\\{ x \\log\\left( \\frac{p}{1-p} \\right) + \\log(1-p) \\right\\} \\end{align*}\\] \\(\\eta(p) = \\log\\left( \\frac{p}{1-p} \\right)\\) \\(T(x) = x\\) \\(A(\\eta) = \\log\\left(1 + e^\\eta\\right)\\) 32.4 Example: Normal \\[\\begin{align*} f(x ; \\mu, \\sigma^2) &amp; = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left\\{-\\frac{(x-\\mu)^2}{2 \\sigma^2}\\right\\} \\\\ &amp; = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left\\{\\frac{\\mu}{\\sigma^2} x - \\frac{1}{2 \\sigma^2} x^2 - \\log(\\sigma) - \\frac{\\mu^2}{2 \\sigma^2}\\right\\} \\end{align*}\\] \\(\\boldsymbol{\\eta}(\\mu, \\sigma^2) = \\left(\\frac{\\mu}{\\sigma^2}, - \\frac{1}{2 \\sigma^2} \\right)^T\\) \\(\\boldsymbol{T}(x) = \\left(x, x^2\\right)^T\\) \\(A(\\boldsymbol{\\eta}) = \\log(\\sigma) + \\frac{\\mu^2}{2 \\sigma^2} = -\\frac{1}{2} \\log(-2 \\eta_2) - \\frac{\\eta_1^2}{4\\eta_2}\\) 32.5 Natural Single Parameter EFD A natural single parameter EFD simplifies to the scenario where \\(d=1\\) and \\(T(x) = x\\): \\[f(x ; \\eta) = h(x) \\exp \\left\\{ \\eta x - A(\\eta) \\right\\} \\] 32.6 Calculating Moments \\[ \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}) = {\\operatorname{E}}[T_k(X)] \\] \\[ \\frac{\\partial^2}{\\partial \\eta_k^2} A(\\boldsymbol{\\eta}) = {\\operatorname{Var}}[T_k(X)] \\] 32.7 Example: Normal For \\(X \\sim \\mbox{Normal}(\\mu, \\sigma^2)\\), \\[{\\operatorname{E}}[X] = \\frac{\\partial}{\\partial \\eta_1} A(\\boldsymbol{\\eta}) = -\\frac{\\eta_1}{2 \\eta_2} = \\mu,\\] \\[{\\operatorname{Var}}(X) = \\frac{\\partial^2}{\\partial \\eta_1^2} A(\\boldsymbol{\\eta}) = -\\frac{1}{2 \\eta_2} = \\sigma^2.\\] 32.8 Maximum Likelihood Suppose \\(X_1, X_2, \\ldots, X_n\\) are iid from some EFD. Then, \\[ \\ell(\\boldsymbol{\\eta} ; \\boldsymbol{x}) = \\sum_{i=1}^n \\left[ \\log h(x_i) + \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(x_i) - A(\\boldsymbol{\\eta}) \\right] \\] \\[ \\frac{\\partial}{\\partial \\eta_k} \\ell(\\boldsymbol{\\eta} ; \\boldsymbol{x}) = \\sum_{i=1}^n T_k(x_i) - n \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}) \\] Setting the second equation to 0, it follows that the MLE of \\(\\eta_k\\) is the solution to \\[ \\frac{1}{n} \\sum_{i=1}^n T_k(x_i) = \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}). \\] 32.9 Table of Common EFDs "],
["statistical-inference.html", "33 Statistical Inference 33.1 Data Collection as a Probability 33.2 Example: Simple Random Sample 33.3 Example: Randomized Controlled Trial 33.4 Parameters and Statistics 33.5 Sampling Distribution 33.6 Central Dogma of Inference 33.7 Example: Fair Coin?", " 33 Statistical Inference 33.1 Data Collection as a Probability Suppose data are collected in such a way that it is randomly observed according to a probability distribution If that probability distribution can be parameterized, then it is possible that the parameters describe key characteristics of the population of interest Statistical inference reverse engineers this process to estimate the unknown values of the parameters and express a measure of uncertainty about these estimates 33.2 Example: Simple Random Sample Individuals are uniformly and independently randomly sampled from a population. The measurements taken on these individuals are then modeled as random variables, specifically random realizations from the complete population of values. Simple random samples form the basis of modern surveys. 33.3 Example: Randomized Controlled Trial Individuals under study are randomly assigned to one of two or more available treatments. This induces randomization directly into the study and breaks the relationship between the treatments and other variables that may be influencing the response of interest. This is the gold standard study design in clinical trials to assess the evidence that a new drug works on a given disease. 33.4 Parameters and Statistics A parameter is a number that describes a population A parameter is often a fixed number We usually do not know its value A statistic is a number calculated from a sample of data A statistic is used to estimate a parameter 33.5 Sampling Distribution The sampling distribution of a statistic is the probability disribution of the statistic under repeated realizations of the data from the assumed data generating probability distribution. The sampling distribution is how we connect an observed statistic to the population. 33.6 Central Dogma of Inference 33.7 Example: Fair Coin? Suppose I claim that a specific coin is fair, i.e., that it lands on heads or tails with equal probability. I flip it 20 times and it lands on heads 16 times. My data is \\(x=16\\) heads out of \\(n=20\\) flips. My data generation model is \\(X \\sim \\mbox{Binomial}(20, p)\\). I form the statistic \\(\\hat{p} = 16/20\\) as an estimate of \\(p\\). Let’s simulate 10,000 times what my estimate would look like if \\(p=0.5\\) and I repeated the 20 coin flips over and over. &gt; x &lt;- replicate(n=1e4, expr=rbinom(1, size=20, prob=0.5)) &gt; sim_p_hat &lt;- x/20 &gt; my_p_hat &lt;- 16/20 What can I do with this information? "],
["inference-goals-and-strategies.html", "34 Inference Goals and Strategies 34.1 Basic Idea 34.2 Normal Example 34.3 Point Estimate of \\(\\mu\\) 34.4 Sampling Distribution of \\(\\hat{\\mu}\\) 34.5 Pivotal Statistic", " 34 Inference Goals and Strategies 34.1 Basic Idea Data are collected in such a way that there exists a reasonable probability model for this process that involves parameters informative about the population. Common Goals: Form point estimates the parameters Quantify uncertainty on the estimates Test hypotheses on the parameters 34.2 Normal Example Suppose a simple random sample of \\(n\\) data points is collected so that the following model of the data is reasonable: \\(X_1, X_2, \\ldots, X_n\\) are iid Normal(\\(\\mu\\), \\(\\sigma^2\\)). The goal is to do inference on \\(\\mu\\), the population mean. For simplicity, assume that \\(\\sigma^2\\) is known (e.g., \\(\\sigma^2 = 1\\)). 34.3 Point Estimate of \\(\\mu\\) There are a number of ways to form an estimate of \\(\\mu\\), but one that has several justifications is the sample mean: \\[\\hat{\\mu} = \\overline{x} = \\frac{1}{n}\\sum_{i=1}^n x_i,\\] where \\(x_1, x_2, \\ldots, x_n\\) are the observed data points. 34.4 Sampling Distribution of \\(\\hat{\\mu}\\) If we were to repeat this study over and over, how would \\(\\hat{\\mu}\\) behave? \\[\\hat{\\mu} = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\] \\[\\overline{X} \\sim \\mbox{Normal}(\\mu, \\sigma^2/n)\\] How do we use this to quantify uncertainty and test hypotheses? 34.5 Pivotal Statistic One very useful strategy is to work backwards from a pivotal statistic, which is a statistic that does not depend on any unknown paramaters. Example: \\[\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1)\\] Note that in general for a rv \\(Y\\) it is the case that \\((Y - \\operatorname{E}[Y])/\\sqrt{\\operatorname{Var}(Y)}\\) has population mean 0 and variance 1. "],
["confidence-intervals.html", "35 Confidence Intervals 35.1 Goal 35.2 Formulation 35.3 Interpretation 35.4 A Normal CI 35.5 A Simulation 35.6 Normal\\((0,1)\\) Percentiles 35.7 Commonly Used Percentiles 35.8 \\((1-\\alpha)\\)-Level CIs 35.9 One-Sided CIs", " 35 Confidence Intervals 35.1 Goal Once we have a point estimate of a parameter, we would like a measure of its uncertainty. Given that we are working within a probabilistic framework, the natural language of uncertainty is through probability statements. We interpret this measure of uncertainty in terms of hypothetical repetitions of the sampling scheme we used to collect the original data set. 35.2 Formulation Confidence intervals take the form \\[(\\hat{\\mu} - C_{\\ell}, \\hat{\\mu} + C_{u})\\] where \\[{\\rm Pr}(\\mu - C_{\\ell} \\leq \\hat{\\mu} \\leq \\mu + C_{u})\\] forms the “level” or coverage probability of the interval. 35.3 Interpretation If we repeat the study many times, then the CI \\((\\hat{\\mu} - C_{\\ell}, \\hat{\\mu} + C_{u})\\) will contain the true value \\(\\mu\\) with a long run frequency equal to \\({\\rm Pr}(\\mu - C_{\\ell} \\leq \\hat{\\mu} \\leq \\mu + C_{u})\\). A CI calculated on an observed data set is not intepreted as: “There is probability \\({\\rm Pr}(\\mu - C_{\\ell} \\leq \\hat{\\mu} \\leq \\mu + C_{u})\\) that \\(\\mu\\) is in our calculated \\((\\hat{\\mu} - C_{\\ell}, \\hat{\\mu} + C_{u})\\).” Why not? 35.4 A Normal CI If \\(Z \\sim\\) Normal(0,1), then \\({\\rm Pr}(-1.96 \\leq Z \\leq 1.96) = 0.95.\\) \\[\\begin{eqnarray} 0.95 &amp; = &amp; {\\rm Pr} \\left(-1.96 \\leq \\frac{\\hat{\\mu} - \\mu}{\\sigma/\\sqrt{n}} \\leq 1.96 \\right) \\\\ \\ &amp; = &amp; {\\rm Pr} \\left(-1.96 \\frac{\\sigma}{\\sqrt{n}} \\leq \\hat{\\mu} - \\mu \\leq 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\ \\ &amp; = &amp; {\\rm Pr} \\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} \\leq \\hat{\\mu} \\leq \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right) \\end{eqnarray}\\] Therefore, \\(\\left(\\hat{\\mu} - 1.96\\frac{\\sigma}{\\sqrt{n}}, \\hat{\\mu} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\) forms a 95% confidence interval of \\(\\mu\\). 35.5 A Simulation &gt; mu &lt;- 5 &gt; n &lt;- 20 &gt; x &lt;- replicate(10000, rnorm(n=n, mean=mu)) # 10000 studies &gt; m &lt;- apply(x, 2, mean) # the estimate for each study &gt; ci &lt;- cbind(m - 1.96/sqrt(n), m + 1.96/sqrt(n)) &gt; head(ci) [,1] [,2] [1,] 4.797848 5.674386 [2,] 4.599996 5.476534 [3,] 4.472930 5.349468 [4,] 4.778946 5.655485 [5,] 4.778710 5.655248 [6,] 4.425023 5.301561 &gt; cover &lt;- (mu &gt; ci[,1]) &amp; (mu &lt; ci[,2]) &gt; mean(cover) [1] 0.9512 35.6 Normal\\((0,1)\\) Percentiles Above we constructed a 95% CI. How do we construct (1-\\(\\alpha\\))-level CIs? Let \\(z_{\\alpha}\\) be the \\(\\alpha\\) percentile of the Normal(0,1) distribution. If \\(Z \\sim\\) Normal(0,1), then \\[\\begin{eqnarray*} 1-\\alpha &amp; = &amp; {\\rm Pr}(z_{\\alpha/2} \\leq Z \\leq z_{1-\\alpha/2}) \\\\ \\ &amp; = &amp; {\\rm Pr}(-|z_{\\alpha/2}| \\leq Z \\leq |z_{\\alpha/2}|) \\end{eqnarray*}\\] 35.7 Commonly Used Percentiles &gt; # alpha/2 upper and lower percentiles for alpha=0.05 &gt; qnorm(0.025) [1] -1.959964 &gt; qnorm(0.975) [1] 1.959964 &gt; # alpha/2 upper and lower percentiles for alpha=0.10 &gt; qnorm(0.05) [1] -1.644854 &gt; qnorm(0.95) [1] 1.644854 35.8 \\((1-\\alpha)\\)-Level CIs If \\(Z \\sim\\) Normal(0,1), then \\({\\rm Pr}(-|z_{\\alpha/2}| \\leq Z \\leq |z_{\\alpha/2}|) = 1-\\alpha.\\) Repeating the steps from the 95% CI case, we get the following is a \\((1-\\alpha)\\)-Level CI for \\(\\mu\\): \\[\\left(\\hat{\\mu} - |z_{\\alpha/2}| \\frac{\\sigma}{\\sqrt{n}}, \\hat{\\mu} + |z_{\\alpha/2}| \\frac{\\sigma}{\\sqrt{n}}\\right)\\] 35.9 One-Sided CIs The CIs we have considered so far are “two-sided”. Sometimes we are also interested in “one-sided” CIs. If \\(Z \\sim\\) Normal(0,1), then \\(1-\\alpha = {\\rm Pr}(Z \\geq -|z_{\\alpha}|)\\) and \\(1-\\alpha = {\\rm Pr}(Z \\leq |z_{\\alpha}|).\\) We can use this fact along with the earlier derivations to show that the following are valid CIs: \\[(1-\\alpha)\\mbox{-level upper: } \\left(-\\infty, \\hat{\\mu} + |z_{\\alpha}| \\frac{\\sigma}{\\sqrt{n}}\\right)\\] \\[(1-\\alpha)\\mbox{-level lower: } \\left(\\hat{\\mu} - |z_{\\alpha}| \\frac{\\sigma}{\\sqrt{n}}, \\infty\\right)\\] "],
["hypothesis-tests.html", "36 Hypothesis Tests 36.1 Example: HT on Fairness of a Coin 36.2 A Caveat 36.3 Definition 36.4 Return to Normal Example 36.5 HTs on Parameter Values 36.6 Two-Sided vs. One-Sided HT 36.7 Test Statistic 36.8 Null Distribution (Two-Sided) 36.9 Null Distribution (One-Sided) 36.10 P-values 36.11 Calling a Test “Significant” 36.12 Types of Errors 36.13 Error Rates", " 36 Hypothesis Tests 36.1 Example: HT on Fairness of a Coin Suppose I claim that a specific coin is fair, i.e., that it lands on heads or tails with equal probability. I flip it 20 times and it lands on heads 16 times. My data is \\(x=16\\) heads out of \\(n=20\\) flips. My data generation model is \\(X \\sim \\mbox{Binomial}(20, p)\\). I form the statistic \\(\\hat{p} = 16/20\\) as an estimate of \\(p\\). More formally, I want to test the hypothesis: \\(H_0: p=0.5\\) vs. \\(H_1: p \\not=0.5\\) under the model \\(X \\sim \\mbox{Binomial}(20, p)\\) based on the test statistic \\(\\hat{p} = X/n\\). Let’s simulate 10,000 times what my estimate would look like if \\(p=0.5\\) and I repeated the 20 coin flips over and over. &gt; x &lt;- replicate(n=1e4, expr=rbinom(1, size=20, prob=0.5)) &gt; sim_p_hat &lt;- x/20 &gt; my_p_hat &lt;- 16/20 The vector sim_p_hat contains 10,000 draws from the null distribution, i.e., the distribution of my test statstic \\(\\hat{p} = X/n\\) when \\(H_0: p=0.5\\) is true. The deviation of the test statistic from the null hypothesis can be measured by \\(|\\hat{p} - 0.5|\\). Let’s compare our observed deviation \\(|16/20 - 0.5|\\) to the 10,000 simulated null data sets. Specifically, let’s calculate the frequency by which these 10,000 cases are as or more extreme than the observed test statistic. &gt; sum(abs(sim_p_hat-0.5) &gt;= abs(my_p_hat-0.5))/1e4 [1] 0.0055 This quantity is called the p-value of the hypothesis test. 36.2 A Caveat This example is a simplification of a more general framework for testing statistical hypotheses. Given the intuition provided by the example, let’s now formalize these ideas. 36.3 Definition A hypothesis test or significance test is a formal procedure for comparing observed data with a hypothesis whose truth we want to assess The results of a test are expressed in terms of a probability that measures how well the data and the hypothesis agree The null hypothesis (\\(H_0\\)) is the statement being tested, typically the status quo The alternative hypothesis (\\(H_1\\)) is the complement of the null, and it is often the “interesting” state 36.4 Return to Normal Example Let’s return to our Normal example in order to demonstrate the framework. Suppose a simple random sample of \\(n\\) data points is collected so that the following model of the data is reasonable: \\(X_1, X_2, \\ldots, X_n\\) are iid Normal(\\(\\mu\\), \\(\\sigma^2\\)). The goal is to do test a hypothesis on \\(\\mu\\), the population mean. For simplicity, assume that \\(\\sigma^2\\) is known (e.g., \\(\\sigma^2 = 1\\)). 36.5 HTs on Parameter Values Hypothesis tests are usually formulated in terms of values of parameters. For example: \\[H_0: \\mu = 5\\] \\[H_1: \\mu \\not= 5\\] Note that the choice of 5 here is arbitrary, for illustrative purposes only. In a typical real world problem, the values that define the hypotheses will be clear from the context. 36.6 Two-Sided vs. One-Sided HT Hypothesis tests can be two-sided or one-sided: \\[H_0: \\mu = 5 \\mbox{ vs. } H_1: \\mu \\not= 5 \\mbox{ (two-sided)}\\] \\[H_0: \\mu \\leq 5 \\mbox{ vs. } H_1: \\mu &gt; 5 \\mbox{ (one-sided)}\\] \\[H_0: \\mu \\geq 5 \\mbox{ vs. } H_1: \\mu &lt; 5 \\mbox{ (one-sided)}\\] 36.7 Test Statistic A test statistic is designed to quantify the evidence against the null hypothesis in favor of the alternative. They are usually defined (and justified using math theory) so that the larger the test statistic is, the more evidence there is. For the Normal example and the two-sided hypothesis \\((H_0: \\mu = 5 \\mbox{ vs. } H_1: \\mu \\not= 5)\\), here is our test statistic: \\[ |z| = \\frac{\\left|\\overline{x} - 5\\right|}{\\sigma/\\sqrt{n}} \\] What would the test statistic be for the one-sided hypothesis tests? 36.8 Null Distribution (Two-Sided) The null distribution is the sampling distribution of the test statistic when \\(H_0\\) is true. We saw earlier that \\(\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1).\\) When \\(H_0\\) is true, then \\(\\mu=5\\). So when \\(H_0\\) is true it follows that \\[Z = \\frac{\\overline{X} - 5}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1)\\] and then probabiliy calculations on \\(|Z|\\) are straightforward. Note that \\(Z\\) is pivotal when \\(H_0\\) is true! 36.9 Null Distribution (One-Sided) When performing a one-sided hypothesis test, such as \\(H_0: \\mu \\leq 5 \\mbox{ vs. } H_1: \\mu &gt; 5\\), the null distribution is typically calculated under the “least favorable” value, which is the boundary value. In this example it would be \\(\\mu=5\\) and we would again utilize the null distribution \\[Z = \\frac{\\overline{X} - 5}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1).\\] 36.10 P-values The p-value is defined to be the probability that a test statistic from the null distribution is as or more extreme than the observed statistic. In our Normal example on the two-sided hypothesis test, the p-value is \\[{\\rm Pr}(|Z^*| \\geq |z|)\\] where \\(Z^* \\sim \\mbox{Normal}(0,1)\\) and \\(|z|\\) is the value of the test statistic calculated on the data (so it is a fixed number once we observe the data). 36.11 Calling a Test “Significant” A hypothesis test is called statistically significant — meaning we reject \\(H_0\\) in favor of \\(H_1\\) — if its p-value is sufficiently small. Commonly used cut-offs are 0.01 or 0.05, although these are not always appropriate and they are historical artifacts. Applying a specific p-value cut-off to determine significance determines an error rate, which we define next. 36.12 Types of Errors There are two types of errors that can be committed when performing a hypothesis test. A Type I error or false positive is when a hypothesis test is called signficant and the null hypothesis is actually true. A Type II error or false negative is when a hypothesis test is not called signficant and the alternative hypothesis is actually true. 36.13 Error Rates The Type I error rate or false positive rate is the probability of this type of error given that \\(H_0\\) is true. If a hypothesis test is called significant when p-value \\(\\leq \\alpha\\) then it has a Type I error rate equal to \\(\\alpha\\). The Type II error rate or false negative rate is the probability of this type of error given that \\(H_1\\) is true. The power of a hypothesis test is \\(1 -\\) Type II error rate. Hypothesis tests are usually derived with a goal to control the Type I error rate while maximizing the power. "],
["maximum-likelihood-estimation.html", "37 Maximum Likelihood Estimation 37.1 The Normal Example 37.2 MLE \\(\\rightarrow\\) Normal Pivotal Statistics 37.3 Likelihood Function 37.4 Log-Likelihood Function 37.5 Calculating MLEs 37.6 Properties 37.7 Assumptions and Notation 37.8 Consistency 37.9 Equivariance 37.10 Fisher Information 37.11 Standard Error 37.12 Asymptotic Normal 37.13 Asymptotic Pivotal Statistic 37.14 Wald Test 37.15 Confidence Intervals 37.16 Optimality 37.17 Delta Method 37.18 Delta Method Example 37.19 Multiparameter Fisher Info Matrix 37.20 Multiparameter Asymptotic MVN", " 37 Maximum Likelihood Estimation 37.1 The Normal Example We formulated both confidence intervals and hypothesis tests under the following idealized scenario: Suppose a simple random sample of \\(n\\) data points is collected so that the following model of the data is reasonable: \\(X_1, X_2, \\ldots, X_n\\) are iid Normal(\\(\\mu\\), \\(\\sigma^2\\)). The goal is to do inference on \\(\\mu\\), the population mean. For simplicity, assume that \\(\\sigma^2\\) is known (e.g., \\(\\sigma^2 = 1\\)). There is a good reason why we did this. 37.2 MLE \\(\\rightarrow\\) Normal Pivotal Statistics The random variable distributions we introduced last week have maximum likelihood estimators (MLEs) that can be standardized to yield a pivotal statistic with a Normal(0,1) distribution based on MLE theory. For example, if \\(X \\sim \\mbox{Binomial}(n,p)\\) then \\(\\hat{p}=X/n\\) is the MLE. For large \\(n\\) it approximately holds that \\[ \\frac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\sim \\mbox{Normal}(0,1). \\] 37.3 Likelihood Function Suppose that we observe \\(x_1, x_2, \\ldots, x_n\\) according to the model \\(X_1, X_2, \\ldots, X_n \\sim F_{\\theta}\\). The joint pdf is \\(f(\\boldsymbol{x} ; \\theta)\\). We view the pdf as being a function of \\(\\boldsymbol{x}\\) for a fixed \\(\\theta\\). The likelihood function is obtained by reversing the arguments and viewing this as a function of \\(\\theta\\) for a fixed, observed \\(\\boldsymbol{x}\\): \\[L(\\theta ; \\boldsymbol{x}) = f(\\boldsymbol{x} ; \\theta).\\] 37.4 Log-Likelihood Function The log-likelihood function is \\[ \\ell(\\theta ; \\boldsymbol{x}) = \\log L(\\theta ; \\boldsymbol{x}).\\] When the data are iid, we have \\[ \\ell(\\theta ; \\boldsymbol{x}) = \\log \\prod_{i=1}^n f(x_i ; \\theta) = \\sum_{i=1}^n \\log f(x_i ; \\theta).\\] 37.5 Calculating MLEs The maximum likelihood estimate is the value of \\(\\theta\\) that maximizes \\(L(\\theta ; \\boldsymbol{x})\\) for an observe data set \\(\\boldsymbol{x}\\). \\[\\begin{align*} \\hat{\\theta}_{{\\rm MLE}} &amp; = \\operatorname{argmax}_{\\theta} L(\\theta ; \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_{\\theta} \\ell (\\theta ; \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_{\\theta} L (\\theta ; T(\\boldsymbol{x})) \\end{align*}\\] where the last equality holds for sufficient statistics \\(T(\\boldsymbol{x})\\). The MLE can usually be calculated analytically or numerically. 37.6 Properties When “certain regularity assumptions” are true, the following properties hold for MLEs. Consistent Equivariant Asymptotically Normal Asymptotically Efficient (or Optimal) Approximate Bayes Estimator 37.7 Assumptions and Notation We will assume that \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_{\\theta}\\) and let \\(\\hat{\\theta}_n\\) be the MLE of \\(\\theta\\) based on the \\(n\\) observations. The only exception is for the Binomial distribution where we will assume that \\(X \\sim \\mbox{Binomial}(n, p)\\), which is the sum of \\(n\\) iid \\(\\mbox{Bernoulli}(p)\\) rv’s. We will assume that the “certain regularity assumptions” are true in the following results. 37.8 Consistency An estimator is consistent if it converges in probability to the true parameter value. MLEs are consistent so that as \\(n \\rightarrow \\infty\\), \\[\\hat{\\theta}_n \\stackrel{P}{\\rightarrow} \\theta,\\] where \\(\\theta\\) is the true value. 37.9 Equivariance If \\(\\hat{\\theta}_n\\) is the MLE of \\(\\theta\\), then \\(g\\left(\\hat{\\theta}_n\\right)\\) is the MLE of \\(g(\\theta)\\). Example: For the Normal\\((\\mu, \\sigma^2)\\) the MLE of \\(\\mu\\) is \\(\\overline{X}\\). Therefore, the MLE of \\(e^\\mu\\) is \\(e^{\\overline{X}}\\). 37.10 Fisher Information The Fisher Information of \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_{\\theta}\\) is: \\[\\begin{align*} I_n(\\theta) &amp; = \\operatorname{Var}\\left( \\frac{d}{d\\theta} \\log f(\\boldsymbol{X}; \\theta) \\right) = \\sum_{i=1}^n \\operatorname{Var}\\left( \\frac{d}{d\\theta} \\log f(X_i; \\theta) \\right) \\\\ &amp; = - \\operatorname{E}\\left( \\frac{d^2}{d\\theta^2} \\log f(\\boldsymbol{X}; \\theta) \\right) = - \\sum_{i=1}^n \\operatorname{E}\\left( \\frac{d^2}{d\\theta^2} \\log f(X_i; \\theta) \\right) \\end{align*}\\] 37.11 Standard Error In general, the standard error of the standard deviation of sampling distribution of an estimate or statistic. For MLEs, the standard error is \\(\\sqrt{{\\operatorname{Var}}\\left(\\hat{\\theta}_n\\right)}\\). It has the approximation \\[\\operatorname{se}\\left(\\hat{\\theta}_n\\right) \\approx \\frac{1}{\\sqrt{I_n(\\theta)}}\\] and the standard error estimate is \\[\\hat{\\operatorname{se}}\\left(\\hat{\\theta}_n\\right) = \\frac{1}{\\sqrt{I_n\\left(\\hat{\\theta}_n\\right)}}.\\] 37.12 Asymptotic Normal MLEs converge in distribution to the Normal distribution. Specifically, as \\(n \\rightarrow \\infty\\), \\[\\frac{\\hat{\\theta}_n - \\theta}{{\\operatorname{se}}\\left(\\hat{\\theta}_n\\right)} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1)\\] and \\[\\frac{\\hat{\\theta}_n - \\theta}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1).\\] 37.13 Asymptotic Pivotal Statistic By the previous result, we now have an approximate (asymptotic) pivotal statistic: \\[Z = \\frac{\\hat{\\theta}_n - \\theta}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1).\\] This allows us to construct approximate confidence intervals and hypothesis test as in the idealized \\(\\mbox{Normal}(\\mu, \\sigma^2)\\) (with \\(\\sigma^2\\) known) scenario from the previous sections. 37.14 Wald Test Consider the hypothesis test, \\(H_0: \\theta=\\theta_0\\) vs \\(H_1: \\theta \\not= \\theta_0\\). We form test statistic \\[z = \\frac{\\hat{\\theta}_n - \\theta_0}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)},\\] which has approximate p-value \\[\\mbox{p-value} = {\\rm Pr}(|Z^*| \\geq |z|),\\] where \\(Z^*\\) is a Normal\\((0,1)\\) random variable. 37.15 Confidence Intervals Using this MLE theory, we can form approximate \\((1-\\alpha)\\) level confidence intervals as follows. Two-sided: \\[\\left(\\hat{\\theta}_n - |z_{\\alpha/2}| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right), \\hat{\\theta}_n + |z_{\\alpha/2}| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)\\right)\\] Upper: \\[\\left(-\\infty, \\hat{\\theta}_n + |z_{\\alpha}| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)\\right)\\] Lower: \\[\\left(\\hat{\\theta}_n - |z_{\\alpha}| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right), \\infty\\right)\\] 37.16 Optimality The MLE is such that \\[ \\sqrt{n} \\left( \\hat{\\theta}_n - \\theta \\right) \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0, \\tau^2)\\] for some \\(\\tau^2\\). Suppose that \\(\\tilde{\\theta}_n\\) is any other estimate so that \\[ \\sqrt{n} \\left( \\tilde{\\theta}_n - \\theta \\right) \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0, \\gamma^2).\\] It follows that \\[\\frac{\\tau^2}{\\gamma^2} \\leq 1.\\] 37.17 Delta Method Suppose that \\(g()\\) is a differentiable function and \\(g&#39;(\\theta) \\not= 0\\). Note that for some \\(t\\) in a neighborhood of \\(\\theta\\), a first-order Taylor expansion tells us that \\(g(t) \\approx g&#39;(\\theta) (t - \\theta)\\). From this we know that \\[{\\operatorname{Var}}\\left(g(\\hat{\\theta}_n) \\right) \\approx g&#39;(\\theta)^2 {\\operatorname{Var}}(\\hat{\\theta}_n)\\] The delta method shows that \\(\\hat{{\\operatorname{se}}}\\left(g(\\hat{\\theta}_n)\\right) = |g&#39;(\\hat{\\theta}_n)| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)\\) and \\[\\frac{g(\\hat{\\theta}_n) - g(\\theta)}{|g&#39;(\\hat{\\theta}_n)| \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n\\right)} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1).\\] 37.18 Delta Method Example Suppose \\(X \\sim \\mbox{Binomial}(n,p)\\) which has MLE, \\(\\hat{p} = X/n\\). By the equivariance property, the MLE of the per-trial variance \\(p(1-p)\\) is \\(\\hat{p}(1-\\hat{p})\\). It can be calculated that \\(\\hat{{\\operatorname{se}}}(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n}\\). Let \\(g(p) = p(1-p)\\). Then \\(g&#39;(p) = 1-2p\\). By the delta method, \\[\\hat{{\\operatorname{se}}}\\left( \\hat{p}(1-\\hat{p}) \\right) = \\left| (1-2\\hat{p}) \\right| \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}.\\] 37.19 Multiparameter Fisher Info Matrix Suppose that \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_{\\boldsymbol{\\theta}}\\) where \\(\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\ldots, \\theta_d)^T\\) has MLE \\(\\hat{\\boldsymbol{\\theta}}_n\\). The Fisher Information Matrix \\(I_n(\\boldsymbol{\\theta})\\) is the \\(d \\times d\\) matrix with \\((i, j)\\) entry \\[ -\\sum_{k=1}^n \\operatorname{E}\\left( \\frac{\\partial^2}{\\partial\\theta_i \\partial\\theta_j} \\log f(X_k; \\boldsymbol{\\theta}) \\right).\\] 37.20 Multiparameter Asymptotic MVN Under appropriate regularity conditions, as \\(n \\rightarrow \\infty\\), \\[ \\left( \\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta} \\right) \\stackrel{D}{\\longrightarrow} \\mbox{MVN}_d \\left( \\boldsymbol{0}, I_n(\\boldsymbol{\\theta})^{-1} \\right) \\mbox{ and } \\] \\[ \\left( \\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta} \\right)^T I_n(\\hat{\\boldsymbol{\\theta}}_n) \\left( \\hat{\\boldsymbol{\\theta}}_n - \\boldsymbol{\\theta} \\right) \\stackrel{D}{\\longrightarrow} \\mbox{MVN}_d \\left( \\boldsymbol{0}, \\boldsymbol{I} \\right). \\] "],
["mle-examples-one-sample.html", "38 MLE Examples: One Sample 38.1 Exponential Family Distributions 38.2 Summary of MLE Statistics 38.3 Notes 38.4 Binomial 38.5 Normal 38.6 Poisson 38.7 One-Sided CIs and HTs", " 38 MLE Examples: One Sample 38.1 Exponential Family Distributions Suppose \\(X_1, X_2, \\ldots, X_n\\) are iid from some EFD. Then, \\[ \\frac{\\partial}{\\partial \\eta_k} \\ell(\\boldsymbol{\\eta} ; \\boldsymbol{x}) = \\sum_{i=1}^n T_k(x_i) - n \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}) \\] Setting the second equation to 0, it follows that the MLE of \\(\\eta_k\\) is the solution to \\[ \\frac{1}{n} \\sum_{i=1}^n T_k(x_i) = \\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}). \\] where note that \\(\\frac{\\partial}{\\partial \\eta_k} A(\\boldsymbol{\\eta}) = {\\operatorname{E}}[T_k(X)]\\). 38.2 Summary of MLE Statistics In all of these scenarios, \\(Z\\) converges in distribution to Normal\\((0,1)\\) for large \\(n\\). Distribution MLE Std Err \\(Z\\) Statistic Binomial\\((n,p)\\) \\(\\hat{p} = X/n\\) \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) \\(\\frac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}}\\) Normal\\((\\mu, \\sigma^2)\\) \\(\\hat{\\mu} = \\overline{X}\\) \\(\\frac{\\hat{\\sigma}}{\\sqrt{n}}\\) \\(\\frac{\\hat{\\mu} - \\mu}{\\hat{\\sigma}/\\sqrt{n}}\\) Poisson\\((\\lambda)\\) \\(\\hat{\\lambda} = \\overline{X}\\) \\(\\sqrt{\\frac{\\hat{\\lambda}}{n}}\\) \\(\\frac{\\hat{\\lambda} - \\lambda}{\\sqrt{\\hat{\\lambda}/n}}\\) 38.3 Notes For the Normal and Poisson distributions, our model is \\(X_1, X_2, \\ldots, X_n\\) iid from each respective distribution For the Binomial distribution, our model is \\(X \\sim \\mbox{Binomial}(n, p)\\) In the Normal model, \\(\\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n}}\\) is the MLE of \\(\\sigma\\) The above formulas were given in terms of the random variable probability models; on observed data the same formulas are used except we observed data lower case letters, e.g., replace \\(\\overline{X}\\) with \\(\\overline{x}\\) 38.4 Binomial Approximate \\((1-\\alpha)\\)-level two-sided CI: \\[\\left(\\hat{p} - |z_{\\alpha/2}| \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + |z_{\\alpha/2}| \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right)\\] Hypothesis test, \\(H_0: p=p_0\\) vs \\(H_1: p \\not= p_0\\): \\[z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\mbox{ and } \\mbox{p-value} = {\\rm Pr}(|Z^*| \\geq |z|)\\] where \\(Z^*\\) is a Normal\\((0,1)\\) random variable. 38.5 Normal Approximate \\((1-\\alpha)\\)-level two-sided CI: \\[\\left(\\hat{\\mu} - |z_{\\alpha/2}| \\frac{\\hat{\\sigma}}{\\sqrt{n}}, \\hat{\\mu} + |z_{\\alpha/2}| \\frac{\\hat{\\sigma}}{\\sqrt{n}} \\right)\\] Hypothesis test, \\(H_0: \\mu=\\mu_0\\) vs \\(H_1: \\mu \\not= \\mu_0\\): \\[z = \\frac{\\hat{\\mu} - \\mu_0}{\\hat{\\sigma}/\\sqrt{n}} \\mbox{ and } \\mbox{p-value} = {\\rm Pr}(|Z^*| \\geq |z|)\\] where \\(Z^*\\) is a Normal\\((0,1)\\) random variable. 38.6 Poisson Approximate \\((1-\\alpha)\\)-level two-sided CI: \\[\\left(\\hat{\\lambda} - |z_{\\alpha/2}| \\sqrt{\\frac{\\hat{\\lambda}}{n}}, \\hat{\\lambda} + |z_{\\alpha/2}| \\sqrt{\\frac{\\hat{\\lambda}}{n}} \\right)\\] Hypothesis test, \\(H_0: \\lambda=\\lambda_0\\) vs \\(H_1: \\lambda \\not= \\lambda_0\\): \\[z = \\frac{\\hat{\\lambda} - \\lambda_0}{\\sqrt{\\frac{\\hat{\\lambda}}{n}}} \\mbox{ and } \\mbox{p-value} = {\\rm Pr}(|Z^*| \\geq |z|)\\] where \\(Z^*\\) is a Normal\\((0,1)\\) random variable. 38.7 One-Sided CIs and HTs The one-sided versions of these approximate confidence intervals and hypothesis tests work analogously. The procedures shown for the \\(\\mbox{Normal}(\\mu, \\sigma^2)\\) case with known \\(\\sigma^2\\) from last week are utilzied with the appropriate subsitutions as in the above examples. "],
["mle-examples-two-samples.html", "39 MLE Examples: Two-Samples 39.1 Comparing Two Populations 39.2 Two RVs 39.3 Two Sample Means 39.4 Two MLEs 39.5 Poisson 39.6 Normal (Unequal Variances) 39.7 Normal (Equal Variances) 39.8 Binomial 39.9 Example: Binomial CI 39.10 Example: Binomial HT", " 39 MLE Examples: Two-Samples 39.1 Comparing Two Populations So far we have concentrated on analyzing \\(n\\) observations from a single population. However, suppose that we want to do inference to compare two populations? The framework we have described so far is easily extended to accommodate this. 39.2 Two RVs If \\(X\\) and \\(Y\\) are independent rv’s then: \\[{\\rm E}[X - Y] = {\\rm E}[X] - {\\rm E}[Y]\\] \\[{\\rm Var}(X-Y) = {\\rm Var}(X) + {\\rm Var}(Y)\\] 39.3 Two Sample Means Let \\(X_1, X_2, \\ldots, X_{n_1}\\) be iid rv’s with population mean \\(\\mu_1\\) and population variance \\(\\sigma^2_1\\). Let \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) be iid rv’s with population mean \\(\\mu_2\\) and population variance \\(\\sigma^2_2\\). Assume that the two sets of rv’s are independent. Then when the CLT applies to each set of rv’s, as \\(\\min(n_1, n_2) \\rightarrow \\infty\\), it follows that \\[ \\frac{\\overline{X} - \\overline{Y} - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1)\\] 39.4 Two MLEs Suppose \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_{\\theta}\\) and \\(Y_1, Y_2, \\ldots, Y_m \\stackrel{{\\rm iid}}{\\sim} F_{\\gamma}\\) with MLEs \\(\\hat{\\theta}_n\\) and \\(\\hat{\\gamma}_m\\), respectively. Then as \\(\\min(n, m) \\rightarrow \\infty\\), \\[\\frac{\\hat{\\theta}_n - \\hat{\\gamma}_m - (\\theta - \\gamma)}{\\sqrt{\\hat{{\\operatorname{se}}}(\\hat{\\theta}_n)^2 + \\hat{{\\operatorname{se}}}(\\hat{\\gamma}_m)^2}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1).\\] 39.5 Poisson Let \\(X_1, X_2, \\ldots, X_{n_1}\\) be iid \\(\\mbox{Poisson}(\\lambda_1)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) be iid \\(\\mbox{Poisson}(\\lambda_2)\\). We have \\(\\hat{\\lambda}_1 = \\overline{X}\\) and \\(\\hat{\\lambda}_2 = \\overline{Y}\\). As \\(\\min(n_1, n_2) \\rightarrow \\infty\\), \\[ \\frac{\\hat{\\lambda}_1 - \\hat{\\lambda}_2 - (\\lambda_1 - \\lambda_2)}{\\sqrt{\\frac{\\hat{\\lambda}_1}{n_1} + \\frac{\\hat{\\lambda}_2}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1). \\] 39.6 Normal (Unequal Variances) Let \\(X_1, X_2, \\ldots, X_{n_1}\\) be iid \\(\\mbox{Normal}(\\mu_1, \\sigma^2_1)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) be iid \\(\\mbox{Normal}(\\mu_2, \\sigma^2_2)\\). We have \\(\\hat{\\mu}_1 = \\overline{X}\\) and \\(\\hat{\\mu}_2 = \\overline{Y}\\). As \\(\\min(n_1, n_2) \\rightarrow \\infty\\), \\[ \\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\hat{\\sigma}^2_1}{n_1} + \\frac{\\hat{\\sigma}^2_2}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1). \\] 39.7 Normal (Equal Variances) Let \\(X_1, X_2, \\ldots, X_{n_1}\\) be iid \\(\\mbox{Normal}(\\mu_1, \\sigma^2)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2}\\) be iid \\(\\mbox{Normal}(\\mu_2, \\sigma^2)\\). We have \\(\\hat{\\mu}_1 = \\overline{X}\\) and \\(\\hat{\\mu}_2 = \\overline{Y}\\). As \\(\\min(n_1, n_2) \\rightarrow \\infty\\), \\[ \\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\hat{\\sigma}^2}{n_1} + \\frac{\\hat{\\sigma}^2}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1) \\] where \\[ \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n_1}(X_i - \\overline{X})^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline{Y})^2}{n_1 + n_2} \\] 39.8 Binomial Let \\(X \\sim \\mbox{Binomial}(n_1, p_1)\\) and \\(Y \\sim \\mbox{Binomial}(n_2, p_2)\\). We have \\(\\hat{p}_1 = X/n_1\\) and \\(\\hat{p}_2 = Y/n_2\\). As \\(\\min(n_1, n_2) \\rightarrow \\infty\\), \\[ \\frac{\\hat{p}_1 - \\hat{p}_2 - (p_1 - p_2)}{\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1). \\] 39.9 Example: Binomial CI A 95% CI for the difference \\(p_1 - p_2\\) can be obtained by unfolding the above pivotal statistic: \\[\\left((\\hat{p}_1 - \\hat{p}_2) - 1.96 \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\right.,\\] \\[\\left. (\\hat{p}_1 - \\hat{p}_2) + 1.96 \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\right)\\] 39.10 Example: Binomial HT Suppose we wish to test \\(H_0: p_1 = p_2\\) vs \\(H_1: p_1 \\not= p_2\\). First form the \\(z\\)-statistic: \\[ z = \\frac{\\hat{p}_1 - \\hat{p}_2 }{\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}}. \\] Now, calculate the p-value: \\[ {\\rm Pr}(|Z^*| \\geq |z|) \\] where \\(Z^*\\) is a Normal(0,1) random variable. "],
["the-t-distribution.html", "40 The t Distribution 40.1 Normal, Unknown Variance 40.2 Aside: Chi-Square Distribution 40.3 Theoretical Basis of the t 40.4 When Is t Utilized? 40.5 t vs Normal 40.6 t Percentiles 40.7 Confidence Intervals 40.8 Hypothesis Tests 40.9 Two-Sample t-Distribution 40.10 Two-Sample t-Distribution 40.11 Two-Sample t-Distributions", " 40 The t Distribution 40.1 Normal, Unknown Variance Suppose a sample of \\(n\\) data points is modeled by \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu, \\sigma^2)\\) where \\(\\sigma^2\\) is unknown. Recall that \\(S = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}}\\) is the sample standard deviation. The statistic \\[\\frac{\\overline{X} - \\mu}{S/\\sqrt{n}}\\] has a \\(t_{n-1}\\) distribution, a t-distribution with \\(n-1\\) degrees of freedom. 40.2 Aside: Chi-Square Distribution Suppose \\(Z_1, Z_2, \\ldots, Z_v \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(0, 1)\\). Then \\(Z_1^2 + Z_2^2 + \\cdots + Z_v^2\\) has a \\(\\chi^2_v\\) distribution, where \\(v\\) is the degrees of freedom. This \\(\\chi^2_v\\) rv has a pdf, expected value equal to \\(v\\), and variance equal to \\(2v\\). Also, \\[\\frac{(n-1) S^2}{\\sigma^2} \\sim \\chi^2_{n-1}.\\] 40.3 Theoretical Basis of the t Suppose that \\(Z \\sim \\mbox{Normal}(0,1)\\), \\(X \\sim \\chi^2_v\\), and \\(Z\\) and \\(X\\) are independent. Then \\(\\frac{Z}{\\sqrt{X/v}}\\) has a \\(t_v\\) distribution. Since \\(\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mbox{Normal}(0,1)\\) and \\(\\overline{X}\\) and \\(S^2\\) are independent (shown later), it follows that the following has a \\(t_{n-1}\\) distribution: \\[\\frac{\\overline{X} - \\mu}{S/\\sqrt{n}}.\\] 40.4 When Is t Utilized? The t distribution and its corresponding CI’s and HT’s are utilized when the data are Normal (or approximately Normal) and \\(n\\) is small Small typically means that \\(n &lt; 30\\) In this case the inference based on the t distribution will be more accurate When \\(n \\geq 30\\), there is very little difference between using \\(t\\)-statistics and \\(z\\)-statistics 40.5 t vs Normal 40.6 t Percentiles We calculated percentiles of the Normal(0,1) distribution (e.g., \\(z_\\alpha\\)). We can do the analogous calculation with the t distribution. Let \\(t_\\alpha\\) be the \\(\\alpha\\) percentile of the t distribution. Examples: &gt; qt(0.025, df=4) # alpha = 0.025 [1] -2.776445 &gt; qt(0.05, df=4) [1] -2.131847 &gt; qt(0.95, df=4) [1] 2.131847 &gt; qt(0.975, df=4) [1] 2.776445 40.7 Confidence Intervals Here is a \\((1-\\alpha)\\)-level CI for \\(\\mu\\) using this distribution: \\[ \\left(\\hat{\\mu} - |t_{\\alpha/2}| \\frac{s}{\\sqrt{n}}, \\hat{\\mu} + |t_{\\alpha/2}| \\frac{s}{\\sqrt{n}} \\right), \\] where as before \\(\\hat{\\mu} = \\overline{x}\\). This produces a wider CI than the \\(z\\) statistic analogue. 40.8 Hypothesis Tests Suppose we want to test \\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\not= \\mu_0\\) where \\(\\mu_0\\) is a known, given number. The t-statistic is \\[ t = \\frac{\\hat{\\mu} - \\mu_0}{\\frac{s}{\\sqrt{n}}} \\] with p-value \\[ {\\rm Pr}(|T^*| \\geq |t|) \\] where \\(T^* \\sim t_{n-1}\\). 40.9 Two-Sample t-Distribution Let \\(X_1, X_2, \\ldots, X_{n_1} \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu_1, \\sigma^2_1)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2} \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu_2, \\sigma^2_2)\\) have unequal variances. We have \\(\\hat{\\mu}_1 = \\overline{X}\\) and \\(\\hat{\\mu}_2 = \\overline{Y}\\). The unequal variance two-sample t-statistic is \\[ t = \\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{S^2_1}{n_1} + \\frac{S^2_2}{n_2}}}. \\] 40.10 Two-Sample t-Distribution Let \\(X_1, X_2, \\ldots, X_{n_1} \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu_1, \\sigma^2)\\) and \\(Y_1, Y_2, \\ldots, Y_{n_2} \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu_2, \\sigma^2)\\) have equal variance. We have \\(\\hat{\\mu}_1 = \\overline{X}\\) and \\(\\hat{\\mu}_2 = \\overline{Y}\\). The equal variance two-sample t-statistic is \\[ t = \\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{S^2}{n_1} + \\frac{S^2}{n_2}}}. \\] where \\[ S^2 = \\frac{\\sum_{i=1}^{n_1}(X_i - \\overline{X})^2 + \\sum_{i=1}^{n_2}(Y_i - \\overline{Y})^2}{n_1 + n_2 - 2}. \\] 40.11 Two-Sample t-Distributions When the two populations have equal variances, the pivotal t-statistic follows a \\(t_{n_1 + n_2 -2}\\) distribution. When there are unequal variances, the pivotal t-statistic follows a t distribution where the degrees of freedom comes from an approximation using the Welch–Satterthwaite equation (which R calculates). "],
["inference-in-r.html", "41 Inference in R 41.1 BSDA Package 41.2 Example: Poisson 41.3 Direct Calculations 41.4 Commonly Used Functions 41.5 About These Functions 41.6 Normal Data: “Davis” Data Set 41.7 Height vs Weight 41.8 An Error? 41.9 Updated Height vs Weight 41.10 Density Plots of Height 41.11 Density Plots of Weight 41.12 t.test() Function 41.13 Two-Sided Test of Male Height 41.14 Output of t.test() 41.15 Tidying the Output 41.16 Two-Sided Test of Female Height 41.17 Difference of Two Means 41.18 Test with Equal Variances 41.19 Paired Sample Test (v. 1) 41.20 Paired Sample Test (v. 2) 41.21 The Coin Flip Example 41.22 binom.test() 41.23 alternative = &quot;greater&quot; 41.24 alternative = &quot;less&quot; 41.25 prop.test() 41.26 An Observation 41.27 Wording of Surveys 41.28 The Data 41.29 Inference on the Difference 41.30 90% Confidence Interval 41.31 Poisson Data: poisson.test() 41.32 Example: RNA-Seq 41.33 \\(H_1: \\lambda_1 \\not= \\lambda_2\\) 41.34 \\(H_1: \\lambda_1 &lt; \\lambda_2\\) 41.35 \\(H_1: \\lambda_1 &gt; \\lambda_2\\) 41.36 Question", " 41 Inference in R 41.1 BSDA Package &gt; install.packages(&quot;BSDA&quot;) &gt; library(BSDA) &gt; str(z.test) function (x, y = NULL, alternative = &quot;two.sided&quot;, mu = 0, sigma.x = NULL, sigma.y = NULL, conf.level = 0.95) 41.2 Example: Poisson Apply z.test(): &gt; set.seed(210) &gt; n &lt;- 40 &gt; lam &lt;- 14 &gt; x &lt;- rpois(n=n, lambda=lam) &gt; lam.hat &lt;- mean(x) &gt; stddev &lt;- sqrt(lam.hat) &gt; z.test(x=x, sigma.x=stddev, mu=lam) One-sample z-Test data: x z = 0.41885, p-value = 0.6753 alternative hypothesis: true mean is not equal to 14 95 percent confidence interval: 13.08016 15.41984 sample estimates: mean of x 14.25 41.3 Direct Calculations Confidence interval: &gt; lam.hat &lt;- mean(x) &gt; lam.hat [1] 14.25 &gt; stderr &lt;- sqrt(lam.hat)/sqrt(n) &gt; lam.hat - abs(qnorm(0.025)) * stderr # lower bound [1] 13.08016 &gt; lam.hat + abs(qnorm(0.025)) * stderr # upper bound [1] 15.41984 Hypothesis test: &gt; z &lt;- (lam.hat - lam)/stderr &gt; z # test statistic [1] 0.4188539 &gt; 2 * pnorm(-abs(z)) # two-sided p-value [1] 0.6753229 41.4 Commonly Used Functions R has the following functions for doing inference on some of the distributions we have considered. Normal: t.test() Binomial: binomial.test() or prop.test() Poisson: poisson.test() These perform one-sample and two-sample hypothesis testing and confidence interval construction for both the one-sided and two-sided cases. 41.5 About These Functions We covered a convenient, unified MLE framework that allows us to better understand how confidence intervals and hypothesis testing are performed However, this framework requires large sample sizes and is not necessarily the best method to apply in all circumstances The above R functions are versatile functions for analyzing Normal, Binomial, and Poisson distributed data (or approximations thereof) that use much broader theory and methods than we have covered However, the arguments these functions take and the ouput of the functions are in line with the framework that we have covered 41.6 Normal Data: “Davis” Data Set &gt; library(&quot;car&quot;) &gt; data(&quot;Davis&quot;) &gt; htwt &lt;- tbl_df(Davis) &gt; htwt # A tibble: 200 x 5 sex weight height repwt repht &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 M 77 182 77 180 2 F 58 161 51 159 3 F 53 161 54 158 4 M 68 177 70 175 5 F 59 157 59 155 6 M 76 170 76 165 7 M 76 167 77 165 8 M 69 186 73 180 9 M 71 178 71 175 10 M 65 171 64 170 # … with 190 more rows 41.7 Height vs Weight &gt; ggplot(htwt) + + geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) + + scale_colour_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 41.8 An Error? &gt; which(htwt$height &lt; 100) [1] 12 &gt; htwt[12,] # A tibble: 1 x 5 sex weight height repwt repht &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 F 166 57 56 163 &gt; htwt[12,c(2,3)] &lt;- htwt[12,c(3,2)] 41.9 Updated Height vs Weight &gt; ggplot(htwt) + + geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) + + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 41.10 Density Plots of Height &gt; ggplot(htwt) + + geom_density(aes(x=height, color=sex), size=1.5) + + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 41.11 Density Plots of Weight &gt; ggplot(htwt) + + geom_density(aes(x=weight, color=sex), size=1.5) + + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 41.12 t.test() Function From the help file… Usage t.test(x, ...) ## Default S3 method: t.test(x, y = NULL, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...) ## S3 method for class &#39;formula&#39; t.test(formula, data, subset, na.action, ...) 41.13 Two-Sided Test of Male Height &gt; m_ht &lt;- htwt %&gt;% filter(sex==&quot;M&quot;) %&gt;% select(height) &gt; testresult &lt;- t.test(x = m_ht$height, mu=177) &gt; class(testresult) [1] &quot;htest&quot; &gt; is.list(testresult) [1] TRUE 41.14 Output of t.test() &gt; names(testresult) [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;conf.int&quot; &quot;estimate&quot; [6] &quot;null.value&quot; &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; &gt; testresult One Sample t-test data: m_ht$height t = 1.473, df = 87, p-value = 0.1443 alternative hypothesis: true mean is not equal to 177 95 percent confidence interval: 176.6467 179.3760 sample estimates: mean of x 178.0114 41.15 Tidying the Output &gt; library(broom) &gt; tidy(testresult) # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 178. 1.47 0.144 87 177. 179. One S… # … with 1 more variable: alternative &lt;chr&gt; 41.16 Two-Sided Test of Female Height &gt; f_ht &lt;- htwt %&gt;% filter(sex==&quot;F&quot;) %&gt;% select(height) &gt; t.test(x = f_ht$height, mu = 164) One Sample t-test data: f_ht$height t = 1.3358, df = 111, p-value = 0.1844 alternative hypothesis: true mean is not equal to 164 95 percent confidence interval: 163.6547 165.7739 sample estimates: mean of x 164.7143 41.17 Difference of Two Means &gt; t.test(x = m_ht$height, y = f_ht$height) Welch Two Sample t-test data: m_ht$height and f_ht$height t = 15.28, df = 174.29, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 11.57949 15.01467 sample estimates: mean of x mean of y 178.0114 164.7143 41.18 Test with Equal Variances &gt; htwt %&gt;% group_by(sex) %&gt;% summarize(sd(height)) # A tibble: 2 x 2 sex `sd(height)` &lt;fct&gt; &lt;dbl&gt; 1 F 5.66 2 M 6.44 &gt; t.test(x = m_ht$height, y = f_ht$height, var.equal = TRUE) Two Sample t-test data: m_ht$height and f_ht$height t = 15.519, df = 198, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 11.60735 14.98680 sample estimates: mean of x mean of y 178.0114 164.7143 41.19 Paired Sample Test (v. 1) First take the difference between the paired observations. Then apply the one-sample t-test. &gt; htwt &lt;- htwt %&gt;% mutate(diffwt = (weight - repwt), + diffht = (height - repht)) &gt; t.test(x = htwt$diffwt) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.00546 0.0319 0.975 182 -0.332 0.343 One S… # … with 1 more variable: alternative &lt;chr&gt; &gt; t.test(x = htwt$diffht) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 2.08 13.5 2.64e-29 182 1.77 2.38 One S… # … with 1 more variable: alternative &lt;chr&gt; 41.20 Paired Sample Test (v. 2) Enter each sample into the t.test() function, but use the paired=TRUE argument. This is operationally equivalent to the previous version. &gt; t.test(x=htwt$weight, y=htwt$repwt, paired=TRUE) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.00546 0.0319 0.975 182 -0.332 0.343 Paire… # … with 1 more variable: alternative &lt;chr&gt; &gt; t.test(x=htwt$height, y=htwt$repht, paired=TRUE) %&gt;% tidy() # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 2.08 13.5 2.64e-29 182 1.77 2.38 Paire… # … with 1 more variable: alternative &lt;chr&gt; &gt; htwt %&gt;% select(height, repht) %&gt;% na.omit() %&gt;% + summarize(mean(height), mean(repht)) # A tibble: 1 x 2 `mean(height)` `mean(repht)` &lt;dbl&gt; &lt;dbl&gt; 1 171. 168. 41.21 The Coin Flip Example I flip it 20 times and it lands on heads 16 times. My data is \\(x=16\\) heads out of \\(n=20\\) flips. My data generation model is \\(X \\sim \\mbox{Binomial}(20, p)\\). I form the statistic \\(\\hat{p} = 16/20\\) as an estimate of \\(p\\). Let’s do hypothesis testing and confidence interval construction on these data. 41.22 binom.test() &gt; str(binom.test) function (x, n, p = 0.5, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95) &gt; binom.test(x=16, n=20, p = 0.5) Exact binomial test data: 16 and 20 number of successes = 16, number of trials = 20, p-value = 0.01182 alternative hypothesis: true probability of success is not equal to 0.5 95 percent confidence interval: 0.563386 0.942666 sample estimates: probability of success 0.8 41.23 alternative = &quot;greater&quot; Tests \\(H_0: p \\leq 0.5\\) vs. \\(H_1: p &gt; 0.5\\). &gt; binom.test(x=16, n=20, p = 0.5, alternative=&quot;greater&quot;) Exact binomial test data: 16 and 20 number of successes = 16, number of trials = 20, p-value = 0.005909 alternative hypothesis: true probability of success is greater than 0.5 95 percent confidence interval: 0.5989719 1.0000000 sample estimates: probability of success 0.8 41.24 alternative = &quot;less&quot; Tests \\(H_0: p \\geq 0.5\\) vs. \\(H_1: p &lt; 0.5\\). &gt; binom.test(x=16, n=20, p = 0.5, alternative=&quot;less&quot;) Exact binomial test data: 16 and 20 number of successes = 16, number of trials = 20, p-value = 0.9987 alternative hypothesis: true probability of success is less than 0.5 95 percent confidence interval: 0.0000000 0.9286461 sample estimates: probability of success 0.8 41.25 prop.test() This is a “large \\(n\\)” inference method that is very similar to our \\(z\\)-statistic approach. &gt; str(prop.test) function (x, n, p = NULL, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95, correct = TRUE) &gt; prop.test(x=16, n=20, p=0.5) 1-sample proportions test with continuity correction data: 16 out of 20, null probability 0.5 X-squared = 6.05, df = 1, p-value = 0.01391 alternative hypothesis: true p is not equal to 0.5 95 percent confidence interval: 0.5573138 0.9338938 sample estimates: p 0.8 41.26 An Observation &gt; p &lt;- binom.test(x=16, n=20, p = 0.5)$p.value &gt; binom.test(x=16, n=20, p = 0.5, conf.level=(1-p)) Exact binomial test data: 16 and 20 number of successes = 16, number of trials = 20, p-value = 0.01182 alternative hypothesis: true probability of success is not equal to 0.5 98.81821 percent confidence interval: 0.5000000 0.9625097 sample estimates: probability of success 0.8 Exercise: Figure out what happened here. 41.27 Wording of Surveys The way a question is phrased can influence a person’s response. For example, Pew Research Center conducted a survey with the following question: “As you may know, by 2014 nearly all Americans will be required to have health insurance. [People who do not buy insurance will pay a penalty] while [People who cannot afford it will receive financial help from the government]. Do you approve or disapprove of this policy?” For each randomly sampled respondent, the statements in brackets were randomized: either they were kept in the order given above, or the two statements were reversed. Credit: This example comes from Open Intro Statistics, Exercise 6.10. 41.28 The Data The following table shows the results of this experiment. 2nd Statement Sample Size Approve Law Disapprove Law Other “people who cannot afford it will receive financial help from the government” 771 47 49 3 “people who do not buy it will pay a penalty” 732 34 63 3 41.29 Inference on the Difference Create and interpret a 90% confidence interval of the difference in approval. Also perform a hyppthesis test that the approval rates are equal. &gt; x &lt;- round(c(0.47*771, 0.34*732)) &gt; n &lt;- round(c(771*0.97, 732*0.97)) &gt; prop.test(x=x, n=n, conf.level=0.90) 2-sample test for equality of proportions with continuity correction data: x out of n X-squared = 26.023, df = 1, p-value = 3.374e-07 alternative hypothesis: two.sided 90 percent confidence interval: 0.08979649 0.17670950 sample estimates: prop 1 prop 2 0.4839572 0.3507042 41.30 90% Confidence Interval Let’s use MLE theory to construct of a two-sided 90% CI. &gt; p1.hat &lt;- 0.47 &gt; n1 &lt;- 771 &gt; p2.hat &lt;- 0.34 &gt; n2 &lt;- 732 &gt; stderr &lt;- sqrt(p1.hat*(1-p1.hat)/n1 + p2.hat*(1-p2.hat)/n2) &gt; &gt; # the 90% CI &gt; (p1.hat - p2.hat) + c(-1,1)*abs(qnorm(0.05))*stderr [1] 0.08872616 0.17127384 41.31 Poisson Data: poisson.test() &gt; str(poisson.test) function (x, T = 1, r = 1, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95) From the help: Arguments x number of events. A vector of length one or two. T time base for event count. A vector of length one or two. r hypothesized rate or rate ratio alternative indicates the alternative hypothesis and must be one of &quot;two.sided&quot;, &quot;greater&quot; or &quot;less&quot;. You can specify just the initial letter. conf.level confidence level for the returned confidence interval. 41.32 Example: RNA-Seq RNA-Seq gene expression was measured for p53 lung tissue in 12 healthy individuals and 14 individuals with lung cancer. The counts were given as follows. Healthy: 82 64 66 88 65 81 85 87 60 79 80 72 Cancer: 59 50 60 60 78 69 70 67 72 66 66 68 54 62 It is hypothesized that p53 expression is higher in healthy individuals. Test this hypothesis, and form a 99% CI. 41.33 \\(H_1: \\lambda_1 \\not= \\lambda_2\\) &gt; healthy &lt;- c(82, 64, 66, 88, 65, 81, 85, 87, 60, 79, 80, 72) &gt; cancer &lt;- c(59, 50, 60, 60, 78, 69, 70, 67, 72, 66, 66, 68, + 54, 62) &gt; poisson.test(x=c(sum(healthy), sum(cancer)), T=c(12,14), + conf.level=0.99) Comparison of Poisson rates data: c(sum(healthy), sum(cancer)) time base: c(12, 14) count1 = 909, expected count1 = 835.38, p-value = 0.0005739 alternative hypothesis: true rate ratio is not equal to 1 99 percent confidence interval: 1.041626 1.330051 sample estimates: rate ratio 1.177026 41.34 \\(H_1: \\lambda_1 &lt; \\lambda_2\\) &gt; poisson.test(x=c(sum(healthy), sum(cancer)), T=c(12,14), + alternative=&quot;less&quot;, conf.level=0.99) Comparison of Poisson rates data: c(sum(healthy), sum(cancer)) time base: c(12, 14) count1 = 909, expected count1 = 835.38, p-value = 0.9998 alternative hypothesis: true rate ratio is less than 1 99 percent confidence interval: 0.000000 1.314529 sample estimates: rate ratio 1.177026 41.35 \\(H_1: \\lambda_1 &gt; \\lambda_2\\) &gt; poisson.test(x=c(sum(healthy), sum(cancer)), T=c(12,14), + alternative=&quot;greater&quot;, conf.level=0.99) Comparison of Poisson rates data: c(sum(healthy), sum(cancer)) time base: c(12, 14) count1 = 909, expected count1 = 835.38, p-value = 0.0002881 alternative hypothesis: true rate ratio is greater than 1 99 percent confidence interval: 1.053921 Inf sample estimates: rate ratio 1.177026 41.36 Question Which analysis is the more informative and scientifically correct one, and why? "],
["likelihood-ratio-tests.html", "42 Likelihood Ratio Tests 42.1 General Set-up 42.2 Significance Regions 42.3 P-values 42.4 Example: Wald Test 42.5 Neyman-Pearson Lemma 42.6 Simple vs. Composite Hypotheses 42.7 General Hypothesis Tests 42.8 Composite \\(H_0\\) 42.9 Generalized LRT 42.10 Null Distribution of Gen. LRT 42.11 Example: Poisson 42.12 Example: Normal", " 42 Likelihood Ratio Tests 42.1 General Set-up Most hypothesis testing procedures can be formulated so that a test statistic, \\(S(\\boldsymbol{x})\\) is applied to the data \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_n)^T\\) so that: \\(S(\\boldsymbol{x}) \\geq 0\\) The larger \\(S(\\boldsymbol{x})\\) is, the more significant the test is (i.e., the more evidence against the null in favor of the alternative) The p-value is \\(p(\\boldsymbol{x}) = \\Pr(S(\\boldsymbol{X^*}) \\geq S(\\boldsymbol{x}))\\) where \\(S(\\boldsymbol{X^*})\\) is distributed according to the null distribution 42.2 Significance Regions A level \\(\\alpha\\) test is a signficance rule (i.e., a rule for calling a test statistically significant) that results in a false positive rate (i.e., Type I error rate) of \\(\\alpha\\). Under our set-up, significance regions take the form: \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: S(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\},\\] where \\(c_{1-\\alpha}\\) is the \\((1-\\alpha)\\) percentile of \\(S(\\boldsymbol{X}^*)\\) so that \\(\\Pr(S(\\boldsymbol{X}^*) \\geq c_{1-\\alpha}) = \\alpha\\). We restrict \\(0 &lt; \\alpha &lt; 1\\). Note that if \\(\\alpha&#39; \\leq \\alpha\\) then \\(\\Gamma_{\\alpha&#39;} \\subseteq \\Gamma_\\alpha\\). 42.3 P-values A p-value can be defined in terms of significance regions: \\[p(\\boldsymbol{x}) = \\min \\left\\{\\alpha: \\boldsymbol{x} \\in \\Gamma_\\alpha \\right\\}\\] 42.4 Example: Wald Test Consider the hypothesis test, \\(H_0: \\theta=\\theta_0\\) vs \\(H_1: \\theta \\not= \\theta_0\\). Let \\(\\hat{\\theta}_n(\\boldsymbol{x})\\) be the MLE of \\(\\theta\\). We have \\[S(\\boldsymbol{x}) = \\frac{\\left| \\hat{\\theta}_n(\\boldsymbol{x}) - \\theta_0 \\right|}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}_n(\\boldsymbol{x})\\right)},\\] \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: S(\\boldsymbol{x}) \\geq |z_{\\alpha/2}| \\right\\},\\] where \\(z_{\\alpha/2}\\) is the \\(\\alpha/2\\) percentile of the Normal\\((0,1)\\) distribution. 42.5 Neyman-Pearson Lemma Suppose we are testing \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta = \\theta_1\\) where in practice \\(\\theta_0\\) and \\(\\theta_1\\) are known, fixed quantities. The most powerful test has statistic and significance regions: \\[ S(\\boldsymbol{x}) = \\frac{f(\\boldsymbol{x}; \\theta_1)}{f(\\boldsymbol{x}; \\theta_0)} = \\frac{L(\\theta_1; \\boldsymbol{x})}{L(\\theta_0; \\boldsymbol{x})} \\] \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: S(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\},\\] where \\(c_{1-\\alpha}\\) is the \\((1-\\alpha)\\) percentile of \\(S(\\boldsymbol{X}^*)\\) so that \\(\\operatorname{Pr}_{\\theta_0}(S(\\boldsymbol{X}^*) \\geq c_{1-\\alpha}) = \\alpha\\). 42.6 Simple vs. Composite Hypotheses A simple hypothesis is defined in terms of a single value, e.g., \\(H_0: \\mu=0\\) \\(H_0: p = p_0\\) where \\(p_0\\) is a placehold for a known, fixed number in practice \\(H_1: \\lambda=5\\) A composite hypothesis is defined by multiple values, e.g., \\(H_0: \\mu \\leq 0\\) vs \\(H_1: \\mu &gt; 0\\) \\(H_0: p_1 = p_2\\), where \\(p_1\\) and \\(p_2\\) are two unknown parameters corresponding to two populations \\(H_1: \\mu \\not= 0\\) 42.7 General Hypothesis Tests Let \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_\\theta\\) where \\(\\theta \\in \\Theta\\). Let \\(\\Theta_0, \\Theta_1 \\subseteq \\Theta\\) so that \\(\\Theta_0 \\cap \\Theta_1 = \\varnothing\\) and \\(\\Theta_0 \\cup \\Theta_1 = \\Theta\\). The hypothesis test is: \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\) If \\(\\Theta_0\\) or \\(\\Theta_1\\) contain more than one value then the corresponding hypothesis is composite. 42.8 Composite \\(H_0\\) The significance regions indexed by their level \\(\\alpha\\) are determined so that: \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: S(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\},\\] where \\(c_{1-\\alpha}\\) is such that \\[\\max_{\\theta \\in \\Theta_0} \\Pr(S(\\boldsymbol{X^*}) \\geq c_{1-\\alpha}) = \\alpha.\\] In this case, \\[\\begin{align*} p(\\boldsymbol{x}) &amp; = \\min \\left\\{\\alpha: \\boldsymbol{x} \\in \\Gamma_\\alpha \\right\\} \\\\ &amp; = \\max_{\\theta \\in \\Theta_0} \\operatorname{Pr}_\\theta (S(\\boldsymbol{X}^*) \\geq S(\\boldsymbol{x})) \\end{align*}\\] 42.9 Generalized LRT Let \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} F_\\theta\\) where \\(\\theta \\in \\Theta\\) and we are testing \\(H_0: \\theta \\in \\Theta_0\\) vs \\(H_1: \\theta \\in \\Theta_1\\). The generalized LRT utilizes test statistic and significance regions: \\[\\lambda(\\boldsymbol{x}) = \\frac{\\max_{\\theta \\in \\Theta} L(\\theta; \\boldsymbol{x})}{\\max_{\\theta \\in \\Theta_0} L(\\theta; \\boldsymbol{x})} = \\frac{L\\left(\\hat{\\theta}; \\boldsymbol{x}\\right)}{L\\left(\\hat{\\theta}_0; \\boldsymbol{x}\\right)} \\] \\[\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: \\lambda(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\}\\] 42.10 Null Distribution of Gen. LRT The null distribution of \\(\\lambda(\\boldsymbol{x})\\) under “certain regularity assumptions” can be shown to be such that, as \\(n \\rightarrow \\infty\\), \\[ 2 \\log \\lambda(\\boldsymbol{x}) \\stackrel{D}{\\longrightarrow} \\chi^2_v \\] where \\(v = \\operatorname{dim}(\\Theta) - \\operatorname{dim}(\\Theta_0)\\). The significance regions can be more easily written as \\(\\Gamma_\\alpha = \\left\\{\\boldsymbol{x}: 2 \\log \\lambda(\\boldsymbol{x}) \\geq c_{1-\\alpha} \\right\\}\\) where \\(c_{1-\\alpha}\\) is the \\(1-\\alpha\\) percentile of the \\(\\chi^2_v\\) distribution. 42.11 Example: Poisson Let \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} \\mbox{Poisson}(\\theta)\\) where \\(\\theta &gt; 0\\) and we are testing \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\not= \\theta_0\\). The unconstrained MLE is \\(\\hat{\\theta} = \\overline{x}\\). The generalized LRT statistic \\[ 2 \\log \\lambda(\\boldsymbol{x}) = 2 \\log \\frac{e^{-n \\hat{\\theta}} \\hat{\\theta}^{\\sum x_i} }{e^{-n \\theta_0} \\theta_0^{\\sum x_i} } = 2 n \\left[ (\\theta_0 - \\hat{\\theta}) - \\hat{\\theta} \\log(\\theta_0 / \\hat{\\theta}) \\right] \\] which has an asymptotic \\(\\chi^2_1\\) null distribution. 42.12 Example: Normal Let \\(X_1, X_2, \\ldots, X_n \\stackrel{{\\rm iid}}{\\sim} \\mbox{Normal}(\\mu, \\sigma^2)\\) and we are testing \\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\not= \\mu_0\\). The generalized LRT can be applied for multidimensional parameter spaces \\(\\boldsymbol{\\Theta}\\) as well. The statistic, which has asymptotic null distribution \\(\\chi^2_1\\), is \\[ 2 \\log \\lambda(\\boldsymbol{x}) = 2 \\log \\left( \\frac{\\hat{\\sigma}^2_0}{\\hat{\\sigma}^2} \\right)^{n/2} \\] where \\[ \\hat{\\sigma}^2_0 = \\frac{\\sum_{i=1}^n (x_i - \\mu_0)^2}{n}, \\quad \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n}. \\] "],
["likelihood-function-2.html", "43 Likelihood Function 43.1 Same MLE, Different \\(L(\\theta | \\boldsymbol{x})\\) 43.2 Weighted Likelihood Estimate 43.3 Conditional Expected Value 43.4 Standard Errror", " 43 Likelihood Function 43.1 Same MLE, Different \\(L(\\theta | \\boldsymbol{x})\\) 43.2 Weighted Likelihood Estimate Instead of employing estimator \\(\\hat{\\theta}_{{\\rm MLE}} = \\operatorname{argmax}_\\theta L(\\theta ; \\boldsymbol{x})\\), consider instead an arbitrary weight function, \\(g(\\theta)\\). We could take a weighted average of the likelihood function, assuming all of the integrals below exist. \\[ \\tilde{\\theta} = \\frac{\\int \\theta g(\\theta) L(\\theta ; \\boldsymbol{x}) d\\theta}{\\int g(\\theta) L(\\theta ; \\boldsymbol{x}) d\\theta} \\] 43.3 Conditional Expected Value If we set \\[ h(\\theta | \\boldsymbol{x}) = \\frac{g(\\theta) L(\\theta ; \\boldsymbol{x})}{\\int g(\\theta^*) L(\\theta^* ; \\boldsymbol{x}) d\\theta^*} \\] then \\(h(\\theta | \\boldsymbol{x})\\) is a probability density function and \\[ \\tilde{\\theta} = {\\operatorname{E}}_{h(\\theta | \\boldsymbol{x})}[\\theta]. \\] 43.4 Standard Errror Consider the model, \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{\\theta}\\). Since \\(\\tilde{\\theta} = {\\operatorname{E}}_{h(\\theta | \\boldsymbol{x})}[\\theta]\\) is a function of the data \\(\\boldsymbol{x}\\), it follows that in most circumstances it should be possible to obtain an approximation to its standard error, \\(\\sqrt{{\\operatorname{Var}}(\\tilde{\\theta})}\\) and an estimate of the standard error. This allows for frequentist inference of estimates based on a weighted integral of the likelihood function. "],
["bayesian-inference.html", "44 Bayesian Inference 44.1 Frequentist Probability 44.2 Bayesian Probability 44.3 The Framework 44.4 An Example 44.5 Calculations 44.6 In Practice 44.7 Goal 44.8 Advantages 44.9 Computation", " 44 Bayesian Inference 44.1 Frequentist Probability The inference framework we have covered so far uses a frequentist intepretation of probability. We made statements such as, “If we repeat this study over and over, the long run frequency is such that…” 44.2 Bayesian Probability Traditional Bayesian inference is based on a different interpretation of probability, that probability is a measure of subjective belief. We will call this “subjective Bayesian statistics.” 44.3 The Framework A prior probability distribution is introduced for an unknown parameter, which is a probability distribution on the unknown parameter that captures one’s subjective belief about its possible values. The posterior probability distributuon of the parameter is then calculated using Bayes theorem once data are observed. Analogs of confidence intervals and hypothesis tests can then be obtained through the posterior distribution. 44.4 An Example Prior: \\(P \\sim \\mbox{Uniform}(0,1)\\) Data generating distribution: \\(X|P=p \\sim \\mbox{Binomial}(n,p)\\) Posterior pdf (via Bayes Theorem): \\[\\begin{align*} f(p | X=x) &amp; = \\frac{\\Pr(X=x | P=p) f(p)}{\\Pr(X=x)} \\\\ &amp; = \\frac{\\Pr(X=x | P=p) f(p)}{\\int \\Pr(X=x | P=p^*) f(p^*) dp^*} \\end{align*}\\] 44.5 Calculations In the previous example, it is possible to analytically calculate the posterior distribution. (In the example, it is a Beta distribution with parameters that involve \\(x\\).) However, this is often impossible. Bayesian inference often involves complicated and intensive calculations to numerically approximate the posterior probability distribution. 44.6 In Practice Although the Bayesian inference framework has its roots in the subjective view of probability, in modern times this philosophical aspect is often ignored or unimportant. When subjectivism is ignored, is this really Bayesian statistics, or is it frequentist statistics that includes a probability model on the unknown parameter(s) that employes Bayes Theorem? Bayesian inference is often used because it provides a flexible and sometimes superior model for real world problems. But the interpretation and evaluation are often tacitly frequentist. There are very few pure subjective Bayesians working in the natural sciences or in technology industries. 44.7 Goal Suppose we model \\((X_1, X_2, \\ldots, X_n) | \\theta \\ {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\ F_{\\theta}\\) with prior distribution \\(\\theta \\sim F_{\\tau}\\) where it should be noted that \\(\\theta\\) also depends on (possibly unknown or subjective) parameter(s) \\(\\tau\\). The ultimate goal is to determine the posterior distribution of \\(\\theta | \\boldsymbol{X}\\) through Bayes theorem: \\[ f(\\theta | \\boldsymbol{X}) = \\frac{f(\\boldsymbol{X} | \\theta) f(\\theta)}{f(\\boldsymbol{X})} = \\frac{f(\\boldsymbol{X} | \\theta) f(\\theta)}{\\int f(\\boldsymbol{X} | \\theta^*) f(\\theta^*) d\\theta^*}. \\] If there is a true fixed value of \\(\\theta\\), then a well-behaved model should be so that \\(f(\\theta | \\boldsymbol{X})\\) concentrates around this fixed value as \\(n \\rightarrow \\infty\\). 44.8 Advantages Statements on measures of uncertainty and inference are easier to make Often superior numerical stability to the estimates Data across studies or multiple samples easier to combine (e.g., how to combine frequentist p-values?) High-dimensional inference works especially well in a Bayesian framework 44.9 Computation Bayesian inference can be particularly computationally intensive. The challenge is usually in calculating the denominator of the right hand side of Bayes thereom, \\(f(\\boldsymbol{X})\\): \\[ f(\\theta | \\boldsymbol{X}) = \\frac{f(\\boldsymbol{X} | \\theta) f(\\theta)}{f(\\boldsymbol{X})} \\] Markov chain Monte Carlo methods and variational inference methods are particularly popular for dealing with the numerical challenges of obtain good estimates of the posterior distribution. "],
["estimation.html", "45 Estimation 45.1 Assumptions 45.2 Posterior Distribution 45.3 Posterior Expectation 45.4 Posterior Interval 45.5 Maximum A Posteriori Probability 45.6 Loss Functions 45.7 Bayes Risk 45.8 Bayes Estimators", " 45 Estimation 45.1 Assumptions We will assume that \\((X_1, X_2, \\ldots, X_n) | \\theta {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{\\theta}\\) with prior distribution \\(\\theta \\sim F_{\\tau}\\) unless stated otherwise. Shorthand for the former is \\(\\boldsymbol{X} | \\theta {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{\\theta}\\). We will write the pdf or pmf of \\(X\\) as \\(f(x | \\theta)\\) as opposed to \\(f(x ; \\theta)\\) because in the Bayesian framework this actually represents conditional probability. We will write the pdf or pmf of \\(\\theta\\) as \\(f(\\theta)\\) or \\(f(\\theta ; \\tau)\\) or \\(f(\\theta | \\tau)\\). Always remember that prior distributions require paramater values, even if we don’t explicitly write them. 45.2 Posterior Distribution The posterior distribution of \\(\\theta | \\boldsymbol{X}\\) is obtained through Bayes theorem: \\[\\begin{align*} f(\\theta | \\boldsymbol{x}) &amp; = \\frac{f(\\boldsymbol{x} | \\theta) f(\\theta)}{f(\\boldsymbol{x})} = \\frac{f(\\boldsymbol{x} | \\theta) f(\\theta)}{\\int f(\\boldsymbol{x} | \\theta^*) f(\\theta^*) d\\theta^*} \\\\ &amp; \\propto L(\\theta ; \\boldsymbol{x}) f(\\theta) \\end{align*}\\] 45.3 Posterior Expectation A very common point estimate of \\(\\theta\\) in Bayesian inference is the posterior expected value: \\[\\begin{align*} \\operatorname{E}[\\theta | \\boldsymbol{x}] &amp; = \\int \\theta f(\\theta | \\boldsymbol{x}) d\\theta \\\\ &amp; = \\frac{\\int \\theta L(\\theta ; \\boldsymbol{x}) f(\\theta) d\\theta}{\\int L(\\theta ; \\boldsymbol{x}) f(\\theta) d\\theta} \\end{align*}\\] 45.4 Posterior Interval The Bayesian analog of the frequentist confidence interval is the \\(1-\\alpha\\) posterior interval, where \\(C_{\\ell}\\) and \\(C_{u}\\) are determined so that: \\[ 1-\\alpha = \\Pr(C_\\ell \\leq \\theta \\leq C_u | \\boldsymbol{x}) \\] 45.5 Maximum A Posteriori Probability The maximum a posteriori probability (MAP) is the value (or values) of \\(\\theta\\) that maximize the posterior pdf or pmf: \\[\\begin{align*} \\hat{\\theta}_{\\text{MAP}} &amp; = \\operatorname{argmax}_\\theta \\Pr(\\theta | \\boldsymbol{x}) \\\\ &amp; = \\operatorname{argmax}_\\theta L(\\theta ; \\boldsymbol{x}) f(\\theta) \\end{align*}\\] This is a frequentist-esque use of the Bayesian framework. 45.6 Loss Functions Let \\(\\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right)\\) be a loss function for a given estimator \\(\\tilde{\\theta}\\). Examples are \\[ \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) = \\left(\\theta - \\tilde{\\theta}\\right)^2 \\mbox{ or } \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) = \\left|\\theta - \\tilde{\\theta}\\right|. \\] Note that, where the expected value is over \\(f(\\boldsymbol{x}; \\theta)\\): \\[\\begin{align*} \\operatorname{E}\\left[\\left(\\theta - \\tilde{\\theta}\\right)^2\\right] &amp; = \\left(\\operatorname{E}\\left[\\tilde{\\theta}\\right] - \\theta\\right)^2 + \\operatorname{Var}\\left(\\tilde{\\theta}\\right) \\\\ &amp; = \\mbox{bias}^2 + \\mbox{variance} \\end{align*}\\] 45.7 Bayes Risk The Bayes risk, \\(R\\left(\\theta, \\tilde{\\theta}\\right)\\), is the expected loss with respect to the posterior: \\[ {\\operatorname{E}}\\left[ \\left. \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) \\right| \\boldsymbol{x} \\right] = \\int \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) f(\\theta | \\boldsymbol{x}) d\\theta \\] 45.8 Bayes Estimators The Bayes estimator minimizes the Bayes risk. The posterior expectation \\({\\operatorname{E}}\\left[ \\left. \\theta \\right| \\boldsymbol{x} \\right]\\) minimizes the Bayes risk of \\(\\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) = \\left(\\theta - \\tilde{\\theta}\\right)^2\\). The median of \\(f(\\theta | \\boldsymbol{x})\\), calculated by \\(F^{-1}_{\\theta | \\boldsymbol{x}}(1/2)\\), minimizes the Bayes risk of \\(\\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) = \\left|\\theta - \\tilde{\\theta}\\right|\\). "],
["classification.html", "46 Classification 46.1 Assumptions 46.2 Prior Probability on H 46.3 Posterior Probability 46.4 Loss Function 46.5 Bayes Risk 46.6 Bayes Rule", " 46 Classification 46.1 Assumptions Let \\((X_1, X_2, \\ldots, X_n) | \\theta {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_\\theta\\) where \\(\\theta \\in \\Theta\\) and \\(\\theta \\sim F_{\\tau}\\). Let \\(\\Theta_0, \\Theta_1 \\subseteq \\Theta\\) so that \\(\\Theta_0 \\cap \\Theta_1 = \\varnothing\\) and \\(\\Theta_0 \\cup \\Theta_1 = \\Theta\\). Given observed data \\(\\boldsymbol{x}\\), we wish to classify whether \\(\\theta \\in \\Theta_0\\) or \\(\\theta \\in \\Theta_1\\). This is the Bayesian analog of hypothesis testing. 46.2 Prior Probability on H Let \\(H\\) be a rv such that \\(H=0\\) when \\(\\theta \\in \\Theta_0\\) and \\(H=1\\) when \\(\\theta \\in \\Theta_1\\). From the prior distribution on \\(\\theta\\), we can calculate \\[ \\Pr(H=0) = \\int_{\\theta \\in \\Theta_0} f(\\theta) d\\theta \\] and \\(\\Pr(H=1) = 1-\\Pr(H=0)\\). 46.3 Posterior Probability Using Bayes theorem, we can also calculate \\[\\begin{align*} \\Pr(H=0 | \\boldsymbol{x}) &amp; = \\frac{f(\\boldsymbol{x} | H=0) \\Pr(H=0)}{f(\\boldsymbol{x})} \\\\ &amp; = \\frac{\\int_{\\theta \\in \\Theta_0} f(\\boldsymbol{x} | \\theta) f(\\theta) d\\theta}{\\int_{\\theta \\in \\Theta} f(\\boldsymbol{x} | \\theta) f(\\theta) d\\theta} \\end{align*}\\] where note that \\(\\Pr(H=1 | \\boldsymbol{x}) = 1-\\Pr(H=0 | \\boldsymbol{x})\\). 46.4 Loss Function Let \\(\\mathcal{L}\\left(\\tilde{H}, H\\right)\\) be such that \\[\\begin{align*} \\mathcal{L}\\left(\\tilde{H}=1, H=0 \\right) &amp; = c_{I}\\\\ \\mathcal{L}\\left(\\tilde{H}=0, H=1 \\right) &amp; = c_{II} \\end{align*}\\] for some \\(c_{I}, c_{II} &gt; 0\\). 46.5 Bayes Risk The Bayes risk, \\(R\\left(\\tilde{H}, H\\right)\\), is \\[\\begin{align*} \\operatorname{E}\\left[ \\left. \\mathcal{L}\\left(\\theta, \\tilde{\\theta}\\right) \\right| \\boldsymbol{x} \\right] &amp; = c_{I} \\Pr(\\tilde{H}=1, H=0) + c_{II} \\Pr(\\tilde{H}=0, H=1) \\\\ &amp; = c_{I} \\Pr(\\tilde{H}=1 | H=0) \\Pr(H=0) \\\\ &amp; \\quad\\quad + c_{II} \\Pr(\\tilde{H}=0 | H=1) \\Pr(H=1) \\end{align*}\\] Notice how this balances what frequentists call Type I error and Type II error. 46.6 Bayes Rule The estimate \\(\\tilde{H}\\) that minimizes \\(R\\left(\\tilde{H}, H\\right)\\) is \\[\\tilde{H}=1 \\mbox{ when } \\Pr(H=1 | \\boldsymbol{x}) \\geq \\frac{c_{I}}{c_{I} + c_{II}}\\] and \\(\\tilde{H}=0\\) otherwise. "],
["priors.html", "47 Priors 47.1 Conjugate Priors 47.2 Example: Beta-Bernoulli 47.3 Example: Normal-Normal 47.4 Example: Dirichlet-Multinomial 47.5 Example: Gamma-Poisson 47.6 Jeffreys Prior 47.7 Examples: Jeffreys Priors 47.8 Improper Prior", " 47 Priors 47.1 Conjugate Priors A conjugate prior is a prior distribution for a data generating distribution so that the posterior distribution is of the same type as the prior. Conjugate priors are useful for obtaining stratightforward calculations of the posterior. There is a systematic method for calculating conjugate priors for exponential family distributions. 47.2 Example: Beta-Bernoulli Suppose \\(\\boldsymbol{X} | \\mu {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Bernoulli}(p)\\) and suppose that \\(p \\sim \\mbox{Beta}(\\alpha, \\beta)\\). \\[\\begin{align*} f(p | \\boldsymbol{x}) &amp; \\propto L(p ; \\boldsymbol{x}) f(p) \\\\ &amp; = p^{\\sum x_i} (1-p)^{\\sum (1-x_i)} p^{\\alpha - 1} (1-p)^{\\beta-1} \\\\ &amp; = p^{\\alpha - 1 + \\sum x_i} (1-p)^{\\beta - 1 + \\sum (1-x_i)} \\\\ &amp; \\propto \\mbox{Beta}(\\alpha + \\sum x_i, \\beta + \\sum (1-x_i)) \\end{align*}\\] Therefore, \\[ {\\operatorname{E}}[p | \\boldsymbol{x}] = \\frac{\\alpha + \\sum x_i}{\\alpha + \\beta + n}. \\] 47.3 Example: Normal-Normal Suppose \\(\\boldsymbol{X} | \\mu {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known, and suppose that \\(\\mu \\sim \\mbox{Normal}(a, b^2)\\). Then it can be shown that \\(\\mu | \\boldsymbol{x} \\sim \\mbox{Normal}({\\operatorname{E}}[\\mu | \\boldsymbol{x}], {\\operatorname{Var}}(\\mu | \\boldsymbol{x}))\\) where \\[ {\\operatorname{E}}[\\mu | \\boldsymbol{x}] = \\frac{b^2}{\\frac{\\sigma^2}{n} + b^2} \\overline{x} + \\frac{\\frac{\\sigma^2}{n}}{\\frac{\\sigma^2}{n} + b^2} a \\] \\[ {\\operatorname{Var}}(\\mu | \\boldsymbol{x}) = \\frac{b^2 \\frac{\\sigma^2}{n}}{\\frac{\\sigma^2}{n} + b^2} \\] 47.4 Example: Dirichlet-Multinomial This is a problem on Homework 3! 47.5 Example: Gamma-Poisson This is a problem on Homework 3! 47.6 Jeffreys Prior If we do inference based on prior \\(\\theta \\sim F_{\\tau}\\) to obtain \\(f(\\theta | \\boldsymbol{x}) \\propto L(\\theta; \\boldsymbol{x}) f(\\theta)\\), it follows that this inference may not be invariant to transformations of \\(\\theta\\), such as \\(\\eta = g(\\theta)\\). If we utilize a Jeffreys prior, which means it is such that \\[f(\\theta) \\propto \\sqrt{I(\\theta)}\\] then the prior will be invariant to transformations of \\(\\theta\\). We would want to show that \\(f(\\theta) \\propto \\sqrt{I(\\theta)}\\) implies \\(f(\\eta) \\propto \\sqrt{I(\\eta)}\\). 47.7 Examples: Jeffreys Priors Normal\\((\\mu, \\sigma^2)\\), \\(\\sigma^2\\) known: \\(f(\\mu) \\propto 1\\) Normal\\((\\mu, \\sigma^2)\\), \\(\\mu\\) known: \\(f(\\sigma) \\propto \\frac{1}{\\sigma}\\) Poisson\\((\\lambda)\\): \\(f(\\lambda) \\propto \\frac{1}{\\sqrt{\\lambda}}\\) Bernoulli\\((p)\\): \\(f(p) \\propto \\frac{1}{\\sqrt{p(1-p)}}\\) 47.8 Improper Prior An improper prior is a prior such that \\(\\int f(\\theta) d\\theta = \\infty\\). Nevertheless, sometimes it still may be the case that \\(f(\\theta | \\boldsymbol{x}) \\propto L(\\theta; \\boldsymbol{x}) f(\\theta)\\) yields a probability distribution. Take for example the case where \\(\\boldsymbol{X} | \\mu {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known, and suppose that \\(f(\\mu) \\propto 1\\). Then \\(\\int f(\\theta) d\\theta = \\infty\\), but \\[ f(\\theta | \\boldsymbol{x}) \\propto L(\\theta; \\boldsymbol{x}) f(\\theta) \\sim \\mbox{Normal}\\left(\\overline{x}, \\sigma^2/n\\right)\\] which is a proper probability distribution. "],
["theory.html", "48 Theory 48.1 de Finetti’s Theorem 48.2 Admissibility", " 48 Theory 48.1 de Finetti’s Theorem Let \\(X_1, X_2, \\ldots\\) be an infinite exchangeable sequence of Bernoulli rv’s. There exists a random variable \\(P \\in [0, 1]\\) such that: \\(X_1|P, X_2|P, \\ldots\\) are conditionally independent \\(X_1, X_2, \\ldots | P=p \\stackrel{{\\rm iid}}{\\sim} \\mbox{Bernoulli}(p)\\) This theorem is often used to justify the assumption of exchangeability, which is weaker than iid, with a prior distribution on the parameter(s). 48.2 Admissibility An estimator \\(\\tilde{\\theta}\\) is admissible with respect to risk function \\(R(\\cdot, \\theta)\\) if there is exists no other estimator \\(\\hat{\\theta}\\) such that \\(R(\\hat{\\theta}, \\theta) &lt; R(\\tilde{\\theta}, \\theta)\\) for all \\(\\theta \\in \\Theta\\). There’s a theoretical result that says all admissible estimators are Bayes estimates. "],
["empirical-bayes.html", "49 Empirical Bayes 49.1 Rationale 49.2 Approach 49.3 Example: Normal", " 49 Empirical Bayes 49.1 Rationale Under the scenario that \\(\\boldsymbol{X} | \\theta {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{\\theta}\\) with prior distribution \\(\\theta \\sim F_{\\tau}\\), we have to determine values for \\(\\tau\\). The empirical Bayes approach uses the observed data to estimate the prior parameter(s), \\(\\tau\\). This is especially useful for high-dimensional data when many parameters are simultaneously drawn from a prior with multiple observations drawn per parameter realization. 49.2 Approach The usual approach is to first integrate out the parameter to obtain \\[ f(\\boldsymbol{x} ; \\tau) = \\int f(\\boldsymbol{x} | \\theta) f(\\theta ; \\tau) d\\theta. \\] An estimation method (such as MLE) is then applied to estimate \\(\\tau\\). Then inference proceeds as usual under the assumption that \\(\\theta \\sim f(\\theta ; \\hat{\\tau})\\). 49.3 Example: Normal Suppose that \\(X_i | \\mu_i \\sim \\mbox{Normal}(\\mu_i, 1)\\) for \\(i=1, 2, \\ldots, n\\) where these rv’s are independent. Also suppose that \\(\\mu_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(a, b^2)\\). \\[ f(x_i ; a, b) = \\int f(x_i | \\mu_i) f(\\mu_i; a, b) d\\mu_i \\sim \\mbox{Normal}(a, 1+b^2). \\] \\[ \\implies \\hat{a} = \\overline{x}, \\ 1+\\hat{b}^2 = \\frac{\\sum_{k=1}^n (x_k - \\overline{x})^2}{n} \\] \\[\\begin{align*} \\operatorname{E}[\\mu_i | x_i] &amp; = \\frac{1}{1+b^2}a + \\frac{b^2}{1+b^2}x_i \\implies \\\\ &amp; \\\\ \\hat{\\operatorname{E}}[\\mu_i | x_i] &amp; = \\frac{1}{1+\\hat{b}^2}\\hat{a} + \\frac{\\hat{b}^2}{1+\\hat{b}^2}x_i \\\\ &amp; = \\frac{n}{\\sum_{k=1}^n (x_k - \\overline{x})^2} \\overline{x} + \\left(1-\\frac{n}{\\sum_{k=1}^n (x_k - \\overline{x})^2}\\right) x_i \\end{align*}\\] "],
["why-numerical-methods-for-likelihood.html", "50 Why Numerical Methods for Likelihood 50.1 Challenges 50.2 Approaches", " 50 Why Numerical Methods for Likelihood 50.1 Challenges Frequentist model: \\[X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}\\] Bayesian model: \\[X_1, X_2, \\ldots, X_n | {\\boldsymbol{\\theta}}{\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}} \\mbox{ and } {\\boldsymbol{\\theta}}\\sim F_{\\boldsymbol{\\tau}}\\] Sometimes it’s not possible to find formulas for \\(\\hat{{\\boldsymbol{\\theta}}}_{\\text{MLE}}\\), \\(\\hat{{\\boldsymbol{\\theta}}}_{\\text{MAP}}\\), \\({\\operatorname{E}}[{\\boldsymbol{\\theta}}| {\\boldsymbol{x}}]\\), or \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\). We have to use numerical methods instead. 50.2 Approaches We will discuss the following numerical approaches to likelihood based inference: Expectation-maximization (EM) algorithm Variational inference Markov chain Monte Carlo (MCMC) Metropolis sampling Metropolis-Hastings sampling Gibbs sampling "],
["latent-variable-models.html", "51 Latent Variable Models 51.1 Definition 51.2 Empirical Bayes Revisited 51.3 Normal Mixture Model 51.4 Bernoulli Mixture Model", " 51 Latent Variable Models 51.1 Definition Latent variables (or hidden variables) are random variables that are present in the model, but unobserved. We will denote latent variables by \\(Z\\), and we will assume \\[(X_1, Z_1), (X_2, Z_2), \\ldots, (X_n, Z_n) {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}.\\] A realized value of \\(Z\\) is \\(z\\), \\({\\boldsymbol{Z}}= (Z_1, Z_2, \\ldots, Z_n)^T\\), etc. The EM algorithm and variational inference involve latent variables. Bayesian models are a special case of latent variable models: the unobserved random parameters are latent variables. 51.2 Empirical Bayes Revisited In the earlier EB example, we supposed that \\(X_i | \\mu_i \\sim \\mbox{Normal}(\\mu_i, 1)\\) for \\(i=1, 2, \\ldots, n\\) where these rv’s are independent, and also that \\(\\mu_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(a, b^2)\\). The unobserved parameters \\(\\mu_1, \\mu_2, \\ldots, \\mu_n\\) are latent variables. In this case, \\({\\boldsymbol{\\theta}}= (a, b^2)\\). 51.3 Normal Mixture Model Suppose \\({X_1, X_2, \\ldots, X_n}{\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}\\) where \\({\\boldsymbol{\\theta}}= (\\pi_1, \\ldots, \\pi_K, \\mu_1, \\ldots, \\mu_K, \\sigma^2_1, \\ldots, \\sigma^2_K)\\) with pdf \\[ f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n \\sum_{k=1}^K \\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}} \\exp \\left\\{ -\\frac{(x_i - \\mu_k)^2}{2 \\sigma^2_k} \\right\\}. \\] The MLEs of the unknown paramaters cannot be found analytically. This is a mixture common model to work with in applications, so we need to be able to estimate the parameters. There is a latent variable model that produces the same maerginal distribution and likelihood function. Let \\({\\boldsymbol{Z}}_1, {\\boldsymbol{Z}}_2, \\ldots, {\\boldsymbol{Z}}_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Multinomial}_K(1, {\\boldsymbol{\\pi}})\\) where \\({\\boldsymbol{\\pi}}= (\\pi_1, \\ldots, \\pi_K)\\). Note that \\(Z_{ik} \\in \\{0, 1\\}\\) and \\(\\sum_{k=1}^K Z_{ik} = 1\\). Let \\([X_i | Z_{ik} = 1] \\sim \\mbox{Normal}(\\mu_k, \\sigma^2_k)\\), where \\(\\{X_i | {\\boldsymbol{Z}}_i\\}_{i=1}^{n}\\) are jointly independent. The joint pdf is \\[ f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n \\prod_{k=1}^K \\left[ \\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}} \\exp \\left\\{ -\\frac{(x_i - \\mu_k)^2}{2 \\sigma^2_k} \\right\\} \\right]^{z_{ik}}. \\] Note that \\[ f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n f(x_i, {\\boldsymbol{z}}_i; {\\boldsymbol{\\theta}}). \\] It can be verified that \\(f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\) is the marginal distribution of this latent variable model: \\[ f(x_i ; {\\boldsymbol{\\theta}}) = \\sum_{{\\boldsymbol{z}}_i} f(x_i, {\\boldsymbol{z}}_i; {\\boldsymbol{\\theta}}) = \\sum_{k=1}^K \\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}} \\exp \\left\\{ -\\frac{(x_i - \\mu_k)^2}{2 \\sigma^2_k} \\right\\}. \\] 51.4 Bernoulli Mixture Model Suppose \\({X_1, X_2, \\ldots, X_n}{\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}\\) where \\({\\boldsymbol{\\theta}}= (\\pi_1, \\ldots, \\pi_K, p_1, \\ldots, p_K)\\) with pdf \\[ f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n \\sum_{k=1}^K \\pi_k p_k^{x_i} (1-p_k)^{1-x_i}. \\] As in the Normal mixture model, the MLEs of the unknown paramaters cannot be found analytically. As before, there is a latent variable model that produces the same maerginal distribution and likelihood function. Let \\({\\boldsymbol{Z}}_1, {\\boldsymbol{Z}}_2, \\ldots, {\\boldsymbol{Z}}_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Multinomial}_K(1, {\\boldsymbol{\\pi}})\\) where \\({\\boldsymbol{\\pi}}= (\\pi_1, \\ldots, \\pi_K)\\). Note that \\(Z_{ik} \\in \\{0, 1\\}\\) and \\(\\sum_{k=1}^K Z_{ik} = 1\\). Let \\([X_i | Z_{ik} = 1] \\sim \\mbox{Bernoulli}(p_k)\\), where \\(\\{X_i | {\\boldsymbol{Z}}_i\\}_{i=1}^{n}\\) are jointly independent. The joint pdf is \\[ f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) = \\prod_{i=1}^n \\prod_{k=1}^K \\left[ p_k^{x_i} (1-p_k)^{1-x_i} \\right]^{z_{ik}}. \\] "],
["em-algorithm.html", "52 EM Algorithm 52.1 Rationale 52.2 Requirement 52.3 The Algorithm 52.4 \\(Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\) 52.5 EM for MAP", " 52 EM Algorithm 52.1 Rationale For any likelihood function, \\(L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) = f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\), there is an abundance of optimization methods that can be used to find the MLE or MAP. However: Optimization methods can be messy to implement There may be probabilistic structure that we can use to simplify the optimization process and also provide theoretical guarantees on its convergence Optimization isn’t necessarily the only goal, but one may also be interested in point estimates of the latent variable values 52.2 Requirement The expectation-maximization (EM) algorithm allows us to calculate MLEs and MAPs when certain geometric properties are satisfied in the probabilistic model. In order for the EM algorithm to be a practical approach, then we should have a latent variable model \\(f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}})\\) that is used to do inference on \\(f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\) or \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\). Note: Sometimes \\(({\\boldsymbol{x}}, {\\boldsymbol{z}})\\) is called the complete data and \\({\\boldsymbol{x}}\\) is called the observed data when we are using the EM as a method for dealing with missing data. 52.3 The Algorithm Choose initial value \\({\\boldsymbol{\\theta}}^{(0)}\\) Calculate \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}}, {\\boldsymbol{\\theta}}^{(t)})\\) Calculate \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right]\\] Set \\[{\\boldsymbol{\\theta}}^{(t+1)} = {\\text{argmax}}_{{\\boldsymbol{\\theta}}} Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\] Iterate until convergence and set \\(\\widehat{{\\boldsymbol{\\theta}}} = {\\boldsymbol{\\theta}}^{(\\infty)}\\) 52.4 \\(Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\) Continuous \\({\\boldsymbol{Z}}\\): \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = \\int \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}) d{\\boldsymbol{z}}\\] Discrete \\({\\boldsymbol{Z}}\\): \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = \\sum_{{\\boldsymbol{z}}} \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)})\\] 52.5 EM for MAP If we wish to calculate the MAP we replace \\(Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\) with \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right] + \\log f({\\boldsymbol{\\theta}})\\] where \\(f({\\boldsymbol{\\theta}})\\) is the prior distribution on \\({\\boldsymbol{\\theta}}\\). "],
["em-examples.html", "53 EM Examples 53.1 Normal Mixture Model 53.2 E-Step 53.3 M-Step 53.4 Caveat 53.5 Yeast Gene Expression 53.6 Initialize Values 53.7 Run EM Algorithm 53.8 Fitted Mixture Distribution 53.9 Bernoulli Mixture Model 53.10 Other Applications of EM", " 53 EM Examples 53.1 Normal Mixture Model Returning to the Normal mixture model introduced earlier, we first calculate \\[ \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) = \\sum_{i=1}^n \\sum_{k=1}^K z_{ik} \\log \\pi_k + z_{ik} \\log \\phi(x_i; \\mu_k, \\sigma^2_k) \\] where \\[ \\phi(x_i; \\mu_k, \\sigma^2_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}} \\exp \\left\\{ -\\frac{(x_i - \\mu_k)^2}{2 \\sigma^2_k} \\right\\}. \\] In caculating \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right]\\] we only need to know \\({\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}[Z_{ik} | {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}]\\), which turns out to be \\[ {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}[Z_{ik} | {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}] = \\frac{\\pi_k \\phi(x_i; \\mu_k, \\sigma^2_k)}{\\sum_{j=1}^K \\pi_j \\phi(x_i; \\mu_j, \\sigma^2_j)}. \\] Note that we take \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right]\\] so the parameter in \\(\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}})\\) is a free \\({\\boldsymbol{\\theta}}\\), but the paramaters used to take the conditional expectation of \\({\\boldsymbol{Z}}\\) are fixed at \\({\\boldsymbol{\\theta}}^{(t)}\\). Let’s define \\[ \\hat{z}_{ik}^{(t)} = {\\operatorname{E}}\\left[z_{ik} | {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}\\right] = \\frac{\\pi^{(t)}_k \\phi(x_i; \\mu^{(t)}_k, \\sigma^{2, (t)}_k)}{\\sum_{j=1}^K \\pi^{(t)}_j \\phi(x_i; \\mu^{(t)}_j, \\sigma^{2, (t)}_j)}. \\] 53.2 E-Step We calculate \\[Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) = {\\operatorname{E}}_{{\\boldsymbol{Z}}|{\\boldsymbol{X}}={\\boldsymbol{x}}}\\left[\\log f({\\boldsymbol{x}}, {\\boldsymbol{Z}}; {\\boldsymbol{\\theta}}); {\\boldsymbol{\\theta}}^{(t)}\\right]\\] \\[ = \\sum_{i=1}^n \\sum_{k=1}^K \\hat{z}_{ik}^{(t)} \\log \\pi_k + \\hat{z}_{ik}^{(t)} \\log \\phi(x_i; \\mu_k, \\sigma^2_k)\\] At this point the parameters making up \\(\\hat{z}_{ik}^{(t)}\\) are fixed at \\({\\boldsymbol{\\theta}}^{(t)}\\). 53.3 M-Step We now caculate \\({\\boldsymbol{\\theta}}^{(t+1)} = {\\text{argmax}}_{{\\boldsymbol{\\theta}}} Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}\\), which yields: \\[ \\pi_k^{(t+1)} = \\frac{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)}}{n} \\] \\[ \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)} x_i}{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)}} \\] \\[ \\sigma_k^{2, (t+1)} = \\frac{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)} \\left(x_i - \\mu_k^{(t+1)} \\right)^2}{\\sum_{i=1}^n \\hat{z}_{ik}^{(t)}} \\] Note: You need to use a Lagrange multiplier to obtain \\(\\{\\pi_k^{(t+1)}\\}_{k=1}^{K}\\). 53.4 Caveat If we assign one and only one data point to mixture component \\(k\\), meaning \\(\\mu_k^{(t)} = x_i\\) and \\(\\hat{z}_{ik}^{(t)}=1\\) for some \\(k\\) and \\(i\\), then as \\(\\sigma^{2, (t)}_k \\rightarrow 0\\), the likelihood goes to \\(\\infty\\). Therefore, when implementing the EM algorithm for this particular Normal mixture model, we have to be careful to bound all \\(\\sigma^{2, (t)}_k\\) away from zero and avoid this scenario. 53.5 Yeast Gene Expression Measured ratios of the nuclear to cytoplasmic fluorescence for a protein-GFP construct that is hypothesized as being nuclear in mitotic cells and largely cytoplasmic in mating cells. 53.6 Initialize Values &gt; set.seed(508) &gt; B &lt;- 100 &gt; p &lt;- rep(0,B) &gt; mu1 &lt;- rep(0,B) &gt; mu2 &lt;- rep(0,B) &gt; s1 &lt;- rep(0,B) &gt; s2 &lt;- rep(0,B) &gt; p[1] &lt;- runif(1, min=0.1, max=0.9) &gt; mu.start &lt;- sample(x, size=2, replace=FALSE) &gt; mu1[1] &lt;- min(mu.start) &gt; mu2[1] &lt;- max(mu.start) &gt; s1[1] &lt;- var(sort(x)[1:60]) &gt; s2[1] &lt;- var(sort(x)[61:120]) &gt; z &lt;- rep(0,120) 53.7 Run EM Algorithm &gt; for(i in 2:B) { + z &lt;- (p[i-1]*dnorm(x, mean=mu2[i-1], sd=sqrt(s2[i-1])))/ + (p[i-1]*dnorm(x, mean=mu2[i-1], sd=sqrt(s2[i-1])) + + (1-p[i-1])*dnorm(x, mean=mu1[i-1], sd=sqrt(s1[i-1]))) + mu1[i] &lt;- sum((1-z)*x)/sum(1-z) + mu2[i] &lt;- sum(z*x)/sum(z) + s1[i] &lt;- sum((1-z)*(x-mu1[i])^2)/sum(1-z) + s2[i] &lt;- sum(z*(x-mu2[i])^2)/sum(z) + p[i] &lt;- sum(z)/length(z) + } &gt; &gt; tail(cbind(mu1, s1, mu2, s2, p), n=3) mu1 s1 mu2 s2 p [98,] 2.455325 0.3637967 6.7952 6.058291 0.5340015 [99,] 2.455325 0.3637967 6.7952 6.058291 0.5340015 [100,] 2.455325 0.3637967 6.7952 6.058291 0.5340015 53.8 Fitted Mixture Distribution 53.9 Bernoulli Mixture Model As an exercise, derive the EM algorithm of the Bernoilli mixture model introduced earlier. Hint: Replace \\(\\phi(x_i; \\mu_k, \\sigma^2_k)\\) with the appropriate Bernoilli pmf. 53.10 Other Applications of EM Dealing with missing data Multiple imputation of missing data Truncated observations Bayesian hyperparameter estimation Hidden Markov models "],
["theory-of-em.html", "54 Theory of EM 54.1 Decomposition 54.2 Kullback-Leibler Divergence 54.3 Lower Bound 54.4 EM Increases Likelihood", " 54 Theory of EM 54.1 Decomposition Let \\(q({\\boldsymbol{z}})\\) be a probability distribution on the latent variables, \\({\\boldsymbol{z}}\\). Consider the following decomposition: \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) + {\\text{KL}}(q({\\boldsymbol{z}}) \\|f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})) \\] where \\[ \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) = \\int q({\\boldsymbol{z}}) \\log\\left(\\frac{f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}})}{q({\\boldsymbol{z}})}\\right) d{\\boldsymbol{z}}\\] \\[ {\\text{KL}}(q({\\boldsymbol{z}}) \\| f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})) = - \\int q({\\boldsymbol{z}}) \\log\\left(\\frac{f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})}{q({\\boldsymbol{z}})}\\right) d{\\boldsymbol{z}}\\] 54.2 Kullback-Leibler Divergence The KL divergence provides an asymmetric measure of the difference between two probability distributions. The KL divergence is such that \\({\\text{KL}}(q \\| f) \\geq 0\\) where \\({\\text{KL}}(q \\| f) = 0\\) if and only if \\(q=f\\). This property is known as Gibbs inequality. 54.3 Lower Bound Note that \\(\\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}})\\) provides a lower bound on the likelihood function: \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) \\geq \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) \\] If we set \\(q({\\boldsymbol{z}}) = f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)})\\), then for a fixed \\({\\boldsymbol{\\theta}}^{(t)}\\) and as a function of \\({\\boldsymbol{\\theta}}\\), \\[ \\begin{aligned} \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) &amp; \\propto \\int f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}) \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) d{\\boldsymbol{z}}\\\\ &amp; = Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)}) \\end{aligned} \\] 54.4 EM Increases Likelihood Since \\({\\boldsymbol{\\theta}}^{(t+1)} = {\\text{argmax}}_{{\\boldsymbol{\\theta}}} Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\), it follows that \\[Q({\\boldsymbol{\\theta}}^{(t+1)}, {\\boldsymbol{\\theta}}^{(t)}) \\geq Q({\\boldsymbol{\\theta}}^{(t)}, {\\boldsymbol{\\theta}}^{(t)}).\\] Also, by the properties of KL divergence stated above, we have \\[ {\\text{KL}}(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t+1)}) \\| f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)})) \\geq {\\text{KL}}(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}) \\| f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)})). \\] Putting these together we have \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t+1)}) \\geq \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}^{(t)}). \\] "],
["variational-inference.html", "55 Variational Inference 55.1 Rationale 55.2 Optimization Goal 55.3 Mean Field Approximation 55.4 Optimal \\(q_k({\\boldsymbol{z}}_k)\\) 55.5 Remarks", " 55 Variational Inference 55.1 Rationale Performing the EM algorithm required us to be able to compute \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\) and also optimize \\(Q({\\boldsymbol{\\theta}}, {\\boldsymbol{\\theta}}^{(t)})\\). Sometimes this is not possible. Variational inference takes advantage of the decomposition \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) + {\\text{KL}}(q({\\boldsymbol{z}}) \\|f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})) \\] and instead considers other forms of \\(q({\\boldsymbol{z}})\\) to identify a more tractable optimization. 55.2 Optimization Goal Since \\[ \\log f({\\boldsymbol{x}}; {\\boldsymbol{\\theta}}) = \\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}}) + {\\text{KL}}(q({\\boldsymbol{z}}) \\|f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})) \\] it follows that the closer \\(q({\\boldsymbol{z}})\\) is to \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\), the term \\(\\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}})\\) grows larger while \\({\\text{KL}}(q({\\boldsymbol{z}}) \\|f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}}))\\) becomes smaller. The goal is typically to identify a restricted form of \\(q({\\boldsymbol{z}})\\) that maximizes \\(\\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}})\\), which serves as an approximation to the posterior distribution \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}}; {\\boldsymbol{\\theta}})\\). 55.3 Mean Field Approximation A mean field approximation implies we restrict \\(q({\\boldsymbol{z}})\\) to be \\[ q({\\boldsymbol{z}}) = \\prod_{k=1}^K q_k({\\boldsymbol{z}}_k) \\] for some partition \\({\\boldsymbol{z}}= ({\\boldsymbol{z}}_1, {\\boldsymbol{z}}_2, \\ldots, {\\boldsymbol{z}}_K)\\). This partition is very context specific and is usually driven by the original model and what is tractable. 55.4 Optimal \\(q_k({\\boldsymbol{z}}_k)\\) Under the above restriction, it can be shown that the \\(\\{q_k({\\boldsymbol{z}}_k)\\}\\) that maximize \\(\\mathcal{L}(q({\\boldsymbol{z}}), {\\boldsymbol{\\theta}})\\) have the form: \\[ q_k({\\boldsymbol{z}}_k) \\propto \\exp \\left\\{ \\int \\log f({\\boldsymbol{x}}, {\\boldsymbol{z}}; {\\boldsymbol{\\theta}}) \\prod_{j \\not= k} q_j({\\boldsymbol{z}}_j)d{\\boldsymbol{z}}_j \\right\\}. \\] These pdf’s or pmf’s can be calculated iteratively by cycling over \\(k=1, 2, \\ldots, K\\) after intializing them appropriately. Note that convergence is guaranteed. 55.5 Remarks If \\({\\boldsymbol{\\theta}}\\) is also random, then it can be included in \\({\\boldsymbol{z}}\\). The estimated \\(\\hat{f}({\\boldsymbol{z}}| {\\boldsymbol{x}})\\) is typically concentrated around the high density region of the true \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}})\\), so it is useful for calculations such as the MAP, but it is not guaranteed to be a good overall estimate of \\(f({\\boldsymbol{z}}| {\\boldsymbol{x}})\\). Variational inference is typically faster than MCMC (covered next). Given this is an optimization procedure, care can be taken to speed up convergence and avoid unintended local maxima. "],
["markov-chain-monte-carlo.html", "56 Markov Chain Monte Carlo 56.1 Motivation 56.2 Note 56.3 Big Picture 56.4 Metropolis-Hastings Algorithm 56.5 Metropolis Algorithm 56.6 Utilizing MCMC Output 56.7 Remarks 56.8 Full Conditionals 56.9 Gibbs Sampling 56.10 Gibbs and MH 56.11 Latent Variables 56.12 Theory 56.13 Software", " 56 Markov Chain Monte Carlo 56.1 Motivation When performing Bayesian inferece, it is often (but not always) possible to calculate \\[f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}}) \\propto L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}})\\] but it is typically much more difficult to calculate \\[f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}}) = \\frac{L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}})}{f({\\boldsymbol{x}})}.\\] Markov chain Monte Carlo is a method for simulating data approximately from \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\) with knowledge of only \\(L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}})\\). 56.2 Note MCMC can be used to approximately simulate data from any distribution that is only proportionally characterized, but it is probably most well know for doing so in the context of Bayesian infererence. We will explain MCMC in the context of Bayesian inference. 56.3 Big Picture We draw a Markov chain of \\({\\boldsymbol{\\theta}}\\) values so that, in some asymptotic sense, these are equivalent to iid draws from \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\). The draws are done competitively so that the next draw of a realization of \\({\\boldsymbol{\\theta}}\\) depends on the current value. The Markov chain is set up so that it only depends on \\(L({\\boldsymbol{\\theta}}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}})\\). A lot of practical decisions need to be made by the user, so utilize MCMC carefully. 56.4 Metropolis-Hastings Algorithm Initialize \\({\\boldsymbol{\\theta}}^{(0)}\\) Generate \\({\\boldsymbol{\\theta}}^{*} \\sim q({\\boldsymbol{\\theta}}| {\\boldsymbol{\\theta}}^{(b)})\\) for some pdf or pmf \\(q(\\cdot | \\cdot)\\) With probablity \\[A({\\boldsymbol{\\theta}}^{*}, {\\boldsymbol{\\theta}}^{(b)}) = \\min\\left( 1, \\frac{L({\\boldsymbol{\\theta}}^{*}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}}^{*}) q({\\boldsymbol{\\theta}}^{(b)} | {\\boldsymbol{\\theta}}^{*})}{L({\\boldsymbol{\\theta}}^{(b)}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}}^{(b)}) q({\\boldsymbol{\\theta}}^{*} | {\\boldsymbol{\\theta}}^{(b)})} \\right)\\] set \\({\\boldsymbol{\\theta}}^{(b+1)} = {\\boldsymbol{\\theta}}^{*}\\). Otherise, set \\({\\boldsymbol{\\theta}}^{(b+1)} = {\\boldsymbol{\\theta}}^{(b)}\\) Continue for \\(b = 1, 2, \\ldots, B\\) iterations and carefully select which \\({\\boldsymbol{\\theta}}^{(b)}\\) are utilized to approximate iid observations from \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\) 56.5 Metropolis Algorithm The Metropolis algorithm restricts \\(q(\\cdot, \\cdot)\\) to be symmetric so that \\(q({\\boldsymbol{\\theta}}^{(b)} | {\\boldsymbol{\\theta}}^{*}) = q({\\boldsymbol{\\theta}}^{*} | {\\boldsymbol{\\theta}}^{(b)})\\) and \\[ A({\\boldsymbol{\\theta}}^{*}, {\\boldsymbol{\\theta}}^{(b)}) = \\min\\left( 1, \\frac{L({\\boldsymbol{\\theta}}^{*}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}}^{*})}{L({\\boldsymbol{\\theta}}^{(b)}; {\\boldsymbol{x}}) f({\\boldsymbol{\\theta}}^{(b)})} \\right). \\] 56.6 Utilizing MCMC Output Two common uses of the output from MCMC are as follows: \\({\\operatorname{E}}[f({\\boldsymbol{\\theta}}) | {\\boldsymbol{x}}]\\) is approximated by \\[ \\hat{{\\operatorname{E}}}[f({\\boldsymbol{\\theta}}) | {\\boldsymbol{x}}] = \\frac{1}{B} \\sum_{b=1}^B f\\left({\\boldsymbol{\\theta}}^{(b)}\\right). \\] Some subsequence \\({\\boldsymbol{\\theta}}^{(b_1)}, {\\boldsymbol{\\theta}}^{(b_2)}, \\ldots, {\\boldsymbol{\\theta}}^{(b_m)}\\) from \\(\\left\\{{\\boldsymbol{\\theta}}^{(b)}\\right\\}_{b=1}^{B}\\) is utilized as an empirical approximation to iid draws from \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\). 56.7 Remarks The random draw \\({\\boldsymbol{\\theta}}^{*} \\sim q({\\boldsymbol{\\theta}}| {\\boldsymbol{\\theta}}^{(b)})\\) perturbs the current value \\({\\boldsymbol{\\theta}}^{(b)}\\) to the next value \\({\\boldsymbol{\\theta}}^{(b+1)}\\). It is often a Normal distribution for continuous \\({\\boldsymbol{\\theta}}\\). Choosing the variance of \\(q({\\boldsymbol{\\theta}}| {\\boldsymbol{\\theta}}^{(b)})\\) is important as it requires enough variance for the theory to be applicable within a reasonable number of computations, but it cannot be so large that new values of \\({\\boldsymbol{\\theta}}^{(b+1)}\\) are rarely generated. \\(A({\\boldsymbol{\\theta}}^{*}, {\\boldsymbol{\\theta}}^{(b)})\\) is called the acceptance probability. The algorithm must be run for a certain number of iterations (“burn in”) before observed \\({\\boldsymbol{\\theta}}^{(b)}\\) can be utilized. The generated \\({\\boldsymbol{\\theta}}^{(b)}\\) are typically “thinned” (only sampled every so often) to reduce Markov dependence. 56.8 Full Conditionals Suppose that \\({\\boldsymbol{\\theta}}= (\\theta_1, \\theta_2, \\ldots, \\theta_K)\\). Define the subset vector as \\({\\boldsymbol{\\theta}}_{a:b} = (\\theta_a, \\theta_{a+1}, \\ldots, \\theta_{b-1}, \\theta_b)\\) for any \\(1 \\leq a \\leq b \\leq K\\). The full conditional of \\(\\theta_k\\) is \\[ \\Pr(\\theta_k | {\\boldsymbol{\\theta}}_{1:k-1}, {\\boldsymbol{\\theta}}_{k+1:K}, {\\boldsymbol{x}}) \\] 56.9 Gibbs Sampling Gibbs sampling a special type of Metropolis-Hasting MCMC. The algorithm samples one coordinate of \\({\\boldsymbol{\\theta}}\\) at a time. Initialize \\({\\boldsymbol{\\theta}}^{(0)}\\). Sample: \\(\\theta_1^{(b+1)} \\sim \\Pr(\\theta_1 | {\\boldsymbol{\\theta}}_{2:K}^{(b)}, {\\boldsymbol{x}})\\) \\(\\theta_2^{(b+1)} \\sim \\Pr(\\theta_2 | \\theta_{1}^{(b+1)}, {\\boldsymbol{\\theta}}_{3:K}^{(b)}, {\\boldsymbol{x}})\\) \\(\\theta_3^{(b+1)} \\sim \\Pr(\\theta_3 | {\\boldsymbol{\\theta}}_{1:2}^{(b+1)}, {\\boldsymbol{\\theta}}_{3:K}^{(b)}, {\\boldsymbol{x}})\\) \\(\\vdots\\) \\(\\theta_K^{(b+1)} \\sim \\Pr(\\theta_K | {\\boldsymbol{\\theta}}_{1:K-1}^{(b+1)}, {\\boldsymbol{x}})\\) Continue for \\(b = 1, 2, \\ldots, B\\) iterations. 56.10 Gibbs and MH As an exercise, show that Gibbs sampling is a special case of the Metropolis-Hastings algorithm where \\(A({\\boldsymbol{\\theta}}^{*}, {\\boldsymbol{\\theta}}^{(b)}) = 1\\). 56.11 Latent Variables Note that MCMC is often used to calculate a posterior distribution on latent variables. This makes sense because unobserved random paramaters are a special type of latent variable. 56.12 Theory The goal of MCMC is to construct a Markov chain that converges to a stationary distribution that is equivalent to the target probability distribution. Under reasonably general assumptions, one can show that the Metropolis-Hastings algorithm produces a Markov chain that is homogeneous and achieves detailed balance, which implies the Markov chain is ergodic so that \\({\\boldsymbol{\\theta}}^{(B)}\\) converges in distribution to \\(f({\\boldsymbol{\\theta}}| {\\boldsymbol{x}})\\) as \\(B \\rightarrow \\infty\\) and that \\[ \\hat{{\\operatorname{E}}}[f({\\boldsymbol{\\theta}}) | {\\boldsymbol{x}}] = \\frac{1}{B} \\sum_{b=1}^B f\\left({\\boldsymbol{\\theta}}^{(b)}\\right) \\stackrel{B \\rightarrow \\infty}{\\longrightarrow} {\\operatorname{E}}[f({\\boldsymbol{\\theta}}) | {\\boldsymbol{x}}]. \\] 56.13 Software Stan is probably the currently most popular software for doing Bayesian computation, including MCMC and variational inference. There are also popular R packages, such as MCMCpack. "],
["mcmc-example.html", "57 MCMC Example 57.1 Single Nucleotide Polymorphisms 57.2 PSD Admixture Model 57.3 Gibbs Sampling Approach 57.4 The Data 57.5 Model Components 57.6 The Model 57.7 Conditional Independence 57.8 The Posterior 57.9 Full Conditional for \\(\\boldsymbol{Q}\\) 57.10 Full Conditional for \\(\\boldsymbol{P}\\) 57.11 Full Conditional \\(\\boldsymbol{Z}_A\\) &amp; \\(\\boldsymbol{Z}_B\\) 57.12 Gibbs Sampling Updates 57.13 Implementation 57.14 Matrix-wise rdirichlet Function 57.15 Inspect Data 57.16 Model Parameters 57.17 Update \\(\\boldsymbol{P}\\) 57.18 Update \\(\\boldsymbol{Q}\\) 57.19 Update (Each) \\(\\boldsymbol{Z}\\) 57.20 Model Log-likelihood Function 57.21 MCMC Configuration 57.22 Run Sampler 57.23 Posterior Mean of \\(\\boldsymbol{Q}\\) 57.24 Plot Log-likelihood Steps 57.25 What Happens for K=4? 57.26 Run Sampler Again 57.27 Posterior Mean of \\(\\boldsymbol{Q}\\)", " 57 MCMC Example 57.1 Single Nucleotide Polymorphisms SNPs 57.2 PSD Admixture Model PSD PSD model proposed in Pritchard, Stephens, Donnelly (2000) Genetics. 57.3 Gibbs Sampling Approach The Bayesian Gibbs sampling approach to inferring the PSD model touches on many important ideas, such as conjugate priors and mixture models. We will focus on a version of this model for diploid SNPs. 57.4 The Data \\(\\boldsymbol{X}\\), a \\(L \\times N\\) matrix consisting of the genotypes, coded as \\(0,1,2\\). Each row is a SNP, each column is an individual. In order for this model to work, the data needs to be broken down into “phased” genotypes. For the 0 and 2 cases, it’s obvious how to do this, and for the 1 case, it’ll suffice for this model to randomly assign the alleles to chromosomes. We will explore phasing more on HW4. Thus, we wind up with two {0, 1} binary matrices \\(\\boldsymbol{X}_A\\) and \\(\\boldsymbol{X}_B\\), both \\(L \\times N\\). We will refer to allele \\(A\\) and allele \\(B\\). Note \\({\\boldsymbol{X}}= {\\boldsymbol{X}}_A + {\\boldsymbol{X}}_B\\). 57.5 Model Components \\(K\\), the number of populations that we model the genotypes as admixtures of. This is chosen before inference. \\(\\boldsymbol{Q}\\), a \\(N \\times K\\) matrix, the admixture proportions, values are in the interval \\([0, 1]\\) and rows are constrained to sum to 1. \\(\\boldsymbol{P}\\), a \\(L \\times K\\) matrix, the allele frequencies for each population, values are in the interval \\([0, 1]\\). \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\), two \\(L \\times N\\) matrices that tell us which population the respective allele is from. Elements consist of the integers between \\(1\\) and \\(K\\). This is a hidden variable. 57.6 The Model Each allele (elements of \\(\\boldsymbol{X}_A\\) and \\(\\boldsymbol{X}_B\\)) is a Bernoulli random variable, with success probability determined by which population that allele is assigned to (i.e., depends on \\(\\boldsymbol{Z}_A\\), \\(\\boldsymbol{Z}_B\\), and \\(\\boldsymbol{P}\\)). We put a uniform Beta prior, i.e., \\(\\operatorname{Beta}(1, 1)\\), on each element of \\(\\boldsymbol{P}\\). We put a uniform Dirichlet prior, i.e., \\(\\operatorname{Dirichlet(1,\\ldots,1)}\\), on each row of \\(\\boldsymbol{Q}\\). \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) are \\(K\\)-class Multinomial draws where the probability of drawing each class is determined by each row of \\(Q\\). 57.7 Conditional Independence The key observation is to understand which parts of the model are dependent on each other in the data generating process. The data \\(\\boldsymbol{X}_A\\) and \\(\\boldsymbol{X}_B\\) depends directly on \\(\\boldsymbol{Z}_A\\), \\(\\boldsymbol{Z}_B\\), and \\(\\boldsymbol{P}\\) (not \\(\\boldsymbol{Q}\\)!). The latent variable \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) depend only on \\(\\boldsymbol{Q}\\) and they’re conditionally independent given \\(\\boldsymbol{Q}\\). \\(\\boldsymbol{Q}\\) and \\(\\boldsymbol{P}\\) depend only on their priors. \\(\\Pr(\\boldsymbol{X}_A, \\boldsymbol{X}_B, \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{P}, \\boldsymbol{Q}) =\\) \\(\\Pr(\\boldsymbol{X}_A, \\boldsymbol{X}_B | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{P}) \\Pr(\\boldsymbol{Z}_A | \\boldsymbol{Q}) \\Pr(\\boldsymbol{Z}_B | \\boldsymbol{Q}) \\Pr(\\boldsymbol{P}) \\Pr(\\boldsymbol{Q})\\) 57.8 The Posterior We desire to compute the posterior distribution \\(\\Pr(\\boldsymbol{P}, \\boldsymbol{Q}, \\boldsymbol{Z}_A, \\boldsymbol{Z}_B | \\boldsymbol{X}_A, \\boldsymbol{X}_B)\\). Gibbs sampling tells us if we can construct conditional distributions for each random variable in our model, then iteratively sampling and updating our model parameters will result in a stationary distribution that is the same as the posterior distribution. Gibbs sampling is an extremely powerful approach for this model because we can utilize conjugate priors as well as the independence of various parameters in the model to compute these conditional distributions. 57.9 Full Conditional for \\(\\boldsymbol{Q}\\) Note that \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) are the only parts of this model that directly depend on \\(\\boldsymbol{Q}\\). \\[\\begin{align*} &amp;\\Pr(Q_n | \\boldsymbol{Q}_{-n}, \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{P}, \\boldsymbol{X}_A, \\boldsymbol{X}_B)\\\\ =&amp; \\Pr(Q_n | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B) \\\\ \\propto&amp; \\Pr(Z_{An}, Z_{Bn} | Q_n) \\Pr(Q_n)\\\\ =&amp; \\Pr(Z_{An} | Q_n) \\Pr(Z_{Bn} | Q_n) \\Pr(Q_n)\\\\ \\propto&amp; \\left( \\prod_{\\ell=1}^L \\prod_{k=1}^K Q_{nk}^{\\mathbb{1}(Z_{An\\ell}=k)+\\mathbb{1}(Z_{Bn\\ell}=k)} \\right) \\end{align*}\\] \\[=\\prod_{k=1}^K Q_{nk}^{S_{nk}}\\] where \\(S_{nk}\\) is simply the count of the number of alleles for individual \\(n\\) that got assigned to population \\(k\\). Thus, \\(Q_n | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B \\sim \\operatorname{Dirichlet}(S_{j1}+1, \\ldots, S_{jk}+1)\\),. We could have guessed that this distribution is Dirichlet given that \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) are multinomial! Let’s use conjugacy to help us in the future. 57.10 Full Conditional for \\(\\boldsymbol{P}\\) \\[\\begin{align*} &amp;\\Pr(P_\\ell | \\boldsymbol{P}_{-\\ell}, \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{Q}, \\boldsymbol{X}_A, \\boldsymbol{X}_B) \\\\ \\propto&amp; \\Pr(\\boldsymbol{X}_{A\\ell}, \\boldsymbol{X}_{B\\ell} | P_\\ell, \\boldsymbol{Z}_{A\\ell}, \\boldsymbol{Z}_{B\\ell}) \\Pr(P_\\ell) \\end{align*}\\] We know \\(\\Pr(\\boldsymbol{X}_{A\\ell}, \\boldsymbol{X}_{B\\ell} | P_\\ell, \\boldsymbol{Z}_{A\\ell}, \\boldsymbol{Z}_{B\\ell})\\) will be Bernoulli and \\(\\Pr(P_\\ell)\\) will be beta, so the full conditional will be beta as well. In fact, the prior is uniform so it vanishes from the RHS. Thus, all we have to worry about is the Bernoulli portion \\(\\Pr(\\boldsymbol{X}_{A\\ell}, \\boldsymbol{X}_{B\\ell} | P_\\ell, \\boldsymbol{Z}_{A\\ell}, \\boldsymbol{Z}_{B\\ell})\\). Here, we observe that if the \\(\\boldsymbol{Z}_A\\) and \\(\\boldsymbol{Z}_B\\) are “known”, then we known which value of \\(P_\\ell\\) to plug into our Bernoulli for \\(\\boldsymbol{X}_A\\) and \\(\\boldsymbol{X}_B\\). Following the Week 6 lectures, we find that the full conditional for \\(\\boldsymbol{P}\\) is: \\[P_{\\ell k} | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{X}_A, \\boldsymbol{X}_B \\sim \\operatorname{Beta}(1+T_{\\ell k 0}, 1+T_{\\ell k 1})\\] where \\(T_{\\ell k 0}\\) is the total number of 0 alleles at SNP \\(\\ell\\) for population \\(k\\), and \\(T_{\\ell k 1}\\) is the analogous quantity for the 1 allele. 57.11 Full Conditional \\(\\boldsymbol{Z}_A\\) &amp; \\(\\boldsymbol{Z}_B\\) We’ll save some math by first noting that alleles \\(A\\) and \\(B\\) are independent of each other, so we can write this for only \\(\\boldsymbol{Z}_A\\) without losing any information. Also, all elements of \\(\\boldsymbol{Z}_A\\) are independent of each other. Further, note that each element of \\(\\boldsymbol{Z}_A\\) is a single multinomial draw, so we are working with a discrete random variable. \\[\\begin{align*} &amp;\\Pr(Z_{A\\ell n}=k | \\boldsymbol{X}_A, \\boldsymbol{Q}, \\boldsymbol{P}) \\\\ =&amp; \\Pr (Z_{A\\ell n}=k | X_{A \\ell n}, Q_n, P_\\ell) \\\\ \\propto &amp; \\Pr(X_{A \\ell n} | Z_{A\\ell n}=k, Q_n, P_\\ell) \\Pr(Z_{A\\ell n}=k | Q_n, P_\\ell) \\end{align*}\\] We can look at the two factors. First: \\[\\Pr(Z_{A\\ell n}=k | Q_n, P_\\ell) = \\Pr(Z_{A\\ell n}=k | Q_n) = Q_{nk}\\] Then: \\[\\Pr(X_{A \\ell n} | Z_{A\\ell n}=k, Q_n, P_\\ell) = P_{\\ell k}\\] Thus, we arrive at the formula: \\[\\Pr(Z_{A\\ell n}=k | \\boldsymbol{X}_A, \\boldsymbol{Q}, \\boldsymbol{P}) \\propto P_{\\ell k} Q_{n k}\\] 57.12 Gibbs Sampling Updates It’s neat that we wind up just iteratively counting the various discrete random variables along different dimensions. \\[\\begin{align*} Q_n | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B &amp;\\sim \\operatorname{Dirichlet}(S_{j1}+1, \\ldots, S_{jk}+1)\\\\ P_{\\ell k} | \\boldsymbol{Z}_A, \\boldsymbol{Z}_B, \\boldsymbol{X}_A, \\boldsymbol{X}_B &amp;\\sim \\operatorname{Beta}(1+T_{\\ell k 0}, 1+T_{\\ell k 1}) \\\\ Z_{A\\ell n} | \\boldsymbol{X}_A, \\boldsymbol{Q}, \\boldsymbol{P} &amp;\\sim \\operatorname{Multinomial}\\left(\\frac{P_\\ell * Q_n}{P_\\ell \\cdot Q_n}\\right) \\end{align*}\\] where \\(*\\) means element-wise vector multiplication. 57.13 Implementation The Markov chain property means that we can’t use vectorization forward in time, so R is not the best way to implement this algorithm. That being said, we can vectorize the pieces that we can and demonstrate what happens. 57.14 Matrix-wise rdirichlet Function Drawing from a Dirichlet is easy and vectorizable because it consists of normalizing independent gamma draws. &gt; rdirichlet &lt;- function(alpha) { + m &lt;- nrow(alpha) + n &lt;- ncol(alpha) + x &lt;- matrix(rgamma(m * n, alpha), ncol = n) + x/rowSums(x) + } 57.15 Inspect Data &gt; dim(Xa) [1] 400 24 &gt; X[1:3,1:3] NA18516 NA19138 NA19137 rs2051075 0 1 2 rs765546 2 2 0 rs10019399 2 2 2 &gt; Xa[1:3,1:3] NA18516 NA19138 NA19137 rs2051075 0 0 1 rs765546 1 1 0 rs10019399 1 1 1 57.16 Model Parameters &gt; L &lt;- nrow(Xa) &gt; N &lt;- ncol(Xa) &gt; &gt; K &lt;- 3 &gt; &gt; Za &lt;- matrix(sample(1:K, L*N, replace=TRUE), L, N) &gt; Zb &lt;- matrix(sample(1:K, L*N, replace=TRUE), L, N) &gt; P &lt;- matrix(0, L, K) &gt; Q &lt;- matrix(0, N, K) 57.17 Update \\(\\boldsymbol{P}\\) &gt; update_P &lt;- function() { + Na_0 &lt;- Za * (Xa==0) + Na_1 &lt;- Za * (Xa==1) + Nb_0 &lt;- Zb * (Xb==0) + Nb_1 &lt;- Zb * (Xb==1) + for(k in 1:K) { + N0 &lt;- rowSums(Na_0==k)+rowSums(Nb_0==k) + N1 &lt;- rowSums(Na_1==k)+rowSums(Nb_1==k) + P[,k] &lt;- rdirichlet(1+cbind(N1, N0))[,1] + } + P + } 57.18 Update \\(\\boldsymbol{Q}\\) &gt; update_Q &lt;- function() { + M_POP0 &lt;- apply(Za, 2, function(x) {tabulate(x, nbins=K)} ) + M_POP1 &lt;- apply(Zb, 2, function(x) {tabulate(x, nbins=K)} ) + + rdirichlet(t(1+M_POP0+M_POP1)) + } 57.19 Update (Each) \\(\\boldsymbol{Z}\\) &gt; update_Z &lt;- function(X) { + Z &lt;- matrix(0, nrow(X), ncol(X)) + for(n in 1:N) { + PZ0 &lt;- t(t((1-P)) * Q[n,]) + PZ1 &lt;- t(t(P) * Q[n,]) + PZ &lt;- X[,n]*PZ1 + (1-X[,n])*PZ0 + Z[,n] &lt;- apply(PZ, 1, function(p){sample(1:K, 1, prob=p)}) + } + Z + } 57.20 Model Log-likelihood Function &gt; model_ll &lt;- function() { + AFa &lt;- t(sapply(1:L, function(i){P[i,][Za[i,]]})) + AFb &lt;- t(sapply(1:L, function(i){P[i,][Zb[i,]]})) + # hint, hint, HW3 + sum(dbinom(Xa, 1, AFa, log=TRUE)) + + sum(dbinom(Xb, 1, AFb, log=TRUE)) + } 57.21 MCMC Configuration &gt; MAX_IT &lt;- 20000 &gt; BURNIN &lt;- 5000 &gt; THIN &lt;- 20 &gt; &gt; QSUM &lt;- matrix(0, N, K) &gt; &gt; START &lt;- 200 &gt; TAIL &lt;- 500 &gt; LL_start &lt;- rep(0, START) &gt; LL_end &lt;- rep(0, TAIL) 57.22 Run Sampler &gt; set.seed(1234) &gt; &gt; for(it in 1:MAX_IT) { + P &lt;- update_P() + Q &lt;- update_Q() + Za &lt;- update_Z(Xa) + Zb &lt;- update_Z(Xb) + + if(it &gt; BURNIN &amp;&amp; it %% THIN == 0) {QSUM &lt;- QSUM+Q} + if(it &lt;= START) {LL_start[it] &lt;- model_ll()} + if(it &gt; MAX_IT-TAIL) {LL_end[it-(MAX_IT-TAIL)] &lt;- model_ll()} + } &gt; &gt; Q_MEAN &lt;- QSUM/((MAX_IT-BURNIN)/THIN) 57.23 Posterior Mean of \\(\\boldsymbol{Q}\\) 57.24 Plot Log-likelihood Steps Note both the needed burn-in and thinning. 57.25 What Happens for K=4? &gt; K &lt;- 4 &gt; Za &lt;- matrix(sample(1:K, L*N, replace=TRUE), L, N) &gt; Zb &lt;- matrix(sample(1:K, L*N, replace=TRUE), L, N) &gt; P &lt;- matrix(0, L, K) &gt; Q &lt;- matrix(0, N, K) &gt; QSUM &lt;- matrix(0, N, K) 57.26 Run Sampler Again &gt; for(it in 1:MAX_IT) { + P &lt;- update_P() + Q &lt;- update_Q() + Za &lt;- update_Z(Xa) + Zb &lt;- update_Z(Xb) + + if(it &gt; BURNIN &amp;&amp; it %% THIN == 0) { + QSUM &lt;- QSUM+Q + } + } &gt; &gt; Q_MEAN &lt;- QSUM/((MAX_IT-BURNIN)/THIN) 57.27 Posterior Mean of \\(\\boldsymbol{Q}\\) "],
["further-reading-1.html", "58 Further Reading", " 58 Further Reading Bishop (2016) One of the clearest treatments of the EM algorithm, variational inference, and MCMC can be found in Chapters 9-11 of Pattern Recognition and Machine Learning, by Christopher Bishop. This is a great book in general. EM Algorithm Paper that popularized the method: Dempster, Laird, Rubin (1977) Paper that got the theory correct: Wu (1983) Variational Inference Wainwright and Jordan (2008) Ormerod and Wand (2010) Blei et al. (2016) MCMC MCMC Without All the BS Bayesian Data Analysis by Gelman et al. Monte Carlo Strategies in Scientific Computing by Jun Liu "],
["nonparametric-statistics.html", "59 Nonparametric Statistics 59.1 Parametric Inference 59.2 Nonparametric Inference 59.3 Nonparametric Descriptive Statistics 59.4 Semiparametric Inference", " 59 Nonparametric Statistics 59.1 Parametric Inference Parametric inference is based on a family of known probability distributions governed by a defined parameter space. The goal is to perform inference (or more generally statistics) on the values of the parameters. 59.2 Nonparametric Inference Nonparametric inference or modeling can be described in two ways (not mutually exclusive): An inference procedure or model that does not depend on or utilize the parametrized probability distribution from which the data are generated. An inference procedure or model that may have a specific structure or based on a specific formula, but the complexity is adaptive and can grow to arbitrary levels of complexity as the sample size grows. In All of Nonparametric Statistics, Larry Wasserman says: … it is difficult to give a precise definition of nonparametric inference…. For the purposes of this book, we will use the phrase nonparametric inference to refer to a set of modern statistical methods that aim to keep the number of underlying assumptions as weak as possible. He then lists five estimation examples (see Section 1.1): distributions, functionals, densities, regression curves, and Normal means. 59.3 Nonparametric Descriptive Statistics Almost all of the exploratory data analysis methods we covered in the beginning of the course are nonparametric. Sometimes the exploratory methods are calibrated by known probability distributions, but they are usually informative regardless of the underlying probability distribution (or lack thereof) of the data. 59.4 Semiparametric Inference Semiparametric inference or modeling methods contain both parametric and nonparametric components. An example is \\(X_i | \\mu_i \\sim \\mbox{Normal}(\\mu_i, 1)\\) and \\(\\mu_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\) for some arbitrary distribution \\(F\\). "],
["empirical-distribution-functions.html", "60 Empirical Distribution Functions 60.1 Definition 60.2 Example: Normal 60.3 Pointwise Convergence 60.4 Glivenko-Cantelli Theorem 60.5 Dvoretzky-Kiefer-Wolfowitz (DKW) Inequality 60.6 Statistical Functionals 60.7 Plug-In Estimator 60.8 EDF Standard Error 60.9 EDF CLT", " 60 Empirical Distribution Functions 60.1 Definition Suppose \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\). The empirical distribution function (edf) – or empirical cumulative distribution function (ecdf) – is the distribution that puts probability \\(1/n\\) on each observed value \\(X_i\\). Let \\(1(X_i \\leq y) = 1\\) if \\(X_i \\leq y\\) and \\(1(X_i \\leq y) = 0\\) if \\(X_i &gt; y\\). \\[ \\mbox{Random variable: } \\hat{F}_{{\\boldsymbol{X}}}(y) = \\frac{1}{n} \\sum_{i=1}^{n} 1(X_i \\leq y) \\] \\[ \\mbox{Observed variable: } \\hat{F}_{{\\boldsymbol{x}}}(y) = \\frac{1}{n} \\sum_{i=1}^{n} 1(x_i \\leq y) \\] 60.2 Example: Normal 60.3 Pointwise Convergence Under our assumptions, by the strong law of large numbers for each \\(y \\in \\mathbb{R}\\), \\[ \\hat{F}_{{\\boldsymbol{X}}}(y) \\stackrel{\\text{a.s.}}{\\longrightarrow} F(y) \\] as \\(n \\rightarrow \\infty\\). 60.4 Glivenko-Cantelli Theorem Under our assumptions, we can get a much stronger convergence result: \\[ \\sup_{y \\in \\mathbb{R}} \\left| \\hat{F}_{{\\boldsymbol{X}}}(y) - F(y) \\right| \\stackrel{\\text{a.s.}}{\\longrightarrow} 0 \\] as \\(n \\rightarrow \\infty\\). Here, “sup” is short for supremum, which is a mathematical generalization of maximum. This result says that even the worst difference between the edf and the true cdf converges with probability 1 to zero. 60.5 Dvoretzky-Kiefer-Wolfowitz (DKW) Inequality This result gives us an upper bound on how far off the edf is from the true cdf, which allows us to construct confidence bands about the edf. \\[ \\Pr\\left( \\sup_{y \\in \\mathbb{R}} \\left| \\hat{F}_{{\\boldsymbol{X}}}(y) - F(y) \\right| &gt; \\epsilon \\right) \\leq 2 \\exp{-2 n \\epsilon^2} \\] As outlined in All of Nonparametric Statistics, setting \\[\\epsilon_n = \\sqrt{\\frac{1}{2n} \\log\\left(\\frac{2}{\\alpha}\\right)}\\] \\[L(y) = \\max\\{\\hat{F}_{{\\boldsymbol{X}}}(y) - \\epsilon_n, 0 \\}\\] \\[U(y) = \\min\\{\\hat{F}_{{\\boldsymbol{X}}}(y) + \\epsilon_n, 1 \\}\\] guarantees that \\(\\Pr(L(y) \\leq F(y) \\leq U(y) \\mbox{ for all } y) \\geq 1-\\alpha\\). 60.6 Statistical Functionals A statistical functional \\(T(F)\\) is any function of \\(F\\). Examples: \\(\\mu(F) = \\int x dF(x)\\) \\(\\sigma^2(F) = \\int (x-\\mu(F))^2 dF(x)\\) \\(\\text{median}(F) = F^{-1}(1/2)\\) A linear statistical functional is such that \\(T(F) = \\int a(x) dF(x)\\). 60.7 Plug-In Estimator A plug-in estimator of \\(T(F)\\) based on the edf is \\(T(\\hat{F}_{{\\boldsymbol{X}}})\\). Examples: \\(\\hat{\\mu} = \\mu(\\hat{F}_{{\\boldsymbol{X}}}) = \\int x \\hat{F}_{{\\boldsymbol{X}}}(x) = \\frac{1}{n} \\sum_{i=1}^n X_i\\) \\(\\hat{\\sigma}^2 = \\sigma^2(\\hat{F}_{{\\boldsymbol{X}}}) = \\int (x-\\hat{\\mu})^2 \\hat{F}_{{\\boldsymbol{X}}}(x) = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\hat{\\mu})^2\\) \\(\\text{median}(\\hat{F}_{{\\boldsymbol{X}}}) = \\hat{F}_{{\\boldsymbol{X}}}^{-1}(1/2)\\) 60.8 EDF Standard Error Suppose that \\(T(F) = \\int a(x) dF(x)\\) is a linear functional. Then: \\[ \\begin{aligned} \\ &amp; {\\operatorname{Var}}(T(\\hat{F}_{{\\boldsymbol{X}}})) = \\frac{1}{n^2} \\sum_{i=1}^n {\\operatorname{Var}}(a(X_i)) = \\frac{{\\operatorname{Var}}_F(a(X))}{n} \\\\ \\ &amp; {\\operatorname{se}}(T(\\hat{F}_{{\\boldsymbol{X}}})) = \\sqrt{\\frac{{\\operatorname{Var}}_F(a(X))}{n}} \\\\ \\ &amp; \\hat{{\\operatorname{se}}}(T(\\hat{F}_{{\\boldsymbol{X}}})) = \\sqrt{\\frac{{\\operatorname{Var}}_{\\hat{F}_{{\\boldsymbol{X}}}}(a(X))}{n}} \\end{aligned} \\] Note that \\[ {\\operatorname{Var}}_F(a(X)) = \\int (a(x) - T(F))^2 dF(x) \\] because \\(T(F) = \\int a(x) dF(x) = {\\operatorname{E}}_F[a(X)]\\). Likewise, \\[ {\\operatorname{Var}}_{\\hat{F}_{{\\boldsymbol{X}}}}(a(X)) = \\frac{1}{n} \\sum_{i=1}^n (a(X_i) - T(\\hat{F}_{{\\boldsymbol{X}}}))^2 \\] where \\(T(\\hat{F}_{{\\boldsymbol{X}}}) = \\frac{1}{n} \\sum_{i=1}^n a(X_i)\\). 60.9 EDF CLT Suppose that \\({\\operatorname{Var}}_F(a(X)) &lt; \\infty\\). Then we have the following convergences as \\(n \\rightarrow \\infty\\): \\[ \\frac{{\\operatorname{Var}}_{\\hat{F}_{{\\boldsymbol{X}}}}(a(X))}{{\\operatorname{Var}}_{F}(a(X))} \\stackrel{P}{\\longrightarrow} 1 \\mbox{ , } \\frac{\\hat{{\\operatorname{se}}}(T(\\hat{F}_{{\\boldsymbol{X}}}))}{{\\operatorname{se}}(T(\\hat{F}_{{\\boldsymbol{X}}}))} \\stackrel{P}{\\longrightarrow} 1 \\] \\[ \\frac{T(F) - T(\\hat{F}_{{\\boldsymbol{X}}})}{\\hat{{\\operatorname{se}}}(T(\\hat{F}_{{\\boldsymbol{X}}}))} \\stackrel{D}{\\longrightarrow} \\mbox{Normal}(0,1) \\] The estimators are very easy to calculate on real data, so this a powerful set of results. "],
["bootstrap.html", "61 Bootstrap 61.1 Rationale 61.2 Big Picture 61.3 Bootstrap Variance 61.4 Caveat 61.5 Bootstrap Sample 61.6 Bootstrap CIs 61.7 Invoking the CLT 61.8 Percentile Interval 61.9 Pivotal Interval 61.10 Studentized Pivotal Interval 61.11 Bootstrap Hypothesis Testing 61.12 Example: t-test 61.13 Parametric Bootstrap 61.14 Example: Exponential Data", " 61 Bootstrap 61.1 Rationale Suppose \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\). If the edf \\(\\hat{F}_{{\\boldsymbol{X}}}\\) is an accurate approximation for the true cdf \\(F\\), then we can utilize \\(\\hat{F}_{{\\boldsymbol{X}}}\\) in place of \\(F\\) to nonparametrically characterize the sampling distribution of a statistic \\(T({\\boldsymbol{X}})\\). This allows for the sampling distribution of more general statistics to be considered, such as the median or a percentile, as well as more traditional statistics, such as the mean, when the underlying distribution is unknown. When we encounter modeling fitting, the bootstrap may be very useful for characterizing the sampling distribution of complex statistics we calculate from fitted models. 61.2 Big Picture We calculate \\(T({\\boldsymbol{x}})\\) on the observed data, and we also form the edf, \\(\\hat{F}_{{\\boldsymbol{x}}}\\). To approximate the sampling distribution of \\(T({\\boldsymbol{X}})\\) we generate \\(B\\) random samples of \\(n\\) iid data points from \\(\\hat{F}_{{\\boldsymbol{x}}}\\) and calculate \\(T({\\boldsymbol{x}}^{*(b)})\\) for each bootstrap sample \\(b = 1, 2, \\ldots, B\\) where \\({\\boldsymbol{x}}^{*(b)} = (x_1^{*(b)}, x_2^{*(b)}, \\ldots, x_n^{*(b)})^T\\). Sampling \\(X_1^{*}, \\ldots, X_n^{*} {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\hat{F}_{{\\boldsymbol{x}}}\\) is accomplished by sampling \\(n\\) times with replacement from the observed data \\(x_1, x_2, \\ldots, x_n\\). This means \\(\\Pr\\left(X^{*} = x_j\\right) = \\frac{1}{n}\\) for all \\(j\\). 61.3 Bootstrap Variance For each bootstrap sample \\({\\boldsymbol{x}}^{*(b)} = (x_1^{*(b)}, x_2^{*(b)}, \\ldots, x_n^{*(b)})^T\\), calculate bootstrap statistic \\(T({\\boldsymbol{x}}^{*(b)})\\). Repeat this for \\(b = 1, 2, \\ldots, B\\). Estimate the sampling variance of \\(T({\\boldsymbol{x}})\\) by \\[ \\hat{{\\operatorname{Var}}}(T({\\boldsymbol{x}})) = \\frac{1}{B} \\sum_{b=1}^B \\left(T\\left({\\boldsymbol{x}}^{*(b)}\\right) - \\frac{1}{B} \\sum_{k=1}^B T\\left({\\boldsymbol{x}}^{*(k)}\\right) \\right)^2 \\] 61.4 Caveat Why haven’t we just been doing this the entire time?! In All of Nonparametric Statistics, Larry Wasserman states: There is a tendency to treat the bootstrap as a panacea for all problems. But the bootstrap requires regularity conditions to yield valid answers. It should not be applied blindly. The bootstrap is easy to motivate, but it is quite tricky to implement outside of the very standard problems. It sometimes requires deeper knowledge of statistical theory than likelihood-based inference. 61.5 Bootstrap Sample For a sample of size \\(n\\), what percentage of the data is present in any given bootstrap sample? 61.6 Bootstrap CIs Suppose that \\(\\theta = T(F)\\) and \\(\\hat{\\theta} = T(\\hat{F}_{{\\boldsymbol{x}}})\\). We can use the bootstrap to generate data from \\(\\hat{F}_{{\\boldsymbol{x}}}\\). For \\(b = 1, 2, \\ldots, B\\), we draw \\(x_1^{*(b)}, x_2^{*(b)}, \\ldots, x_n^{*(b)}\\) as iid realiztions from \\(\\hat{F}_{{\\boldsymbol{x}}}\\), and calculate \\(\\hat{\\theta}^{*(b)} = T(\\hat{F}_{{\\boldsymbol{x}}^{*(b)}})\\). Let \\(p^{*}_{\\alpha}\\) be the \\(\\alpha\\) percentile of \\(\\left\\{\\hat{\\theta}^{*(1)}, \\hat{\\theta}^{*(2)}, \\ldots, \\hat{\\theta}^{*(B)}\\right\\}\\). Let’s discuss several ways of calculating confidence intervals for \\(\\theta = T(F)\\). 61.7 Invoking the CLT If we have evidence that the central limit theorem can be applied, we can form the \\((1-\\alpha)\\) CI as: \\[ (\\hat{\\theta} - |z_{\\alpha/2}| {\\operatorname{se}}^*, \\hat{\\theta} + |z_{\\alpha/2}| {\\operatorname{se}}^*) \\] where \\({\\operatorname{se}}^*\\) is the bootstrap standard error calculated as \\[ {\\operatorname{se}}^{*} = \\sqrt{\\frac{1}{B} \\sum_{b=1}^B \\left(\\hat{\\theta}^{*(b)} - \\frac{1}{B} \\sum_{k=1}^B \\hat{\\theta}^{*(k)} \\right)^2}. \\] Note that \\({\\operatorname{se}}^*\\) serves as estimate of \\({\\operatorname{se}}(\\hat{\\theta})\\). Note that to get this confidence interval we need to justify that the following pivotal statistics are approximately Normal(0,1): \\[ \\frac{\\hat{\\theta} - \\theta}{{\\operatorname{se}}(\\hat{\\theta})} \\approx \\frac{\\hat{\\theta} - \\theta}{{\\operatorname{se}}^*} \\] 61.8 Percentile Interval If a monotone function \\(m(\\cdot)\\) exists so that \\(m\\left(\\hat{\\theta}\\right) \\sim \\mbox{Normal}(m(\\theta), b^2)\\), then we can form the \\((1-\\alpha)\\) CI as: \\[ \\left(p^*_{\\alpha/2}, p^*_{1-\\alpha/2} \\right) \\] where recall that in general \\(p^{*}_{\\alpha}\\) is the \\(\\alpha\\) percentile of bootstrap estimates \\(\\left\\{\\hat{\\theta}^{*(1)}, \\hat{\\theta}^{*(2)}, \\ldots, \\hat{\\theta}^{*(B)}\\right\\}\\) 61.9 Pivotal Interval Suppose we can calculate percentiles of \\(\\hat{\\theta} - \\theta\\), say \\(q_{\\alpha}\\). Note that the \\(\\alpha\\) percentile of \\(\\hat{\\theta}\\) is \\(q_\\alpha + \\theta\\). The \\(1-\\alpha\\) CI is \\[ (\\hat{\\theta}-q_{1-\\alpha/2}, \\hat{\\theta}-q_{\\alpha/2}) \\] which comes from: \\[ \\begin{aligned} 1-\\alpha &amp; = \\Pr(q_{\\alpha/2} \\leq \\hat{\\theta} - \\theta \\leq q_{1-\\alpha/2}) \\\\ &amp; = \\Pr(-q_{1-\\alpha/2} \\leq \\theta - \\hat{\\theta} \\leq -q_{\\alpha/2}) \\\\ &amp; = \\Pr(\\hat{\\theta}-q_{1-\\alpha/2} \\leq \\theta \\leq \\hat{\\theta}-q_{\\alpha/2}) \\\\ \\end{aligned} \\] Suppose the sampling distribution of \\(\\hat{\\theta}^* - \\hat{\\theta}\\) is an approximation for that of \\(\\hat{\\theta} - \\theta\\). If \\(p^*_{\\alpha}\\) is the \\(\\alpha\\) percentile of \\(\\hat{\\theta}^*\\) then, \\(p^*_{\\alpha} - \\hat{\\theta}\\) is the \\(\\alpha\\) percentile of \\(\\hat{\\theta}^* - \\hat{\\theta}\\). Therefore, \\(p^*_{\\alpha} - \\hat{\\theta}\\) is the bootstrap estimate of \\(q_{\\alpha}\\). Plugging this into \\((\\hat{\\theta}-q_{1-\\alpha/2}, \\hat{\\theta}-q_{\\alpha/2})\\), we get the following \\((1-\\alpha)\\) bootstrap CI: \\[ \\left(2\\hat{\\theta}-p^*_{1-\\alpha/2}, 2\\hat{\\theta}-p^*_{\\alpha/2}\\right). \\] 61.10 Studentized Pivotal Interval In the previous scenario, we needed to assume that the sampling distribution of \\(\\hat{\\theta}^* - \\hat{\\theta}\\) is an approximation for that of \\(\\hat{\\theta} - \\theta\\). Sometimes this will not be the case and instead we can studentize this pivotal quantity. That is, the distribution of \\[ \\frac{\\hat{\\theta} - \\theta}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right)} \\] is well-approximated by that of \\[ \\frac{\\hat{\\theta}^* - \\hat{\\theta}}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*}\\right)}. \\] Let \\(z^{*}_{\\alpha}\\) be the \\(\\alpha\\) percentile of \\[ \\left\\{ \\frac{\\hat{\\theta}^{*(1)} - \\hat{\\theta}}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(1)}\\right)}, \\ldots, \\frac{\\hat{\\theta}^{*(B)} - \\hat{\\theta}}{\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(B)}\\right)} \\right\\}. \\] Then a \\((1-\\alpha)\\) bootstrap CI is \\[ \\left(\\hat{\\theta} - z^{*}_{1-\\alpha/2} \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right), \\hat{\\theta} - z^{*}_{\\alpha/2} \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right)\\right). \\] Exercise: Why? How do we obtain \\(\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right)\\) and \\(\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(b)}\\right)\\)? If we have an analytical formula for these, then \\(\\hat{{\\operatorname{se}}}(\\hat{\\theta})\\) is calculated from the original data and \\(\\hat{{\\operatorname{se}}}(\\hat{\\theta}^{*(b)})\\) from the bootstrap data sets. But we probably don’t since we’re using the bootstrap. Instead, we can calculate: \\[ \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}\\right) = \\sqrt{\\frac{1}{B} \\sum_{b=1}^B \\left(\\hat{\\theta}^{*(b)} - \\frac{1}{B} \\sum_{k=1}^B \\hat{\\theta}^{*(k)} \\right)^2}. \\] This is what we called \\({\\operatorname{se}}^*\\) above. But what about \\(\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(b)}\\right)\\)? To estimate \\(\\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(b)}\\right)\\) we need to do a double bootstrap. For each bootstrap sample \\(b\\) we need to bootstrap that daat set another \\(B\\) times to calculate: \\[ \\hat{{\\operatorname{se}}}\\left(\\hat{\\theta}^{*(b)}\\right) = \\sqrt{\\frac{1}{B} \\sum_{v=1}^B \\left(\\hat{\\theta}^{*(b)*(v)} - \\frac{1}{B} \\sum_{k=1}^B \\hat{\\theta}^{*(b)*(k)} \\right)^2} \\] where \\(\\hat{\\theta}^{*(b)*(v)}\\) is the statistic calculated from bootstrap sample \\(v\\) within bootstrap sample \\(b\\). This can be very computationally intensive, and it requires a large sample size \\(n\\). 61.11 Bootstrap Hypothesis Testing As we have seen, hypothesis testing and confidence intervals are very related. For a simple null hypothesis, a bootstrap hypothesis test p-value can be calculated by finding the minimum \\(\\alpha\\) for which the \\((1-\\alpha)\\) CI does not contain the null hypothesis value. You showed this on your homework. The general approach is to calculate a test statistic based on the observed data. Then the null distribution of this statistic is approximated by forming bootstrap test statistics under the scenario that the null hypothesis is true. This can often be accomplished because the \\(\\hat{\\theta}\\) estimated from the observed data is the population parameter from the bootstrap distribution. 61.12 Example: t-test Suppose \\(X_1, X_2, \\ldots, X_m {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_X\\) and \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_Y\\). We wish to test \\(H_0: \\mu(F_X) = \\mu(F_Y)\\) vs \\(H_1: \\mu(F_X) \\not= \\mu(F_Y)\\). Suppose that we know \\(\\sigma^2(F_X) = \\sigma^2(F_Y)\\) (if not, it is straightforward to adjust the proecure below). Our test statistic is \\[ t = \\frac{\\overline{x} - \\overline{y}}{\\sqrt{\\left(\\frac{1}{m} + \\frac{1}{n}\\right) s^2}} \\] where \\(s^2\\) is the pooled sample variance. Note that the bootstrap distributions are such that \\(\\mu(\\hat{F}_{X^{*}}) = \\overline{x}\\) and \\(\\mu(\\hat{F}_{Y^{*}}) = \\overline{y}\\). Thus we want to center the bootstrap t-statistics about these known means. Specifically, for a bootstrap data set \\(x^{*} = (x_1^{*}, x_2^{*}, \\ldots, x_m^{*})^T\\) and \\(y^{*} = (y_1^{*}, y_2^{*}, \\ldots, y_n^{*})^T\\), we form null t-statistic \\[ t^{*} = \\frac{\\overline{x}^{*} - \\overline{y}^{*} - (\\overline{x} - \\overline{y})}{\\sqrt{\\left(\\frac{1}{m} + \\frac{1}{n}\\right) s^{2*}}} \\] where again \\(s^{2*}\\) is the pooled sample variance. In order to obtain a p-value, we calculate \\(t^{*(b)}\\) for \\(b=1, 2, \\ldots, B\\) bootstrap data sets. The p-value of \\(t\\) is then the proportion of bootstrap statistics as or more extreme than the observed statistic: \\[ \\mbox{p-value}(t) = \\frac{1}{B} \\sum_{b=1}^{B} 1\\left(|t^{*(b)}| \\geq |t|\\right). \\] 61.13 Parametric Bootstrap Suppose \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_\\theta\\) for some parametric \\(F_\\theta\\). We form estimate \\(\\hat{\\theta}\\), but we don’t have a known sampling distribution we can use to do inference with \\(\\hat{\\theta}\\). The parametric bootstrap generates bootstrap data sets from \\(F_{\\hat{\\theta}}\\) rather than from the edf. It proceeds as we outlined above for these bootstrap data sets. 61.14 Example: Exponential Data In the homework, you will be performing a bootstrap t-test of the mean and a bootstrap percentile CI of the median for the following Exponential(\\(\\lambda\\)) data: &gt; set.seed(1111) &gt; pop.mean &lt;- 2 &gt; X &lt;- matrix(rexp(1000*30, rate=1/pop.mean), nrow=1000, ncol=30) Let’s construct a pivotal bootstrap CI of the median here instead. &gt; # population median 2*log(2) &gt; pop_med &lt;- qexp(0.5, rate=1/pop.mean); pop_med [1] 1.386294 &gt; &gt; obs_meds &lt;- apply(X, 1, median) &gt; plot(density(obs_meds, adj=1.5), main=&quot; &quot;); abline(v=pop_med) Some embarrassingly inefficient code to calculate bootstrap medians. &gt; B &lt;- 1000 &gt; boot_meds &lt;- matrix(0, nrow=1000, ncol=B) &gt; &gt; for(b in 1:B) { + idx &lt;- sample(1:30, replace=TRUE) + boot_meds[,b] &lt;- apply(X[,idx], 1, median) + } Plot the bootstrap medians. &gt; plot(density(obs_meds, adj=1.5), main=&quot; &quot;); abline(v=pop_med) &gt; lines(density(as.vector(boot_meds[1:4,]), adj=1.5), col=&quot;red&quot;) &gt; lines(density(as.vector(boot_meds), adj=1.5), col=&quot;blue&quot;) Compare sampling distribution of \\(\\hat{\\theta}-\\theta\\) to \\(\\hat{\\theta}^{*} - \\hat{\\theta}\\). &gt; v &lt;- obs_meds - pop_med &gt; w &lt;- as.vector(boot_meds - obs_meds) &gt; qqplot(v, w, pch=20); abline(0,1) Does a 95% bootstrap pivotal interval provide coverage? &gt; ci_lower &lt;- apply(boot_meds, 1, quantile, probs=0.975) &gt; ci_upper &lt;- apply(boot_meds, 1, quantile, probs=0.025) &gt; &gt; ci_lower &lt;- 2*obs_meds - ci_lower &gt; ci_upper &lt;- 2*obs_meds - ci_upper &gt; &gt; ci_lower[1]; ci_upper[1] [1] 0.8958224 [1] 2.113859 &gt; &gt; cover &lt;- (pop_med &gt;= ci_lower) &amp; (pop_med &lt;= ci_upper) &gt; mean(cover) [1] 0.809 &gt; &gt; # :-( Let’s check the bootstrap variances. &gt; sampling_var &lt;- var(obs_meds) &gt; boot_var &lt;- apply(boot_meds, 1, var) &gt; plot(density(boot_var, adj=1.5), main=&quot; &quot;) &gt; abline(v=sampling_var) We repeated this simulation over a range of \\(n\\) and \\(B\\). n B coverage avg CI width 1e+02 1000 0.868 0.7805404 1e+02 2000 0.872 0.7882278 1e+02 4000 0.865 0.7852837 1e+02 8000 0.883 0.7817222 1e+03 1000 0.923 0.2465840 1e+03 2000 0.909 0.2477463 1e+03 4000 0.915 0.2475550 1e+03 8000 0.923 0.2458167 1e+04 1000 0.935 0.0781421 1e+04 2000 0.937 0.0784541 1e+04 4000 0.942 0.0784559 1e+04 8000 0.948 0.0785591 1e+05 1000 0.949 0.0246918 1e+05 2000 0.942 0.0246938 "],
["permutation-methods.html", "62 Permutation Methods 62.1 Rationale 62.2 Permutation Test 62.3 Wilcoxon Rank Sum Test 62.4 Wilcoxon Signed Rank-Sum Test 62.5 Examples 62.6 Permutation t-test", " 62 Permutation Methods 62.1 Rationale Permutation methods are useful for testing hypotheses about equality of distributions. Observations can be permuted among populations to simulate the case where the distributions are equivalent. Many permutation methods only depend on the ranks of the data, so they are a class of robust methods for performing hypothesis tests. However, the types of hypotheses that can be tested are limited. 62.2 Permutation Test Suppose \\(X_1, X_2, \\ldots, X_m {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_X\\) and \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_Y\\). We wish to test \\(H_0: F_X = F_Y\\) vs \\(H_1: F_X \\not= F_Y\\). Consider a general test statistic \\(S = S(X_1, X_2, \\ldots, X_m, Y_1, Y_2, \\ldots, Y_n)\\) so that the larger \\(S\\) is the more evidence there is against the null hypothesis. Under the null hypothesis, any reordering of these values, where \\(m\\) are randomly assigned to the “\\(X\\)” population and \\(n\\) are assigned to the “\\(Y\\)” population, should be equivalently distributed. For \\(B\\) permutations (possibly all unique permutations), we calculate \\[ S^{*(b)} = S\\left(Z^{*(b)}_1, Z^{*(b)}_2, \\ldots, Z^{*(b)}_m, Z^{*(b)}_{m+1}, \\ldots, Z^{*(b)}_{m+n}\\right) \\] where \\(Z^{*(b)}_1, Z^{*(b)}_2, \\ldots, Z^{*(b)}_m, Z^{*(b)}_{m+1}, \\ldots, Z^{*(b)}_{m+n}\\) is a random permutation of the values \\(X_1, X_2, \\ldots, X_m, Y_1, Y_2, \\ldots, Y_n\\). Example permutation in R: &gt; z &lt;- c(x, y) &gt; zstar &lt;- sample(z, replace=FALSE) The p-value is calculated as proportion of permutations where the resulting permutation statistic exceeds the observed statistics: \\[ \\mbox{p-value}(s) = \\frac{1}{B} \\sum_{b=1}^{B} 1\\left(S^{*(b)} \\geq S\\right). \\] This can be (1) an exact calculation where all permutations are considered, (2) a Monte Carlo approximation where \\(B\\) random permutations are considered, or (3) a large \\(\\min(m, n)\\) calculation where an asymptotic probabilistic approximation is used. 62.3 Wilcoxon Rank Sum Test Also called the Mann-Whitney-Wilcoxon test. Consider the ranks of the data as a whole, \\(X_1, X_2, \\ldots, X_m, Y_1, Y_2, \\ldots, Y_n\\), where \\(r(X_i)\\) is the rank of \\(X_i\\) and \\(r(Y_j)\\) is the rank of \\(Y_j\\). Note that \\(r(\\cdot) \\in \\{1, 2, \\ldots, m+n\\}\\). The smallest value is such that \\(r(X_i)=1\\) or \\(r(Y_j)=1\\), the next smallest value maps to 2, etc. Note that \\[ \\sum_{i=1}^m r(X_i) + \\sum_{j=1}^n r(Y_j) = \\frac{(m+n)(m+n+1)}{2}. \\] The statistic \\(W\\) is calculated by: \\[ \\begin{aligned} &amp; R_X = \\sum_{i=1}^m r(X_i) &amp; R_Y = \\sum_{j=1}^n r(Y_j) \\\\ &amp; W_X = R_X - \\frac{m(m+1)}{2} &amp; W_Y = R_Y - \\frac{n(n+1)}{2} \\\\ &amp; W = \\min(W_X, W_Y) &amp; \\end{aligned} \\] In this case, the smaller \\(W\\) is, the more significant it is. Note that \\(mn-W = \\max(W_X, W_Y)\\), so we just as well could utilize large \\(\\max(W_X, W_Y)\\) as a test statistic. 62.4 Wilcoxon Signed Rank-Sum Test The Wilcoxon signed rank test is similar to the Wilcoxon two-sample test, except here we have paired observations \\((X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\). An example is an individual’s clinical measurement before (\\(X\\)) and after (\\(Y\\)) treatment. In order to test the hypothesis, we calculate \\(r(X_i, Y_i) = |Y_i - X_i|\\) and also \\(s(X_i, Y_i) = \\operatorname{sign}(Y_i - X_i)\\). The test statistic is \\(|W|\\) where \\[ W = \\sum_{i=1}^n r(X_i, Y_i) s(X_i, Y_i). \\] Both of these tests can be carried out using the wilcox.test() function in R. wilcox.test(x, y = NULL, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), mu = 0, paired = FALSE, exact = NULL, correct = TRUE, conf.int = FALSE, conf.level = 0.95, ...) 62.5 Examples Same population mean and variance. &gt; x &lt;- rnorm(100, mean=1) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 4480, p-value = 0.2043 alternative hypothesis: true location shift is not equal to 0 &gt; qqplot(x, y); abline(0,1) Same population mean and variance. Large sample size. &gt; x &lt;- rnorm(10000, mean=1) &gt; y &lt;- rexp(10000, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 53312000, p-value = 4.985e-16 alternative hypothesis: true location shift is not equal to 0 &gt; qqplot(x, y); abline(0,1) Same mean, very different variances. &gt; x &lt;- rnorm(100, mean=1, sd=0.01) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 6579, p-value = 0.0001148 alternative hypothesis: true location shift is not equal to 0 &gt; qqplot(x, y); abline(0,1) Same variances, different means. &gt; x &lt;- rnorm(100, mean=2) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 7836, p-value = 4.261e-12 alternative hypothesis: true location shift is not equal to 0 &gt; qqplot(x, y); abline(0,1) Same population mean and variance. &gt; x &lt;- rnorm(100, mean=1) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y, paired=TRUE) Wilcoxon signed rank test with continuity correction data: x and y V = 2394, p-value = 0.6536 alternative hypothesis: true location shift is not equal to 0 &gt; hist(y-x) Same population mean and variance. Large sample size. &gt; x &lt;- rnorm(10000, mean=1) &gt; y &lt;- rexp(10000, rate=1) &gt; wilcox.test(x, y, paired=TRUE) Wilcoxon signed rank test with continuity correction data: x and y V = 26770000, p-value = 9.23e-10 alternative hypothesis: true location shift is not equal to 0 &gt; hist(y-x) 62.6 Permutation t-test As above, suppose \\(X_1, X_2, \\ldots, X_m {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_X\\) and \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_Y\\), and we wish to test \\(H_0: F_X = F_Y\\) vs \\(H_1: F_X \\not= F_Y\\). However, suppose we additionally know that \\({\\operatorname{Var}}(X) = {\\operatorname{Var}}(Y)\\). We can use a t-statistic to test this hypothesis: \\[ t = \\frac{\\overline{x} - \\overline{y}}{\\sqrt{\\left(\\frac{1}{m} + \\frac{1}{n}\\right) s^2}} \\] where \\(s^2\\) is the pooled sample variance. To obtain the null distribution, we randomly permute the observations to assign \\(m\\) data points to the \\(X\\) sample and \\(n\\) to the \\(Y\\) sample. This yields permutation data set \\(x^{*} = (x_1^{*}, x_2^{*}, \\ldots, x_m^{*})^T\\) and \\(y^{*} = (y_1^{*}, y_2^{*}, \\ldots, y_n^{*})^T\\). We form null t-statistic \\[ t^{*} = \\frac{\\overline{x}^{*} - \\overline{y}^{*}}{\\sqrt{\\left(\\frac{1}{m} + \\frac{1}{n}\\right) s^{2*}}} \\] where again \\(s^{2*}\\) is the pooled sample variance. In order to obtain a p-value, we calculate \\(t^{*(b)}\\) for \\(b=1, 2, \\ldots, B\\) permutation data sets. The p-value of \\(t\\) is then the proportion of permutation statistics as or more extreme than the observed statistic: \\[ \\mbox{p-value}(t) = \\frac{1}{B} \\sum_{b=1}^{B} 1\\left(|t^{*(b)}| \\geq |t|\\right). \\] "],
["goodness-of-fit.html", "63 Goodness of Fit 63.1 Rationale 63.2 Chi-Square GoF Test 63.3 Example: Hardy-Weinberg 63.4 Kolmogorov–Smirnov Test 63.5 One Sample KS Test 63.6 Two Sample KS Test 63.7 Example: Exponential vs Normal", " 63 Goodness of Fit 63.1 Rationale Sometimes we want to figure out which probability distribution is a reasonable model for the data. This is related to nonparametric inference in that we wish to go from being in a nonparametric framework to a parametric framework. Goodness of fit (GoF) tests allow one to perform a hypothesis test of how well a particular parametric probability model explains variation observed in a data set. 63.2 Chi-Square GoF Test Suppose we have data generating process \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\) for some probability distribution \\(F\\). We wish to test \\(H_0: F \\in \\{F_{{\\boldsymbol{\\theta}}}: {\\boldsymbol{\\theta}}\\in \\boldsymbol{\\Theta} \\}\\) vs \\(H_1: \\mbox{not } H_0\\). Suppose that \\(\\boldsymbol{\\Theta}\\) is \\(d\\)-dimensional. Divide the support of \\(\\{F_{{\\boldsymbol{\\theta}}}: {\\boldsymbol{\\theta}}\\in \\boldsymbol{\\Theta} \\}\\) into \\(k\\) bins \\(I_1, I_2, \\ldots, I_k\\). For \\(j=1, 2, \\ldots, k\\), calculate \\[ q_j({\\boldsymbol{\\theta}}) = \\int_{I_j} dF_{{\\boldsymbol{\\theta}}}(x). \\] Suppose we observe data \\(x_1, x_2, \\ldots, x_n\\). For \\(j = 1, 2, \\ldots, k\\), let \\(n_j\\) be the number of values \\(x_i \\in I_j\\). Let \\(\\tilde{\\theta}_1, \\tilde{\\theta}_2, \\ldots, \\tilde{\\theta}_d\\) be the values that maximize the multinomial likelihood \\[ \\prod_{j=1}^k q_j({\\boldsymbol{\\theta}})^{n_j}. \\] Form GoF statistic \\[ s({\\boldsymbol{x}}) = \\sum_{j=1}^k \\frac{\\left(n_j - n q_j\\left(\\tilde{{\\boldsymbol{\\theta}}} \\right) \\right)^2}{n q_j\\left(\\tilde{{\\boldsymbol{\\theta}}} \\right)} \\] When \\(H_0\\) is true, \\(S \\sim \\chi^2_v\\) where \\(v = k - d - 1\\). The p-value is calculated by \\(\\Pr(S^* \\geq s({\\boldsymbol{x}}))\\) where \\(S^* \\sim \\chi^2_{k-d-1}\\). 63.3 Example: Hardy-Weinberg Suppose at your favorite SNP, we observe genotypes from 100 randomly sampled individuals as follows: AA AT TT 28 60 12 If we code these genotypes as 0, 1, 2, testing for Hardy-Weinberg equilibrium is equivalent to testing whether \\(X_1, X_2, \\ldots, X_{100} {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Binomial}(2, \\theta)\\) for some unknown allele frequency of T, \\(\\theta\\). The parameter dimension is such that \\(d=1\\). We will also set \\(k=3\\), where each bin is a genotype. Therefore, we have \\(n_1 = 28\\), \\(n_2 = 60\\), and \\(n_3 = 12\\). Also, \\[ q_1(\\theta) = (1-\\theta)^2, \\ \\ q_2(\\theta) = 2 \\theta (1-\\theta), \\ \\ q_3(\\theta) = \\theta^2. \\] Forming the multinomial likelihood under these bin probabilities, we find \\(\\tilde{\\theta} = (n_2 + 2n_3)/(2n)\\). The degrees of freedom of the \\(\\chi^2_v\\) null distribution is \\(v = k - d - 1 = 3 - 1 - 1 = 1\\). Let’s carry out the test in R. &gt; n &lt;- 100 &gt; nj &lt;- c(28, 60, 12) &gt; &gt; # parameter estimates &gt; theta &lt;- (nj[2] + 2*nj[3])/(2*n) &gt; qj &lt;- c((1-theta)^2, 2*theta*(1-theta), theta^2) &gt; &gt; # gof statistic &gt; s &lt;- sum((nj - n*qj)^2 / (n*qj)) &gt; &gt; # p-value &gt; 1-pchisq(s, df=1) [1] 0.02059811 63.4 Kolmogorov–Smirnov Test The KS test can be used to compare a sample of data to a particular distribution, or to compare two samples of data. The former is a parametric GoF test, and the latter is a nonparametric test of equal distributions. 63.5 One Sample KS Test Suppose we have data generating process \\(X_1, X_2, \\ldots, X_n \\sim F\\) for some probability distribution \\(F\\). We wish to test \\(H_0: F = F_{{\\boldsymbol{\\theta}}}\\) vs \\(H_1: F \\not= F_{{\\boldsymbol{\\theta}}}\\) for some parametric distribution \\(F_{{\\boldsymbol{\\theta}}}\\). For observed data \\(x_1, x_2, \\ldots, x_n\\) we form the edf \\(\\hat{F}_{{\\boldsymbol{x}}}\\) and test-statistic \\[ D({\\boldsymbol{x}}) = \\sup_{z} \\left| \\hat{F}_{{\\boldsymbol{x}}}(z) - F_{{\\boldsymbol{\\theta}}}(z) \\right|. \\] The null distribution of this test can be approximated based on a stochastic process called the Brownian bridge. 63.6 Two Sample KS Test Suppose \\(X_1, X_2, \\ldots, X_m {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_X\\) and \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_Y\\). We wish to test \\(H_0: F_X = F_Y\\) vs \\(H_1: F_X \\not= F_Y\\). For observed data \\(x_1, x_2, \\ldots, x_m\\) and \\(y_1, y_2, \\ldots, y_n\\) we form the edf’s \\(\\hat{F}_{{\\boldsymbol{x}}}\\) and \\(\\hat{F}_{\\boldsymbol{y}}\\). We then form test-statistic \\[ D({\\boldsymbol{x}},\\boldsymbol{y}) = \\sup_{z} \\left| \\hat{F}_{{\\boldsymbol{x}}}(z) - \\hat{F}_{\\boldsymbol{y}}(z) \\right|. \\] The null distribution of this statistic can be approximated using results on edf’s. Both of these tests can be carried out using the ks.test() function in R. ks.test(x, y, ..., alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), exact = NULL) 63.7 Example: Exponential vs Normal Two sample KS test. &gt; x &lt;- rnorm(100, mean=1) &gt; y &lt;- rexp(100, rate=1) &gt; wilcox.test(x, y) Wilcoxon rank sum test with continuity correction data: x and y W = 5021, p-value = 0.9601 alternative hypothesis: true location shift is not equal to 0 &gt; ks.test(x, y) Two-sample Kolmogorov-Smirnov test data: x and y D = 0.19, p-value = 0.0541 alternative hypothesis: two-sided &gt; qqplot(x, y); abline(0,1) One sample KS tests. &gt; ks.test(x=x, y=&quot;pnorm&quot;) One-sample Kolmogorov-Smirnov test data: x D = 0.41398, p-value = 2.554e-15 alternative hypothesis: two-sided &gt; &gt; ks.test(x=x, y=&quot;pnorm&quot;, mean=1) One-sample Kolmogorov-Smirnov test data: x D = 0.068035, p-value = 0.7436 alternative hypothesis: two-sided Standardize (mean center, sd scale) the observations before comparing to a Normal(0,1) distribution. &gt; ks.test(x=((x-mean(x))/sd(x)), y=&quot;pnorm&quot;) One-sample Kolmogorov-Smirnov test data: ((x - mean(x))/sd(x)) D = 0.05896, p-value = 0.8778 alternative hypothesis: two-sided &gt; &gt; ks.test(x=((y-mean(y))/sd(y)), y=&quot;pnorm&quot;) One-sample Kolmogorov-Smirnov test data: ((y - mean(y))/sd(y)) D = 0.14439, p-value = 0.03092 alternative hypothesis: two-sided "],
["method-of-moments.html", "64 Method of Moments 64.1 Rationale 64.2 Definition 64.3 Example: Normal 64.4 Exploring Goodness of Fit", " 64 Method of Moments 64.1 Rationale Suppose that \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F\\). By the strong law of large numbers we have, as \\(n \\rightarrow \\infty\\) \\[ \\frac{\\sum_{i=1}^n X_i^k}{n} \\stackrel{\\text{a.s.}}{\\longrightarrow} {\\operatorname{E}}_{F}\\left[X^k\\right] \\] when \\({\\operatorname{E}}_{F}\\left[X^k\\right]\\) exists. This means that we can nonparametrically estimate the moments of a distribution. Also, in the parametric setting, these moments can be used to form parameter estimates. 64.2 Definition Suppose that \\(X_1, X_2, \\ldots, X_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}F_{{\\boldsymbol{\\theta}}}\\) where \\({\\boldsymbol{\\theta}}\\) is \\(d\\)-dimensional. Calculate moments \\({\\operatorname{E}}\\left[X^k\\right]\\) for \\(k = 1, 2, \\ldots, d&#39;\\) where \\(d&#39; \\geq d\\). For each parameter \\(j = 1, 2, \\ldots, d\\), solve for \\(\\theta_j\\) in terms of \\({\\operatorname{E}}\\left[X^k\\right]\\) for \\(k = 1, 2, \\ldots, d&#39;\\). The method of moments estimator of \\(\\theta_j\\) is formed by replacing the function of moments \\({\\operatorname{E}}\\left[X^k\\right]\\) that equals \\(\\theta_j\\) with the empirical moments \\(\\sum_{i=1}^n X_i^k / n\\). 64.3 Example: Normal For a \\(\\mbox{Normal}(\\mu, \\sigma^2)\\) distribution, we have \\[ {\\operatorname{E}}[X] = \\mu \\] \\[ {\\operatorname{E}}\\left[X^2\\right] = \\sigma^2 + \\mu^2 \\] Solving for \\(\\mu\\) and \\(\\sigma^2\\), we have \\(\\mu = {\\operatorname{E}}[X]\\) and \\(\\sigma^2 = {\\operatorname{E}}[X^2] - {\\operatorname{E}}[X]^2\\). This yields method of moments estimators \\[ \\tilde{\\mu} = \\frac{\\sum_{i=1}^n X_i}{n}, \\ \\ \\ \\tilde{\\sigma}^2 = \\frac{\\sum_{i=1}^n X_i^2}{n} - \\left[\\frac{\\sum_{i=1}^n X_i}{n}\\right]^2. \\] 64.4 Exploring Goodness of Fit As mentioned above, moments can be nonparametrically estimated. At the same time, for a given parametric distribution, these moments can also be written in terms of the parameters. For example, consider a single parameter exponential family distribution. The variance is going to be defined in terms of the parameter. At the same time, we can estimate variance through the empirical moments \\[ \\frac{\\sum_{i=1}^n X_i^2}{n} - \\left[\\frac{\\sum_{i=1}^n X_i}{n}\\right]^2. \\] In the scenario where several sets of variables are measured, the MLEs of the variance in terms of the single parameter can be compared to the moment estimates of variance to assess goodness of fit of that distribution. "],
["types-of-models.html", "65 Types of Models 65.1 Probabilistic Models 65.2 Multivariate Models 65.3 Variables 65.4 Statistical Model 65.5 Parametric vs Nonparametric 65.6 Simple Linear Regression 65.7 Ordinary Least Squares 65.8 Generalized Least Squares 65.9 Matrix Form of Linear Models 65.10 Least Squares Regression 65.11 Generalized Linear Models 65.12 Generalized Additive Models 65.13 Some Trade-offs 65.14 Bias and Variance", " 65 Types of Models 65.1 Probabilistic Models So far we have covered inference of paramters that quantify a population of interest. This is called inference of probabilistic models. 65.2 Multivariate Models Some of the probabilistic models we considered involve calculating conditional probabilities such as \\(\\Pr({\\boldsymbol{Z}}| {\\boldsymbol{X}}; {\\boldsymbol{\\theta}})\\) or \\(\\Pr({\\boldsymbol{\\theta}}| {\\boldsymbol{X}})\\). It is often the case that we would like to build a model that explains the variation of one variable in terms of other variables. Statistical modeling typically refers to this goal. 65.3 Variables Let’s suppose our does comes in the form \\(({\\boldsymbol{X}}_1, Y_1), ({\\boldsymbol{X}}_2, Y_2), \\ldots, ({\\boldsymbol{X}}_n, Y_n) \\sim F\\). We will call \\({\\boldsymbol{X}}_i = (X_{i1}, X_{i2}, \\ldots, X_{ip}) \\in \\mathbb{R}_{1 \\times p}\\) the explanatory variables and \\(Y_i \\in \\mathbb{R}\\) the dependent variable or response variable. We can collect all variables as matrices \\[ {\\boldsymbol{Y}}_{n \\times 1} \\ \\mbox{ and } \\ {\\boldsymbol{X}}_{n \\times p}\\] where each row is a unique observation. 65.4 Statistical Model Statistical models are concerned with how variables are dependent. The most general model would be to infer \\[ \\Pr(Y | {\\boldsymbol{X}}) = h({\\boldsymbol{X}}) \\] where we would specifically study the form of \\(h(\\cdot)\\) to understand how \\(Y\\) is dependent on \\({\\boldsymbol{X}}\\). A more modest goal is to infer the transformed conditional expecation \\[ g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = h({\\boldsymbol{X}}) \\] which sometimes leads us back to an estimate of \\(\\Pr(Y | {\\boldsymbol{X}})\\). 65.5 Parametric vs Nonparametric A parametric model is a pre-specified form of \\(h(X)\\) whose terms can be characterized by a formula and interpreted. This usually involves parameters on which inference can be performed, such as coefficients in a linear model. A nonparametric model is a data-driven form of \\(h(X)\\) that is often very flexible and is not easily expressed or intepreted. A nonparametric model often does not include parameters on which we can do inference. 65.6 Simple Linear Regression For random variables \\((X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\), simple linear regression estimates the model \\[ Y_i = \\beta_1 + \\beta_2 X_i + E_i \\] where \\({\\operatorname{E}}[E_i] = 0\\), \\({\\operatorname{Var}}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). Note that in this model \\({\\operatorname{E}}[Y | X] = \\beta_1 + \\beta_2 X.\\) 65.7 Ordinary Least Squares Ordinary least squares (OLS) estimates the model \\[ \\begin{aligned} Y_i &amp; = \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + E_i \\\\ &amp; = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}+ E_i \\end{aligned} \\] where \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). Note that typically \\(X_{i1} = 1\\) for all \\(i\\) so that \\(\\beta_1 X_{i1} = \\beta_1\\) serves as the intercept. 65.8 Generalized Least Squares Generalized least squares (GLS) assumes the same model as OLS, except it allows for heteroskedasticity and covariance among the \\(E_i\\). Specifically, it is assumed that \\({\\boldsymbol{E}}= (E_1, \\ldots, E_n)^T\\) is distributed as \\[ {\\boldsymbol{E}}_{n \\times 1} \\sim (\\boldsymbol{0}, {\\boldsymbol{\\Sigma}}) \\] where \\(\\boldsymbol{0}\\) is the expected value \\({\\boldsymbol{\\Sigma}}= (\\sigma_{ij})\\) is the \\(n \\times n\\) symmetric covariance matrix. 65.9 Matrix Form of Linear Models We can write the models as \\[ {\\boldsymbol{Y}}_{n \\times 1} = {\\boldsymbol{X}}_{n \\times p} {\\boldsymbol{\\beta}}_{p \\times 1} + {\\boldsymbol{E}}_{n \\times 1} \\] where simple linear regression, OLS, and GLS differ in the value of \\(p\\) or the distribution of the \\(E_i\\). We can also write the conditional expecation and covariance as \\[ {\\operatorname{E}}[{\\boldsymbol{Y}}| {\\boldsymbol{X}}] = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}, \\ {\\operatorname{Cov}}({\\boldsymbol{Y}}| {\\boldsymbol{X}}) = {\\boldsymbol{\\Sigma}}. \\] 65.10 Least Squares Regression In simple linear regression, OLS, and GLS, the \\({\\boldsymbol{\\beta}}\\) parameters are fit by minimizing the sum of squares between \\({\\boldsymbol{Y}}\\) and \\({\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\). Fitting these models by “least squares” satisfies two types of optimality: Gauss-Markov Theorem Maximum likelihood estimate when in addition \\({\\boldsymbol{E}}\\sim \\mbox{MVN}_n(\\boldsymbol{0}, {\\boldsymbol{\\Sigma}})\\) Details will follow on these. 65.11 Generalized Linear Models The generalized linear model (GLM) builds from OLS and GLS to allow the response variable to be distributed according to an exponential family distribution. Suppose that \\(\\eta(\\theta)\\) is function of the expected value into the natural parameter. The estimated model is \\[ \\eta\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\] which is fit by maximized likelihood estimation. 65.12 Generalized Additive Models Next week, we will finally arrive at inferring semiparametric models where \\(Y | {\\boldsymbol{X}}\\) is distributed according to an exponential family distribution. The models, which are called generalized additive models (GAMs), will be of the form \\[ \\eta\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = \\sum_{j=1}^p \\sum_{k=1}^d h_k(X_{j}) \\] where \\(\\eta\\) is the canonical link function and the \\(h_k(\\cdot)\\) functions are very flexible. 65.13 Some Trade-offs There are several important trade-offs encountered in statistical modeling: Bias vs variance Accuracy vs computational time Flexibility vs intepretability These are not mutually exclusive phenomena. 65.14 Bias and Variance Suppose we estimate \\(Y = h({\\boldsymbol{X}}) + E\\) by some \\(\\hat{Y} = \\hat{h}({\\boldsymbol{X}})\\). The following bias-variance trade-off exists: \\[ \\begin{aligned} {\\operatorname{E}}\\left[\\left(Y - \\hat{Y}\\right)^2\\right] &amp; = {\\rm E}\\left[\\left(h({\\boldsymbol{X}}) + E - \\hat{h}({\\boldsymbol{X}})\\right)^2\\right] \\\\ \\ &amp; = {\\rm E}\\left[\\left(h({\\boldsymbol{X}}) - \\hat{h}({\\boldsymbol{X}})\\right)^2\\right] + {\\rm Var}(E) \\\\ \\ &amp; = \\left(h({\\boldsymbol{X}}) - {\\rm E}[\\hat{h}({\\boldsymbol{X}})]\\right)^2 + {\\rm Var}\\left(\\hat{h}({\\boldsymbol{X}})\\right)^2 + {\\rm Var}(E) \\\\ \\ &amp; = \\mbox{bias}^2 + \\mbox{variance} + {\\rm Var}(E) \\end{aligned} \\] "],
["motivating-examples.html", "66 Motivating Examples 66.1 Sample Correlation 66.2 Example: Hand Size Vs. Height 66.3 Cor. of Hand Size and Height 66.4 L/R Hand Sizes 66.5 Correlation of Hand Sizes 66.6 Davis Data 66.7 Height and Weight 66.8 Correlation of Height and Weight 66.9 Correlation Among Females 66.10 Correlation Among Males", " 66 Motivating Examples 66.1 Sample Correlation Least squares regression “modelizes” correlation. Suppose we observe \\(n\\) pairs of data \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\). Their sample correlation is \\[\\begin{eqnarray} r_{xy} &amp; = &amp; \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2 \\sum_{i=1}^n (y_i - \\overline{y})^2}} \\\\ \\ &amp; = &amp; \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{(n-1) s_x s_y} \\end{eqnarray}\\] where \\(s_x\\) and \\(s_y\\) are the sample standard deviations of each measured variable. 66.2 Example: Hand Size Vs. Height &gt; library(&quot;MASS&quot;) &gt; data(&quot;survey&quot;, package=&quot;MASS&quot;) &gt; head(survey) Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height 1 Female 18.5 18.0 Right R on L 92 Left Some Never 173.00 2 Male 19.5 20.5 Left R on L 104 Left None Regul 177.80 3 Male 18.0 13.3 Right L on R 87 Neither None Occas NA 4 Male 18.8 18.9 Right R on L NA Neither None Never 160.00 5 Male 20.0 20.0 Right Neither 35 Right Some Never 165.00 6 Female 18.0 17.7 Right L on R 64 Right Some Never 172.72 M.I Age 1 Metric 18.250 2 Imperial 17.583 3 &lt;NA&gt; 16.917 4 Metric 20.333 5 Metric 23.667 6 Imperial 21.000 &gt; ggplot(data = survey, mapping=aes(x=Wr.Hnd, y=Height)) + + geom_point() + geom_vline(xintercept=mean(survey$Wr.Hnd, na.rm=TRUE)) + + geom_hline(yintercept=mean(survey$Height, na.rm=TRUE)) 66.3 Cor. of Hand Size and Height &gt; cor.test(x=survey$Wr.Hnd, y=survey$Height) Pearson&#39;s product-moment correlation data: survey$Wr.Hnd and survey$Height t = 10.792, df = 206, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.5063486 0.6813271 sample estimates: cor 0.6009909 66.4 L/R Hand Sizes &gt; ggplot(data = survey) + + geom_point(aes(x=Wr.Hnd, y=NW.Hnd)) 66.5 Correlation of Hand Sizes &gt; cor.test(x=survey$Wr.Hnd, y=survey$NW.Hnd) Pearson&#39;s product-moment correlation data: survey$Wr.Hnd and survey$NW.Hnd t = 45.712, df = 234, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.9336780 0.9597816 sample estimates: cor 0.9483103 66.6 Davis Data &gt; library(&quot;car&quot;) &gt; data(&quot;Davis&quot;, package=&quot;car&quot;) Warning in data(&quot;Davis&quot;, package = &quot;car&quot;): data set &#39;Davis&#39; not found &gt; htwt &lt;- tbl_df(Davis) &gt; htwt[12,c(2,3)] &lt;- htwt[12,c(3,2)] &gt; head(htwt) # A tibble: 6 x 5 sex weight height repwt repht &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 M 77 182 77 180 2 F 58 161 51 159 3 F 53 161 54 158 4 M 68 177 70 175 5 F 59 157 59 155 6 M 76 170 76 165 66.7 Height and Weight &gt; ggplot(htwt) + + geom_point(aes(x=height, y=weight, color=sex), size=2, alpha=0.5) + + scale_color_manual(values=c(&quot;red&quot;, &quot;blue&quot;)) 66.8 Correlation of Height and Weight &gt; cor.test(x=htwt$height, y=htwt$weight) Pearson&#39;s product-moment correlation data: htwt$height and htwt$weight t = 17.04, df = 198, p-value &lt; 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.7080838 0.8218898 sample estimates: cor 0.7710743 66.9 Correlation Among Females &gt; htwt %&gt;% filter(sex==&quot;F&quot;) %&gt;% + cor.test(~ height + weight, data = .) Pearson&#39;s product-moment correlation data: height and weight t = 6.2801, df = 110, p-value = 6.922e-09 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.3627531 0.6384268 sample estimates: cor 0.5137293 66.10 Correlation Among Males &gt; htwt %&gt;% filter(sex==&quot;M&quot;) %&gt;% + cor.test(~ height + weight, data = .) Pearson&#39;s product-moment correlation data: height and weight t = 5.9388, df = 86, p-value = 5.922e-08 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.3718488 0.6727460 sample estimates: cor 0.5392906 Why are the stratified correlations lower? "],
["simple-linear-regression-1.html", "67 Simple Linear Regression 67.1 Definition 67.2 Rationale 67.3 Setup 67.4 Line Minimizing Squared Error 67.5 Least Squares Solution 67.6 Visualizing Least Squares Line 67.7 Example: Height and Weight 67.8 Calculate the Line Directly 67.9 Plot the Line 67.10 Observed Data, Fits, and Residuals 67.11 Proportion of Variation Explained", " 67 Simple Linear Regression 67.1 Definition For random variables \\((X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\), simple linear regression estimates the model \\[ Y_i = \\beta_1 + \\beta_2 X_i + E_i \\] where \\({\\operatorname{E}}[E_i] = 0\\), \\({\\operatorname{Var}}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). 67.2 Rationale Least squares linear regression is one of the simplest and most useful modeling systems for building a model that explains the variation of one variable in terms of other variables. It is simple to fit, it satisfies some optimality criteria, and it is straightforward to check assumptions on the data so that statistical inference can be performed. 67.3 Setup Suppose that we have observed \\(n\\) pairs of data \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\). Least squares linear regression models variation of the response variable \\(y\\) in terms of the explanatory variable \\(x\\) in the form of \\(\\beta_1 + \\beta_2 x\\), where \\(\\beta_1\\) and \\(\\beta_2\\) are chosen to satisfy a least squares optimization. 67.4 Line Minimizing Squared Error The least squares regression line is formed from the value of \\(\\beta_1\\) and \\(\\beta_2\\) that minimize: \\[\\sum_{i=1}^n \\left( y_i - \\beta_1 - \\beta_2 x_i \\right)^2.\\] For a given set of data, there is a unique solution to this minimization as long as there are at least two unique values among \\(x_1, x_2, \\ldots, x_n\\). Let \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_2}\\) be the values that minimize this sum of squares. 67.5 Least Squares Solution These values are: \\[\\hat{\\beta}_2 = r_{xy} \\frac{s_y}{s_x}\\] \\[\\hat{\\beta}_1 = \\overline{y} - \\hat{\\beta}_2 \\overline{x}\\] These values have a useful interpretation. 67.6 Visualizing Least Squares Line 67.7 Example: Height and Weight &gt; ggplot(data=htwt, mapping=aes(x=height, y=weight)) + + geom_point(size=2, alpha=0.5) + + geom_smooth(method=&quot;lm&quot;, se=FALSE, formula=y~x) 67.8 Calculate the Line Directly &gt; beta2 &lt;- cor(htwt$height, htwt$weight) * + sd(htwt$weight) / sd(htwt$height) &gt; beta2 [1] 1.150092 &gt; &gt; beta1 &lt;- mean(htwt$weight) - beta2 * mean(htwt$height) &gt; beta1 [1] -130.9104 &gt; &gt; yhat &lt;- beta1 + beta2 * htwt$height 67.9 Plot the Line &gt; df &lt;- data.frame(htwt, yhat=yhat) &gt; ggplot(data=df) + geom_point(aes(x=height, y=weight), size=2, alpha=0.5) + + geom_line(aes(x=height, y=yhat), color=&quot;blue&quot;, size=1.2) 67.10 Observed Data, Fits, and Residuals We observe data \\((x_1, y_1), \\ldots, (x_n, y_n)\\). Note that we only observe \\(X_i\\) and \\(Y_i\\) from the generative model \\(Y_i = \\beta_1 + \\beta_2 X_i + E_i\\). We calculate fitted values and observed residuals: \\[\\hat{y}_i = \\hat{\\beta}_1 + \\hat{\\beta}_2 x_i\\] \\[\\hat{e}_i = y_i - \\hat{y}_i\\] By construction, it is the case that \\(\\sum_{i=1}^n \\hat{e}_i = 0\\). 67.11 Proportion of Variation Explained The proportion of variance explained by the fitted model is called \\(R^2\\) or \\(r^2\\). It is calculated by: \\[r^2 = \\frac{s^2_{\\hat{y}}}{s^2_{y}}\\] "],
["lm-function-in-r.html", "68 lm() Function in R 68.1 Calculate the Line in R 68.2 An lm Object is a List 68.3 From the R Help 68.4 Some of the List Items 68.5 summary() 68.6 summary() List Elements 68.7 Using tidy() 68.8 Proportion of Variation Explained 68.9 Assumptions to Verify 68.10 Residual Distribution 68.11 Normal Residuals Check 68.12 Fitted Values Vs. Obs. Residuals", " 68 lm() Function in R 68.1 Calculate the Line in R The syntax for a model in R is response variable ~ explanatory variables where the explanatory variables component can involve several types of terms. &gt; myfit &lt;- lm(weight ~ height, data=htwt) &gt; myfit Call: lm(formula = weight ~ height, data = htwt) Coefficients: (Intercept) height -130.91 1.15 68.2 An lm Object is a List &gt; class(myfit) [1] &quot;lm&quot; &gt; is.list(myfit) [1] TRUE &gt; names(myfit) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; 68.3 From the R Help lm returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”). The functions summary and anova are used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm. 68.4 Some of the List Items These are some useful items to access from the lm object: coefficients: a named vector of coefficients residuals: the residuals, that is response minus fitted values. fitted.values: the fitted mean values. df.residual: the residual degrees of freedom. call: the matched call. model: if requested (the default), the model frame used. 68.5 summary() &gt; summary(myfit) Call: lm(formula = weight ~ height, data = htwt) Residuals: Min 1Q Median 3Q Max -19.658 -5.381 -0.555 4.807 42.894 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -130.91040 11.52792 -11.36 &lt;2e-16 *** height 1.15009 0.06749 17.04 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 8.505 on 198 degrees of freedom Multiple R-squared: 0.5946, Adjusted R-squared: 0.5925 F-statistic: 290.4 on 1 and 198 DF, p-value: &lt; 2.2e-16 68.6 summary() List Elements &gt; mysummary &lt;- summary(myfit) &gt; names(mysummary) [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; 68.7 Using tidy() &gt; library(broom) &gt; tidy(myfit) # A tibble: 2 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -131. 11.5 -11.4 2.44e-23 2 height 1.15 0.0675 17.0 1.12e-40 68.8 Proportion of Variation Explained The proportion of variance explained by the fitted model is called \\(R^2\\) or \\(r^2\\). It is calculated by: \\[r^2 = \\frac{s^2_{\\hat{y}}}{s^2_{y}}\\] &gt; summary(myfit)$r.squared [1] 0.5945555 &gt; &gt; var(myfit$fitted.values)/var(htwt$weight) [1] 0.5945555 68.9 Assumptions to Verify The assumptions on the above linear model are really about the joint distribution of the residuals, which are not directly observed. On data, we try to verify: The fitted values and the residuals show no trends with respect to each other The residuals are distributed approximately Normal\\((0, \\sigma^2)\\) A constant variance is called homoscedasticity A non-constant variance is called heteroscedascity There are no lurking variables There are two plots we will use in this course to investigate the first two. 68.10 Residual Distribution &gt; plot(myfit, which=1) 68.11 Normal Residuals Check &gt; plot(myfit, which=2) 68.12 Fitted Values Vs. Obs. Residuals "],
["ordinary-least-squares-1.html", "69 Ordinary Least Squares 69.1 OLS Solution 69.2 Sample Variance 69.3 Sample Covariance 69.4 Expected Values 69.5 Standard Error 69.6 Proportion of Variance Explained 69.7 Normal Errors 69.8 Sampling Distribution 69.9 CLT 69.10 Gauss-Markov Theorem", " 69 Ordinary Least Squares Ordinary least squares (OLS) estimates the model \\[ \\begin{aligned} Y_i &amp; = \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + E_i \\\\ &amp; = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}+ E_i \\end{aligned} \\] where \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). Note that typically \\(X_{i1} = 1\\) for all \\(i\\) so that \\(\\beta_1 X_{i1} = \\beta_1\\) serves as the intercept. 69.1 OLS Solution The estimates of \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are found by identifying the values that minimize: \\[ \\begin{aligned} \\sum_{i=1}^n \\left[ Y_i - (\\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip}) \\right]^2 \\\\ = ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}) \\end{aligned} \\] The solution is expressed in terms of matrix algebra computations: \\[ \\hat{{\\boldsymbol{\\beta}}} = ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{Y}}. \\] 69.2 Sample Variance Let the predicted values of the model be \\[ \\hat{{\\boldsymbol{Y}}} = {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}} = {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{Y}}. \\] We estimate \\(\\sigma^2\\) by the OLS sample variance \\[ S^2 = \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{n-p}. \\] 69.3 Sample Covariance The \\(p\\)-vector \\(\\hat{{\\boldsymbol{\\beta}}}\\) has covariance matrix \\[ {\\operatorname{Cov}}(\\hat{{\\boldsymbol{\\beta}}} | {\\boldsymbol{X}}) = ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2. \\] Its estimated covariance matrix is \\[ \\widehat{{\\operatorname{Cov}}}(\\hat{{\\boldsymbol{\\beta}}}) = ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} S^2. \\] 69.4 Expected Values Under the assumption that \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\), we have the following: \\[ {\\operatorname{E}}\\left[ \\left. \\hat{{\\boldsymbol{\\beta}}} \\right| {\\boldsymbol{X}}\\right] = {\\boldsymbol{\\beta}}\\] \\[ {\\operatorname{E}}\\left[ \\left. S^2 \\right| {\\boldsymbol{X}}\\right] = \\sigma^2 \\] \\[ {\\operatorname{E}}\\left[\\left. ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} S^2 \\right| {\\boldsymbol{X}}\\right] = {\\operatorname{Cov}}\\left(\\hat{{\\boldsymbol{\\beta}}}\\right) \\] \\[ {\\operatorname{Cov}}\\left(\\hat{\\beta}_j, Y_i - \\hat{Y}_i\\right) = \\boldsymbol{0}. \\] 69.5 Standard Error The standard error of \\(\\hat{\\beta}_j\\) is the square root of the \\((j, j)\\) diagonal entry of \\(({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2\\) \\[ {\\operatorname{se}}(\\hat{\\beta}_j) = \\sqrt{\\left[({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2\\right]_{jj}} \\] and estimated standard error is \\[ \\hat{{\\operatorname{se}}}(\\hat{\\beta}_j) = \\sqrt{\\left[({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} S^2\\right]_{jj}} \\] 69.6 Proportion of Variance Explained The proportion of variance explained is defined equivalently to the simple linear regression scneario: \\[ R^2 = \\frac{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}. \\] 69.7 Normal Errors Suppose we assume \\(E_1, E_2, \\ldots, E_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(0, \\sigma^2)\\). Then \\[ \\ell\\left({\\boldsymbol{\\beta}}, \\sigma^2 ; {\\boldsymbol{Y}}, {\\boldsymbol{X}}\\right) \\propto -n\\log(\\sigma^2) -\\frac{1}{\\sigma^2} ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}). \\] Since minimizing \\(({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})\\) maximizes the likelihood with respect to \\({\\boldsymbol{\\beta}}\\), this implies \\(\\hat{{\\boldsymbol{\\beta}}}\\) is the MLE for \\({\\boldsymbol{\\beta}}\\). It can also be calculated that \\(\\frac{n-p}{n} S^2\\) is the MLE for \\(\\sigma^2\\). 69.8 Sampling Distribution When \\(E_1, E_2, \\ldots, E_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(0, \\sigma^2)\\), it follows that, conditional on \\({\\boldsymbol{X}}\\): \\[ \\hat{{\\boldsymbol{\\beta}}} \\sim \\mbox{MVN}_p\\left({\\boldsymbol{\\beta}}, ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2 \\right) \\] \\[ \\begin{aligned} S^2 \\frac{n-p}{\\sigma^2} &amp; \\sim \\chi^2_{n-p} \\\\ \\frac{\\hat{\\beta}_j - \\beta_j}{\\hat{{\\operatorname{se}}}(\\hat{\\beta}_j)} &amp; \\sim t_{n-p} \\end{aligned} \\] 69.9 CLT Under the assumption that \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for \\(i \\not= j\\), it follows that as \\(n \\rightarrow \\infty\\), \\[ \\sqrt{n} \\left(\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}}\\right) \\stackrel{D}{\\longrightarrow} \\mbox{MVN}_p\\left( \\boldsymbol{0}, ({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} \\sigma^2 \\right). \\] 69.10 Gauss-Markov Theorem Under the assumption that \\({\\rm E}[E_i] = 0\\), \\({\\rm Var}(E_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j) = 0\\) for \\(i \\not= j\\), the Gauss-Markov theorem shows that among all BLUEs, best linear unbiased estimators, the least squares estimate has the smallest mean-squared error. Specifically, suppose that \\(\\tilde{{\\boldsymbol{\\beta}}}\\) is a linear estimator (calculated from a linear operator on \\({\\boldsymbol{Y}}\\)) where \\({\\operatorname{E}}[\\tilde{{\\boldsymbol{\\beta}}} | {\\boldsymbol{X}}] = {\\boldsymbol{\\beta}}\\). Then \\[ {\\operatorname{E}}\\left[ \\left. ({\\boldsymbol{Y}}- {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}) \\right| {\\boldsymbol{X}}\\right] \\leq {\\operatorname{E}}\\left[ \\left. ({\\boldsymbol{Y}}- {\\boldsymbol{X}}\\tilde{{\\boldsymbol{\\beta}}})^T ({\\boldsymbol{Y}}- {\\boldsymbol{X}}\\tilde{{\\boldsymbol{\\beta}}}) \\right| {\\boldsymbol{X}}\\right]. \\] "],
["generalized-least-squares-1.html", "70 Generalized Least Squares 70.1 GLS Solution 70.2 Other Results", " 70 Generalized Least Squares Generalized least squares (GLS) assumes the same model as OLS, except it allows for heteroskedasticity and covariance among the \\(E_i\\). Specifically, it is assumed that \\({\\boldsymbol{E}}= (E_1, \\ldots, E_n)^T\\) is distributed as \\[ {\\boldsymbol{E}}_{n \\times 1} \\sim (\\boldsymbol{0}, {\\boldsymbol{\\Sigma}}) \\] where \\(\\boldsymbol{0}\\) is the expected value \\({\\boldsymbol{\\Sigma}}= (\\sigma_{ij})\\) is the \\(n \\times n\\) covariance matrix. The most straightforward way to navigate GLS results is to recognize that \\[ {\\boldsymbol{\\Sigma}}^{-1/2} {\\boldsymbol{Y}}= {\\boldsymbol{\\Sigma}}^{-1/2}{\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{\\Sigma}}^{-1/2}{\\boldsymbol{E}}\\] satisfies the assumptions of the OLS model. 70.1 GLS Solution The solution to minimizing \\[ ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}})^T {\\boldsymbol{\\Sigma}}^{-1} ({\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}) \\] is \\[ \\hat{{\\boldsymbol{\\beta}}} = \\left( {\\boldsymbol{X}}^T {\\boldsymbol{\\Sigma}}^{-1} {\\boldsymbol{X}}\\right)^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{\\Sigma}}^{-1} {\\boldsymbol{Y}}. \\] 70.2 Other Results The issue of estimating \\({\\boldsymbol{\\Sigma}}\\) if it is unknown is complicated. Other than estimates of \\(\\sigma^2\\), the results from the OLS section recapitulate by replacing \\({\\boldsymbol{Y}}= {\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{E}}\\) with \\[ {\\boldsymbol{\\Sigma}}^{-1/2} {\\boldsymbol{Y}}= {\\boldsymbol{\\Sigma}}^{-1/2}{\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{\\Sigma}}^{-1/2}{\\boldsymbol{E}}. \\] For example, as \\(n \\rightarrow \\infty\\), \\[ \\sqrt{n} \\left(\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}}\\right) \\stackrel{D}{\\longrightarrow} \\mbox{MNV}_p\\left( \\boldsymbol{0}, ({\\boldsymbol{X}}^T {\\boldsymbol{\\Sigma}}^{-1} {\\boldsymbol{X}})^{-1} \\right). \\] We also still have that \\[ {\\operatorname{E}}\\left[ \\left. \\hat{{\\boldsymbol{\\beta}}} \\right| {\\boldsymbol{X}}\\right] = {\\boldsymbol{\\beta}}. \\] And when \\({\\boldsymbol{E}}\\sim \\mbox{MVN}_n(\\boldsymbol{0}, {\\boldsymbol{\\Sigma}})\\), \\(\\hat{{\\boldsymbol{\\beta}}}\\) is the MLE. "],
["ols-in-r.html", "71 OLS in R 71.1 Weight Regressed on Height + Sex 71.2 One Variable, Two Scales 71.3 Interactions 71.4 More on Interactions 71.5 Visualizing Three Different Models", " 71 OLS in R R implements OLS of multiple explanatory variables exactly the same as with a single explanatory variable, except we need to show the sum of all explanatory variables that we want to use. &gt; lm(weight ~ height + sex, data=htwt) Call: lm(formula = weight ~ height + sex, data = htwt) Coefficients: (Intercept) height sexM -76.6167 0.8106 8.2269 71.1 Weight Regressed on Height + Sex &gt; summary(lm(weight ~ height + sex, data=htwt)) Call: lm(formula = weight ~ height + sex, data = htwt) Residuals: Min 1Q Median 3Q Max -20.131 -4.884 -0.640 5.160 41.490 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -76.6167 15.7150 -4.875 2.23e-06 *** height 0.8105 0.0953 8.506 4.50e-15 *** sexM 8.2269 1.7105 4.810 3.00e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 8.066 on 197 degrees of freedom Multiple R-squared: 0.6372, Adjusted R-squared: 0.6335 F-statistic: 173 on 2 and 197 DF, p-value: &lt; 2.2e-16 71.2 One Variable, Two Scales We can include a single variable but on two different scales: &gt; htwt &lt;- htwt %&gt;% mutate(height2 = height^2) &gt; summary(lm(weight ~ height + height2, data=htwt)) Call: lm(formula = weight ~ height + height2, data = htwt) Residuals: Min 1Q Median 3Q Max -24.265 -5.159 -0.499 4.549 42.965 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 107.117140 175.246872 0.611 0.542 height -1.632719 2.045524 -0.798 0.426 height2 0.008111 0.005959 1.361 0.175 Residual standard error: 8.486 on 197 degrees of freedom Multiple R-squared: 0.5983, Adjusted R-squared: 0.5943 F-statistic: 146.7 on 2 and 197 DF, p-value: &lt; 2.2e-16 71.3 Interactions It is possible to include products of explanatory variables, which is called an interaction. &gt; summary(lm(weight ~ height + sex + height:sex, data=htwt)) Call: lm(formula = weight ~ height + sex + height:sex, data = htwt) Residuals: Min 1Q Median 3Q Max -20.869 -4.835 -0.897 4.429 41.122 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -45.6730 22.1342 -2.063 0.0404 * height 0.6227 0.1343 4.637 6.46e-06 *** sexM -55.6571 32.4597 -1.715 0.0880 . height:sexM 0.3729 0.1892 1.971 0.0502 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 8.007 on 196 degrees of freedom Multiple R-squared: 0.6442, Adjusted R-squared: 0.6388 F-statistic: 118.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 71.4 More on Interactions What happens when there is an interaction between a quantitative explanatory variable and a factor explanatory variable? In the next plot, we show three models: Grey solid: lm(weight ~ height, data=htwt) Color dashed: lm(weight ~ height + sex, data=htwt) Color solid: lm(weight ~ height + sex + height:sex, data=htwt) 71.5 Visualizing Three Different Models "],
["categorical-explanatory-variables.html", "72 Categorical Explanatory Variables 72.1 Example: Chicken Weights 72.2 Factor Variables in lm() 72.3 Plot the Fit 72.4 ANOVA (Version 1) 72.5 anova() 72.6 How It Works 72.7 Top of Design Matrix 72.8 Bottom of Design Matrix 72.9 Model Fits", " 72 Categorical Explanatory Variables 72.1 Example: Chicken Weights &gt; data(&quot;chickwts&quot;, package=&quot;datasets&quot;) &gt; head(chickwts) weight feed 1 179 horsebean 2 160 horsebean 3 136 horsebean 4 227 horsebean 5 217 horsebean 6 168 horsebean &gt; summary(chickwts$feed) casein horsebean linseed meatmeal soybean sunflower 12 10 12 11 14 12 72.2 Factor Variables in lm() &gt; chick_fit &lt;- lm(weight ~ feed, data=chickwts) &gt; summary(chick_fit) Call: lm(formula = weight ~ feed, data = chickwts) Residuals: Min 1Q Median 3Q Max -123.909 -34.413 1.571 38.170 103.091 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 323.583 15.834 20.436 &lt; 2e-16 *** feedhorsebean -163.383 23.485 -6.957 2.07e-09 *** feedlinseed -104.833 22.393 -4.682 1.49e-05 *** feedmeatmeal -46.674 22.896 -2.039 0.045567 * feedsoybean -77.155 21.578 -3.576 0.000665 *** feedsunflower 5.333 22.393 0.238 0.812495 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 54.85 on 65 degrees of freedom Multiple R-squared: 0.5417, Adjusted R-squared: 0.5064 F-statistic: 15.36 on 5 and 65 DF, p-value: 5.936e-10 72.3 Plot the Fit &gt; plot(chickwts$feed, chickwts$weight, xlab=&quot;Feed&quot;, ylab=&quot;Weight&quot;, las=2) &gt; points(chickwts$feed, chick_fit$fitted.values, col=&quot;blue&quot;, pch=20, cex=2) 72.4 ANOVA (Version 1) ANOVA (analysis of variance) was originally developed as a statistical model and method for comparing differences in mean values between various groups. ANOVA quantifies and tests for differences in response variables with respect to factor variables. In doing so, it also partitions the total variance to that due to within and between groups, where groups are defined by the factor variables. 72.5 anova() The classic ANOVA table: &gt; anova(chick_fit) Analysis of Variance Table Response: weight Df Sum Sq Mean Sq F value Pr(&gt;F) feed 5 231129 46226 15.365 5.936e-10 *** Residuals 65 195556 3009 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; n &lt;- length(chick_fit$residuals) # n &lt;- 71 &gt; (n-1)*var(chick_fit$fitted.values) [1] 231129.2 &gt; (n-1)*var(chick_fit$residuals) [1] 195556 &gt; (n-1)*var(chickwts$weight) # sum of above two quantities [1] 426685.2 &gt; (231129/5)/(195556/65) # F-statistic [1] 15.36479 72.6 How It Works &gt; levels(chickwts$feed) [1] &quot;casein&quot; &quot;horsebean&quot; &quot;linseed&quot; &quot;meatmeal&quot; &quot;soybean&quot; &quot;sunflower&quot; &gt; head(chickwts, n=3) weight feed 1 179 horsebean 2 160 horsebean 3 136 horsebean &gt; tail(chickwts, n=3) weight feed 69 222 casein 70 283 casein 71 332 casein &gt; x &lt;- model.matrix(weight ~ feed, data=chickwts) &gt; dim(x) [1] 71 6 72.7 Top of Design Matrix &gt; head(x) (Intercept) feedhorsebean feedlinseed feedmeatmeal feedsoybean 1 1 1 0 0 0 2 1 1 0 0 0 3 1 1 0 0 0 4 1 1 0 0 0 5 1 1 0 0 0 6 1 1 0 0 0 feedsunflower 1 0 2 0 3 0 4 0 5 0 6 0 72.8 Bottom of Design Matrix &gt; tail(x) (Intercept) feedhorsebean feedlinseed feedmeatmeal feedsoybean 66 1 0 0 0 0 67 1 0 0 0 0 68 1 0 0 0 0 69 1 0 0 0 0 70 1 0 0 0 0 71 1 0 0 0 0 feedsunflower 66 0 67 0 68 0 69 0 70 0 71 0 72.9 Model Fits &gt; chick_fit$fitted.values %&gt;% round(digits=4) %&gt;% unique() [1] 160.2000 218.7500 246.4286 328.9167 276.9091 323.5833 &gt; chickwts %&gt;% group_by(feed) %&gt;% summarize(mean(weight)) # A tibble: 6 x 2 feed `mean(weight)` &lt;fct&gt; &lt;dbl&gt; 1 casein 324. 2 horsebean 160. 3 linseed 219. 4 meatmeal 277. 5 soybean 246. 6 sunflower 329. "],
["variable-transformations.html", "73 Variable Transformations 73.1 Rationale 73.2 Power and Log Transformations 73.3 Diamonds Data 73.4 Nonlinear Relationship 73.5 Regression with Nonlinear Relationship 73.6 Residual Distribution 73.7 Normal Residuals Check 73.8 Log-Transformation 73.9 OLS on Log-Transformed Data 73.10 Residual Distribution 73.11 Normal Residuals Check 73.12 Tree Pollen Study 73.13 Tree Pollen Count by Week 73.14 A Clever Transformation 73.15 week Transformed", " 73 Variable Transformations 73.1 Rationale In order to obtain reliable model fits and inference on linear models, the model assumptions described earlier must be satisfied. Sometimes it is necessary to transform the response variable and/or some of the explanatory variables. This process should involve data visualization and exploration. 73.2 Power and Log Transformations It is often useful to explore power and log transforms of the variables, e.g., \\(\\log(y)\\) or \\(y^\\lambda\\) for some \\(\\lambda\\) (and likewise \\(\\log(x)\\) or \\(x^\\lambda\\)). You can read more about the Box-Cox family of power transformations. 73.3 Diamonds Data &gt; data(&quot;diamonds&quot;, package=&quot;ggplot2&quot;) &gt; head(diamonds) # A tibble: 6 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 73.4 Nonlinear Relationship &gt; ggplot(data = diamonds) + + geom_point(mapping=aes(x=carat, y=price, color=clarity), alpha=0.3) 73.5 Regression with Nonlinear Relationship &gt; diam_fit &lt;- lm(price ~ carat + clarity, data=diamonds) &gt; anova(diam_fit) Analysis of Variance Table Response: price Df Sum Sq Mean Sq F value Pr(&gt;F) carat 1 7.2913e+11 7.2913e+11 435639.9 &lt; 2.2e-16 *** clarity 7 3.9082e+10 5.5831e+09 3335.8 &lt; 2.2e-16 *** Residuals 53931 9.0264e+10 1.6737e+06 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 73.6 Residual Distribution &gt; plot(diam_fit, which=1) 73.7 Normal Residuals Check &gt; plot(diam_fit, which=2) 73.8 Log-Transformation &gt; ggplot(data = diamonds) + + geom_point(aes(x=carat, y=price, color=clarity), alpha=0.3) + + scale_y_log10(breaks=c(1000,5000,10000)) + + scale_x_log10(breaks=1:5) 73.9 OLS on Log-Transformed Data &gt; diamonds &lt;- mutate(diamonds, log_price = log(price, base=10), + log_carat = log(carat, base=10)) &gt; ldiam_fit &lt;- lm(log_price ~ log_carat + clarity, data=diamonds) &gt; anova(ldiam_fit) Analysis of Variance Table Response: log_price Df Sum Sq Mean Sq F value Pr(&gt;F) log_carat 1 9771.9 9771.9 1452922.6 &lt; 2.2e-16 *** clarity 7 339.1 48.4 7203.3 &lt; 2.2e-16 *** Residuals 53931 362.7 0.0 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 73.10 Residual Distribution &gt; plot(ldiam_fit, which=1) 73.11 Normal Residuals Check &gt; plot(ldiam_fit, which=2) 73.12 Tree Pollen Study Suppose that we have a study where tree pollen measurements are averaged every week, and these data are recorded for 10 years. These data are simulated: &gt; pollen_study # A tibble: 520 x 3 week year pollen &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 2001 1842. 2 2 2001 1966. 3 3 2001 2381. 4 4 2001 2141. 5 5 2001 2210. 6 6 2001 2585. 7 7 2001 2392. 8 8 2001 2105. 9 9 2001 2278. 10 10 2001 2384. # … with 510 more rows 73.13 Tree Pollen Count by Week &gt; ggplot(pollen_study) + geom_point(aes(x=week, y=pollen)) 73.14 A Clever Transformation We can see there is a linear relationship between pollen and week if we transform week to be number of weeks from the peak week. &gt; pollen_study &lt;- pollen_study %&gt;% + mutate(week_new = abs(week-20)) Note that this is a very different transformation from taking a log or power transformation. 73.15 week Transformed &gt; ggplot(pollen_study) + geom_point(aes(x=week_new, y=pollen)) "],
["ols-goodness-of-fit.html", "74 OLS Goodness of Fit 74.1 Pythagorean Theorem 74.2 OLS Normal Model 74.3 Projection Matrices 74.4 Decomposition 74.5 Distribution of Projection 74.6 Distribution of Residuals 74.7 Degrees of Freedom 74.8 Submodels 74.9 Hypothesis Testing 74.10 Generalized LRT 74.11 Nested Projections 74.12 F Statistic 74.13 F Distribution 74.14 F Test 74.15 Example: Davis Data 74.16 Comparing Linear Models in R 74.17 ANOVA (Version 2) 74.18 Comparing Two Models with anova() 74.19 When There’s a Single Variable Difference 74.20 Calculating the F-statistic 74.21 Calculating the Generalized LRT 74.22 ANOVA on More Distant Models 74.23 Compare Multiple Models at Once", " 74 OLS Goodness of Fit 74.1 Pythagorean Theorem PythMod Least squares model fitting can be understood through the Pythagorean theorem: \\(a^2 + b^2 = c^2\\). However, here we have: \\[ \\sum_{i=1}^n Y_i^2 = \\sum_{i=1}^n \\hat{Y}_i^2 + \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\] where the \\(\\hat{Y}_i\\) are the result of a linear projection of the \\(Y_i\\). 74.2 OLS Normal Model In this section, let’s assume that \\(({\\boldsymbol{X}}_1, Y_1), \\ldots, ({\\boldsymbol{X}}_n, Y_n)\\) are distributed so that \\[ \\begin{aligned} Y_i &amp; = \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + E_i \\\\ &amp; = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}+ E_i \\end{aligned} \\] where \\({\\boldsymbol{E}}| {\\boldsymbol{X}}\\sim \\mbox{MVN}_n({\\boldsymbol{0}}, \\sigma^2 {\\boldsymbol{I}})\\). Note that we haven’t specified the distribution of the \\({\\boldsymbol{X}}_i\\) rv’s. 74.3 Projection Matrices In the OLS framework we have: \\[ \\hat{{\\boldsymbol{Y}}} = {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{Y}}. \\] The matrix \\({\\boldsymbol{P}}_{n \\times n} = {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T\\) is a projection matrix. The vector \\({\\boldsymbol{Y}}\\) is projected into the space spanned by the column space of \\({\\boldsymbol{X}}\\). Project matrices have the following properties: \\({\\boldsymbol{P}}\\) is symmetric \\({\\boldsymbol{P}}\\) is idempotent so that \\({\\boldsymbol{P}}{\\boldsymbol{P}}= {\\boldsymbol{P}}\\) If \\({\\boldsymbol{X}}\\) has column rank \\(p\\), then \\({\\boldsymbol{P}}\\) has rank \\(p\\) The eigenvalues of \\({\\boldsymbol{P}}\\) are \\(p\\) 1’s and \\(n-p\\) 0’s The trace (sum of diagonal entries) is \\(\\operatorname{tr}({\\boldsymbol{P}}) = p\\) \\({\\boldsymbol{I}}- {\\boldsymbol{P}}\\) is also a projection matrix with rank \\(n-p\\) 74.4 Decomposition Note that \\({\\boldsymbol{P}}({\\boldsymbol{I}}- {\\boldsymbol{P}}) = {\\boldsymbol{P}}- {\\boldsymbol{P}}{\\boldsymbol{P}}= {\\boldsymbol{P}}- {\\boldsymbol{P}}= {\\boldsymbol{0}}\\). We have \\[ \\begin{aligned} \\| {\\boldsymbol{Y}}\\|_{2}^{2} = {\\boldsymbol{Y}}^T {\\boldsymbol{Y}}&amp; = ({\\boldsymbol{P}}{\\boldsymbol{Y}}+ ({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}})^T ({\\boldsymbol{P}}{\\boldsymbol{Y}}+ ({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}}) \\\\ &amp; = ({\\boldsymbol{P}}{\\boldsymbol{Y}})^T ({\\boldsymbol{P}}{\\boldsymbol{Y}}) + (({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}})^T (({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}}) \\\\ &amp; = \\| {\\boldsymbol{P}}{\\boldsymbol{Y}}\\|_{2}^{2} + \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}) {\\boldsymbol{Y}}\\|_{2}^{2} \\end{aligned} \\] where the cross terms disappear because \\({\\boldsymbol{P}}({\\boldsymbol{I}}- {\\boldsymbol{P}}) = {\\boldsymbol{0}}\\). Note: The \\(\\ell_p\\) norm of an \\(n\\)-vector \\(\\boldsymbol{w}\\) is defined as \\[ \\| \\boldsymbol{w} \\|_p = \\left(\\sum_{i=1}^n |w_i|^p\\right)^{1/p}. \\] Above we calculated \\[ \\| \\boldsymbol{w} \\|_2^2 = \\sum_{i=1}^n w_i^2. \\] 74.5 Distribution of Projection Suppose that \\(Y_1, Y_2, \\ldots, Y_n {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(0,\\sigma^2)\\). This can also be written as \\({\\boldsymbol{Y}}\\sim \\mbox{MVN}_n({\\boldsymbol{0}}, \\sigma^2 {\\boldsymbol{I}})\\). It follows that \\[ {\\boldsymbol{P}}{\\boldsymbol{Y}}\\sim \\mbox{MVN}_{n}({\\boldsymbol{0}}, \\sigma^2 {\\boldsymbol{P}}{\\boldsymbol{I}}{\\boldsymbol{P}}^T). \\] where \\({\\boldsymbol{P}}{\\boldsymbol{I}}{\\boldsymbol{P}}^T = {\\boldsymbol{P}}{\\boldsymbol{P}}^T = {\\boldsymbol{P}}{\\boldsymbol{P}}= {\\boldsymbol{P}}\\). Also, \\(({\\boldsymbol{P}}{\\boldsymbol{Y}})^T ({\\boldsymbol{P}}{\\boldsymbol{Y}}) = {\\boldsymbol{Y}}^T {\\boldsymbol{P}}^T {\\boldsymbol{P}}{\\boldsymbol{Y}}= {\\boldsymbol{Y}}^T {\\boldsymbol{P}}{\\boldsymbol{Y}}\\), a quadratic form. Given the eigenvalues of \\({\\boldsymbol{P}}\\), \\({\\boldsymbol{Y}}^T {\\boldsymbol{P}}{\\boldsymbol{Y}}\\) is equivalent in distribution to \\(p\\) squared iid Normal(0,1) rv’s, so \\[ \\frac{{\\boldsymbol{Y}}^T {\\boldsymbol{P}}{\\boldsymbol{Y}}}{\\sigma^2} \\sim \\chi^2_{p}. \\] 74.6 Distribution of Residuals If \\({\\boldsymbol{P}}{\\boldsymbol{Y}}= \\hat{{\\boldsymbol{Y}}}\\) are the fitted OLS values, then \\(({\\boldsymbol{I}}-{\\boldsymbol{P}}) {\\boldsymbol{Y}}= {\\boldsymbol{Y}}- \\hat{{\\boldsymbol{Y}}}\\) are the residuals. It follows by the same argument as above that \\[ \\frac{{\\boldsymbol{Y}}^T ({\\boldsymbol{I}}-{\\boldsymbol{P}}) {\\boldsymbol{Y}}}{\\sigma^2} \\sim \\chi^2_{n-p}. \\] It’s also straightforward to show that \\(({\\boldsymbol{I}}-{\\boldsymbol{P}}){\\boldsymbol{Y}}\\sim \\mbox{MVN}_{n}({\\boldsymbol{0}}, \\sigma^2({\\boldsymbol{I}}-{\\boldsymbol{P}}))\\) and \\({\\operatorname{Cov}}({\\boldsymbol{P}}{\\boldsymbol{Y}}, ({\\boldsymbol{I}}-{\\boldsymbol{P}}){\\boldsymbol{Y}}) = {\\boldsymbol{0}}\\). 74.7 Degrees of Freedom The degrees of freedom, \\(p\\), of a linear projection model fit is equal to The number of linearly dependent columns of \\({\\boldsymbol{X}}\\) The number of nonzero eigenvalues of \\({\\boldsymbol{P}}\\) (where nonzero eigenvalues are equal to 1) The trace of the projection matrix, \\(\\operatorname{tr}({\\boldsymbol{P}})\\). The reason why we divide estimates of variance by \\(n-p\\) is because this is the number of effective independent sources of variation remaining after the model is fit by projecting the \\(n\\) observations into a \\(p\\) dimensional linear space. 74.8 Submodels Consider the OLS model \\({\\boldsymbol{Y}}= {\\boldsymbol{X}}{\\boldsymbol{\\beta}}+ {\\boldsymbol{E}}\\) where there are \\(p\\) columns of \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{\\beta}}\\) is a \\(p\\)-vector. Let \\({\\boldsymbol{X}}_0\\) be a subset of \\(p_0\\) columns of \\({\\boldsymbol{X}}\\) and let \\({\\boldsymbol{X}}_1\\) be a subset of \\(p_1\\) columns, where \\(1 \\leq p_0 &lt; p_1 \\leq p\\). Also, assume that the columns of \\({\\boldsymbol{X}}_0\\) are a subset of \\({\\boldsymbol{X}}_1\\). We can form \\(\\hat{{\\boldsymbol{Y}}}_0 = {\\boldsymbol{P}}_0 {\\boldsymbol{Y}}\\) where \\({\\boldsymbol{P}}_0\\) is the projection matrix built from \\({\\boldsymbol{X}}_0\\). We can analogously form \\(\\hat{{\\boldsymbol{Y}}}_1 = {\\boldsymbol{P}}_1 {\\boldsymbol{Y}}\\). 74.9 Hypothesis Testing Without loss of generality, suppose that \\({\\boldsymbol{\\beta}}_0 = (\\beta_1, \\beta_2, \\ldots, \\beta_{p_0})^T\\) and \\({\\boldsymbol{\\beta}}_1 = (\\beta_1, \\beta_2, \\ldots, \\beta_{p_1})^T\\). How do we compare these models, specifically to test \\(H_0: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) = {\\boldsymbol{0}}\\) vs \\(H_1: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) \\not= {\\boldsymbol{0}}\\)? The basic idea to perform this test is to compare the goodness of fits of each model via a pivotal statistic. We will discuss the generalized LRT and ANOVA approaches. 74.10 Generalized LRT Under the OLS Normal model, it follows that \\(\\hat{{\\boldsymbol{\\beta}}}_0 = ({\\boldsymbol{X}}^T_0 {\\boldsymbol{X}}_0)^{-1} {\\boldsymbol{X}}_0^T {\\boldsymbol{Y}}\\) is the MLE under the null hypothesis and \\(\\hat{{\\boldsymbol{\\beta}}}_1 = ({\\boldsymbol{X}}^T_1 {\\boldsymbol{X}}_1)^{-1} {\\boldsymbol{X}}_1^T {\\boldsymbol{Y}}\\) is the unconstrained MLE. Also, the respective MLEs of \\(\\sigma^2\\) are \\[ \\hat{\\sigma}^2_0 = \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_{0,i})^2}{n} \\] \\[ \\hat{\\sigma}^2_1 = \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_{1,i})^2}{n} \\] where \\(\\hat{{\\boldsymbol{Y}}}_{0} = {\\boldsymbol{X}}_0 \\hat{{\\boldsymbol{\\beta}}}_0\\) and \\(\\hat{{\\boldsymbol{Y}}}_{1} = {\\boldsymbol{X}}_1 \\hat{{\\boldsymbol{\\beta}}}_1\\). The generalized LRT statistic is \\[ \\lambda({\\boldsymbol{X}}, {\\boldsymbol{Y}}) = \\frac{L\\left(\\hat{{\\boldsymbol{\\beta}}}_1, \\hat{\\sigma}^2_1; {\\boldsymbol{X}}, {\\boldsymbol{Y}}\\right)}{L\\left(\\hat{{\\boldsymbol{\\beta}}}_0, \\hat{\\sigma}^2_0; {\\boldsymbol{X}}, {\\boldsymbol{Y}}\\right)} \\] where \\(2\\log\\lambda({\\boldsymbol{X}}, {\\boldsymbol{Y}})\\) has a \\(\\chi^2_{p_1 - p_0}\\) null distribution. 74.11 Nested Projections We can apply the Pythagorean theorem we saw earlier to linear subspaces to get: \\[ \\begin{aligned} \\| {\\boldsymbol{Y}}\\|^2_2 &amp; = \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|_{2}^{2} + \\| {\\boldsymbol{P}}_1 {\\boldsymbol{Y}}\\|_{2}^{2} \\\\ &amp; = \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|_{2}^{2} + \\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|_{2}^{2} + \\| {\\boldsymbol{P}}_0 {\\boldsymbol{Y}}\\|_{2}^{2} \\end{aligned} \\] We can also use the Pythagorean theorem to decompose the residuals from the smaller projection \\({\\boldsymbol{P}}_0\\): \\[ \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 = \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2 + \\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 \\] 74.12 F Statistic The \\(F\\) statistic compares the improvement of goodness in fit of the larger model to that of the smaller model in terms of sums of squared residuals, and it scales this improvement by an estimate of \\(\\sigma^2\\): \\[ \\begin{aligned} F &amp; = \\frac{\\left[\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 - \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2\\right]/(p_1 - p_0)}{\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2/(n-p_1)} \\\\ &amp; = \\frac{\\left[\\sum_{i=1}^n (Y_i - \\hat{Y}_{0,i})^2 - \\sum_{i=1}^n (Y_i - \\hat{Y}_{1,i})^2 \\right]/(p_1 - p_0)}{\\sum_{i=1}^n (Y_i - \\hat{Y}_{1,i})^2 / (n - p_1)} \\\\ \\end{aligned} \\] Since \\(\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 - \\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2 = \\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2\\), we can equivalently write the \\(F\\) statistic as: \\[ \\begin{aligned} F &amp; = \\frac{\\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2 / (p_1 - p_0)}{\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2/(n-p_1)} \\\\ &amp; = \\frac{\\sum_{i=1}^n (\\hat{Y}_{1,i} - \\hat{Y}_{0,i})^2 / (p_1 - p_0)}{\\sum_{i=1}^n (Y_i - \\hat{Y}_{1,i})^2 / (n - p_1)} \\end{aligned} \\] 74.13 F Distribution Suppose we have independent random variables \\(V \\sim \\chi^2_a\\) and \\(W \\sim \\chi^2_b\\). It follows that \\[ \\frac{V/a}{W/b} \\sim F_{a,b} \\] where \\(F_{a,b}\\) is the \\(F\\) distribution with \\((a, b)\\) degrees of freedom. By arguments similar to those given above, we have \\[ \\frac{\\| ({\\boldsymbol{P}}_1 - {\\boldsymbol{P}}_0) {\\boldsymbol{Y}}\\|^2_2}{\\sigma^2} \\sim \\chi^2_{p_1 - p_0} \\] \\[ \\frac{\\| ({\\boldsymbol{I}}- {\\boldsymbol{P}}_1) {\\boldsymbol{Y}}\\|^2_2}{\\sigma^2} \\sim \\chi^2_{n-p_1} \\] and these two rv’s are independent. 74.14 F Test Suppose that the OLS model holds where \\({\\boldsymbol{E}}| {\\boldsymbol{X}}\\sim \\mbox{MVN}_n({\\boldsymbol{0}}, \\sigma^2 {\\boldsymbol{I}})\\). In order to test \\(H_0: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) = {\\boldsymbol{0}}\\) vs \\(H_1: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) \\not= {\\boldsymbol{0}}\\), we can form the \\(F\\) statistic as given above, which has null distribution \\(F_{p_1 - p_0, n - p_1}\\). The p-value is calculated as \\(\\Pr(F^* \\geq F)\\) where \\(F\\) is the observed \\(F\\) statistic and \\(F^* \\sim F_{p_1 - p_0, n - p_1}\\). If the above assumption on the distribution of \\({\\boldsymbol{E}}| {\\boldsymbol{X}}\\) only approximately holds, then the \\(F\\) test p-value is also an approximation. 74.15 Example: Davis Data &gt; library(&quot;car&quot;) &gt; data(&quot;Davis&quot;, package=&quot;car&quot;) Warning in data(&quot;Davis&quot;, package = &quot;car&quot;): data set &#39;Davis&#39; not found &gt; htwt &lt;- tbl_df(Davis) &gt; htwt[12,c(2,3)] &lt;- htwt[12,c(3,2)] &gt; head(htwt) # A tibble: 6 x 5 sex weight height repwt repht &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 M 77 182 77 180 2 F 58 161 51 159 3 F 53 161 54 158 4 M 68 177 70 175 5 F 59 157 59 155 6 M 76 170 76 165 74.16 Comparing Linear Models in R Example: Davis Data Suppose we are considering the three following models: &gt; f1 &lt;- lm(weight ~ height, data=htwt) &gt; f2 &lt;- lm(weight ~ height + sex, data=htwt) &gt; f3 &lt;- lm(weight ~ height + sex + height:sex, data=htwt) How do we determine if the additional terms in models f2 and f3 are needed? 74.17 ANOVA (Version 2) A generalization of ANOVA exists that allows us to compare two nested models, quantifying their differences in terms of goodness of fit and performing a hypothesis test of whether this difference is statistically significant. A model is nested within another model if their difference is simply the absence of certain terms in the smaller model. The null hypothesis is that the additional terms have coefficients equal to zero, and the alternative hypothesis is that at least one coefficient is nonzero. Both versions of ANOVA can be described in a single, elegant mathematical framework. 74.18 Comparing Two Models with anova() This provides a comparison of the improvement in fit from model f2 compared to model f1: &gt; anova(f1, f2) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 198 14321 2 197 12816 1 1504.9 23.133 2.999e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 74.19 When There’s a Single Variable Difference Compare above anova(f1, f2) p-value to that for the sex term from the f2 model: &gt; library(broom) &gt; tidy(f2) # A tibble: 3 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -76.6 15.7 -4.88 2.23e- 6 2 height 0.811 0.0953 8.51 4.50e-15 3 sexM 8.23 1.71 4.81 3.00e- 6 74.20 Calculating the F-statistic &gt; anova(f1, f2) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 198 14321 2 197 12816 1 1504.9 23.133 2.999e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How the F-statistic is calculated: &gt; n &lt;- nrow(htwt) &gt; ss1 &lt;- (n-1)*var(f1$residuals) &gt; ss1 [1] 14321.11 &gt; ss2 &lt;- (n-1)*var(f2$residuals) &gt; ss2 [1] 12816.18 &gt; ((ss1 - ss2)/anova(f1, f2)$Df[2])/(ss2/f2$df.residual) [1] 23.13253 74.21 Calculating the Generalized LRT &gt; anova(f1, f2, test=&quot;LRT&quot;) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex Res.Df RSS Df Sum of Sq Pr(&gt;Chi) 1 198 14321 2 197 12816 1 1504.9 1.512e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; library(lmtest) &gt; lrtest(f1, f2) Likelihood ratio test Model 1: weight ~ height Model 2: weight ~ height + sex #Df LogLik Df Chisq Pr(&gt;Chisq) 1 3 -710.9 2 4 -699.8 1 22.205 2.45e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 These tests produce slightly different answers because anova() adjusts for degrees of freedom when estimating the variance, whereas lrtest() is the strict generalized LRT. See here. 74.22 ANOVA on More Distant Models We can compare models with multiple differences in terms: &gt; anova(f1, f3) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex + height:sex Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 198 14321 2 196 12567 2 1754 13.678 2.751e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 74.23 Compare Multiple Models at Once We can compare multiple models at once: &gt; anova(f1, f2, f3) Analysis of Variance Table Model 1: weight ~ height Model 2: weight ~ height + sex Model 3: weight ~ height + sex + height:sex Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 198 14321 2 197 12816 1 1504.93 23.4712 2.571e-06 *** 3 196 12567 1 249.04 3.8841 0.05015 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],
["logistic-regression.html", "75 Logistic Regression 75.1 Goal 75.2 Bernoulli as EFD 75.3 Model 75.4 Maximum Likelihood Estimation 75.5 Iteratively Reweighted Least Squares 75.6 GLMs", " 75 Logistic Regression 75.1 Goal Logistic regression models a Bernoulli distributed response variable in terms of linear combinations of explanatory variables. This extends least squares regression to the case where the response variable captures a “success” or “failure” type outcome. 75.2 Bernoulli as EFD If \\(Y \\sim \\mbox{Bernoulli}(p)\\), then its pmf is: \\[ \\begin{aligned} f(y; p) &amp; = p^{y} (1-p)^{1-y} \\\\ &amp; = \\exp\\left\\{ \\log\\left(\\frac{p}{1-p}\\right)y + \\log(1-p) \\right\\} \\end{aligned} \\] In exponential family distribution (EFD) notation, \\[ \\eta(p) = \\log\\left(\\frac{p}{1-p}\\right) \\equiv {\\operatorname{logit}}(p), \\] \\(A(\\eta(p)) = \\log(1 + \\exp(\\eta)) = \\log(1-p)\\), and \\(y\\) is the sufficient statistic. 75.3 Model \\(({\\boldsymbol{X}}_1, Y_1), ({\\boldsymbol{X}}_2, Y_2), \\ldots, ({\\boldsymbol{X}}_n, Y_n)\\) are distributed so that \\(Y_i | {\\boldsymbol{X}}_i \\sim \\mbox{Bernoulli}(p_i)\\), where \\(\\{Y_i | {\\boldsymbol{X}}_i\\}_{i=1}^n\\) are jointly independent and \\[ {\\operatorname{logit}}\\left({\\operatorname{E}}[Y_i | {\\boldsymbol{X}}_i]\\right) = \\log\\left( \\frac{\\Pr(Y_i = 1 | {\\boldsymbol{X}}_i)}{\\Pr(Y_i = 0 | {\\boldsymbol{X}}_i)} \\right) = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}. \\] From this it follows that \\[ p_i = \\frac{\\exp\\left({\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\right)}{1 + \\exp\\left({\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\right)}. \\] 75.4 Maximum Likelihood Estimation The \\({\\boldsymbol{\\beta}}\\) are estimated from the MLE calculated from: \\[ \\begin{aligned} \\ell\\left({\\boldsymbol{\\beta}}; {\\boldsymbol{y}}, {\\boldsymbol{X}}\\right) &amp; = \\sum_{i=1}^n \\log\\left(\\frac{p_i}{1-p_i}\\right) y_i + \\log(1-p_i) \\\\ &amp; = \\sum_{i=1}^n ({\\boldsymbol{x}}_i {\\boldsymbol{\\beta}}) y_i - \\log\\left(1 + \\exp\\left({\\boldsymbol{x}}_i {\\boldsymbol{\\beta}}\\right) \\right) \\end{aligned} \\] 75.5 Iteratively Reweighted Least Squares Initialize \\({\\boldsymbol{\\beta}}^{(1)}\\). For each iteration \\(t=1, 2, \\ldots\\), set \\[ p_i^{(t)} = {\\operatorname{logit}}^{-1}\\left( {\\boldsymbol{x}}_i {\\boldsymbol{\\beta}}^{(t)} \\right), \\ \\ \\ \\ z_i^{(t)} = {\\operatorname{logit}}\\left(p_i^{(t)}\\right) + \\frac{y_i - p_i^{(t)}}{p_i^{(t)}(1-p_i^{(t)})} \\] and let \\({\\boldsymbol{z}}^{(t)} = \\left\\{z_i^{(t)}\\right\\}_{i=1}^n\\). Form \\(n \\times n\\) diagonal matrix \\(\\boldsymbol{W}^{(t)}\\) with \\((i, i)\\) entry equal to \\(p_i^{(t)}(1-p_i^{(t)})\\). Obtain \\({\\boldsymbol{\\beta}}^{(t+1)}\\) by performing the wieghted least squares regression (see GLS from earlier) \\[ {\\boldsymbol{\\beta}}^{(t+1)} = \\left({\\boldsymbol{X}}^T \\boldsymbol{W}^{(t)} {\\boldsymbol{X}}\\right)^{-1} {\\boldsymbol{X}}^T \\boldsymbol{W}^{(t)} {\\boldsymbol{z}}^{(t)}. \\] Iterate Steps 2-4 over \\(t=1, 2, 3, \\ldots\\) until convergence, setting \\(\\hat{{\\boldsymbol{\\beta}}} = {\\boldsymbol{\\beta}}^{(\\infty)}\\). 75.6 GLMs For exponential family distribution response variables, the generalized linear model is \\[ \\eta\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\] where \\(\\eta(\\theta)\\) is function of the expected value \\(\\theta\\) into the natural parameter. This is called the canonical link function in the GLM setting. The iteratively reweighted least squares algorithm presented above for calculating (local) maximum likelihood estimates of \\({\\boldsymbol{\\beta}}\\) has a generalization to a large class of exponential family distribution response vairables. "],
["glm-function-in-r.html", "76 glm() Function in R 76.1 Example: Grad School Admissions 76.2 Explore the Data 76.3 Logistic Regression in R 76.4 Summary of Fit 76.5 ANOVA of Fit 76.6 Example: Contraceptive Use 76.7 A Different Format 76.8 Fitting the Model 76.9 Summary of Fit 76.10 ANOVA of Fit 76.11 More on this Data Set", " 76 glm() Function in R 76.1 Example: Grad School Admissions &gt; mydata &lt;- + read.csv(&quot;https://stats.idre.ucla.edu/stat/data/binary.csv&quot;) &gt; dim(mydata) [1] 400 4 &gt; head(mydata) admit gre gpa rank 1 0 380 3.61 3 2 1 660 3.67 3 3 1 800 4.00 1 4 1 640 3.19 4 5 0 520 2.93 4 6 1 760 3.00 2 Data and analysis courtesy of http://www.ats.ucla.edu/stat/r/dae/logit.htm. 76.2 Explore the Data &gt; apply(mydata, 2, mean) admit gre gpa rank 0.3175 587.7000 3.3899 2.4850 &gt; apply(mydata, 2, sd) admit gre gpa rank 0.4660867 115.5165364 0.3805668 0.9444602 &gt; &gt; table(mydata$admit, mydata$rank) 1 2 3 4 0 28 97 93 55 1 33 54 28 12 &gt; ggplot(data=mydata) + + geom_boxplot(aes(x=as.factor(admit), y=gre)) &gt; ggplot(data=mydata) + + geom_boxplot(aes(x=as.factor(admit), y=gpa)) 76.3 Logistic Regression in R &gt; mydata$rank &lt;- factor(mydata$rank, levels=c(1, 2, 3, 4)) &gt; myfit &lt;- glm(admit ~ gre + gpa + rank, + data = mydata, family = &quot;binomial&quot;) &gt; myfit Call: glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, data = mydata) Coefficients: (Intercept) gre gpa rank2 rank3 -3.989979 0.002264 0.804038 -0.675443 -1.340204 rank4 -1.551464 Degrees of Freedom: 399 Total (i.e. Null); 394 Residual Null Deviance: 500 Residual Deviance: 458.5 AIC: 470.5 76.4 Summary of Fit &gt; summary(myfit) Call: glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, data = mydata) Deviance Residuals: Min 1Q Median 3Q Max -1.6268 -0.8662 -0.6388 1.1490 2.0790 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.989979 1.139951 -3.500 0.000465 *** gre 0.002264 0.001094 2.070 0.038465 * gpa 0.804038 0.331819 2.423 0.015388 * rank2 -0.675443 0.316490 -2.134 0.032829 * rank3 -1.340204 0.345306 -3.881 0.000104 *** rank4 -1.551464 0.417832 -3.713 0.000205 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 458.52 on 394 degrees of freedom AIC: 470.52 Number of Fisher Scoring iterations: 4 76.5 ANOVA of Fit &gt; anova(myfit, test=&quot;Chisq&quot;) Analysis of Deviance Table Model: binomial, link: logit Response: admit Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 399 499.98 gre 1 13.9204 398 486.06 0.0001907 *** gpa 1 5.7122 397 480.34 0.0168478 * rank 3 21.8265 394 458.52 7.088e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; anova(myfit, test=&quot;LRT&quot;) Analysis of Deviance Table Model: binomial, link: logit Response: admit Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 399 499.98 gre 1 13.9204 398 486.06 0.0001907 *** gpa 1 5.7122 397 480.34 0.0168478 * rank 3 21.8265 394 458.52 7.088e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 76.6 Example: Contraceptive Use &gt; cuse &lt;- + read.table(&quot;http://data.princeton.edu/wws509/datasets/cuse.dat&quot;, + header=TRUE) &gt; dim(cuse) [1] 16 5 &gt; head(cuse) age education wantsMore notUsing using 1 &lt;25 low yes 53 6 2 &lt;25 low no 10 4 3 &lt;25 high yes 212 52 4 &lt;25 high no 50 10 5 25-29 low yes 60 14 6 25-29 low no 19 10 Data and analysis courtesy of http://data.princeton.edu/R/glms.html. 76.7 A Different Format Note that in this data set there are multiple observations per explanatory variable configuration. The last two columns of the data frame count the successes and failures per configuration. &gt; head(cuse) age education wantsMore notUsing using 1 &lt;25 low yes 53 6 2 &lt;25 low no 10 4 3 &lt;25 high yes 212 52 4 &lt;25 high no 50 10 5 25-29 low yes 60 14 6 25-29 low no 19 10 76.8 Fitting the Model When this is the case, we call the glm() function slighlty differently. &gt; myfit &lt;- glm(cbind(using, notUsing) ~ age + education + wantsMore, + data=cuse, family = binomial) &gt; myfit Call: glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, family = binomial, data = cuse) Coefficients: (Intercept) age25-29 age30-39 age40-49 educationlow -0.8082 0.3894 0.9086 1.1892 -0.3250 wantsMoreyes -0.8330 Degrees of Freedom: 15 Total (i.e. Null); 10 Residual Null Deviance: 165.8 Residual Deviance: 29.92 AIC: 113.4 76.9 Summary of Fit &gt; summary(myfit) Call: glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, family = binomial, data = cuse) Deviance Residuals: Min 1Q Median 3Q Max -2.5148 -0.9376 0.2408 0.9822 1.7333 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.8082 0.1590 -5.083 3.71e-07 *** age25-29 0.3894 0.1759 2.214 0.02681 * age30-39 0.9086 0.1646 5.519 3.40e-08 *** age40-49 1.1892 0.2144 5.546 2.92e-08 *** educationlow -0.3250 0.1240 -2.620 0.00879 ** wantsMoreyes -0.8330 0.1175 -7.091 1.33e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 165.772 on 15 degrees of freedom Residual deviance: 29.917 on 10 degrees of freedom AIC: 113.43 Number of Fisher Scoring iterations: 4 76.10 ANOVA of Fit &gt; anova(myfit) Analysis of Deviance Table Model: binomial, link: logit Response: cbind(using, notUsing) Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev NULL 15 165.772 age 3 79.192 12 86.581 education 1 6.162 11 80.418 wantsMore 1 50.501 10 29.917 76.11 More on this Data Set See http://data.princeton.edu/R/glms.html for more on fitting logistic regression to this data set. A number of interesting choices are made that reveal more about the data. "],
["generalized-linear-models-1.html", "77 Generalized Linear Models 77.1 Definition 77.2 Exponential Family Distributions 77.3 Natural Single Parameter EFD 77.4 Dispersion EFDs 77.5 Example: Normal 77.6 EFD for GLMs 77.7 Components of a GLM 77.8 Link Functions 77.9 Calculating MLEs 77.10 Newton-Raphson 77.11 Fisher’s scoring 77.12 Iteratively Reweighted Least Squares 77.13 Estimating Dispersion 77.14 CLT Applied to the MLE 77.15 Approximately Pivotal Statistics 77.16 Deviance 77.17 Generalized LRT 77.18 Example: Grad School Admissions 77.19 glm() Function", " 77 Generalized Linear Models 77.1 Definition The generalized linear model (GLM) builds from OLS and GLS to allow for the case where \\(Y | {\\boldsymbol{X}}\\) is distributed according to an exponential family distribution. The estimated model is \\[ g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\] where \\(g(\\cdot)\\) is called the link function. This model is typically fit by numerical methods to calculate the maximum likelihood estimate of \\({\\boldsymbol{\\beta}}\\). 77.2 Exponential Family Distributions Recall that if \\(Y\\) follows an EFD then it has pdf of the form \\[f(y ; \\boldsymbol{\\theta}) = h(y) \\exp \\left\\{ \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(y) - A(\\boldsymbol{\\eta}) \\right\\} \\] where \\(\\boldsymbol{\\theta}\\) is a vector of parameters, \\(\\{T_k(y)\\}\\) are sufficient statistics, \\(A(\\boldsymbol{\\eta})\\) is the cumulant generating function. The functions \\(\\eta_k(\\boldsymbol{\\theta})\\) for \\(k=1, \\ldots, d\\) map the usual parameters \\({\\boldsymbol{\\theta}}\\) (often moments of the rv \\(Y\\)) to the natural parameters or canonical parameters. \\(\\{T_k(y)\\}\\) are sufficient statistics for \\(\\{\\eta_k\\}\\) due to the factorization theorem. \\(A(\\boldsymbol{\\eta})\\) is sometimes called the log normalizer because \\[A(\\boldsymbol{\\eta}) = \\log \\int h(y) \\exp \\left\\{ \\sum_{k=1}^d \\eta_k(\\boldsymbol{\\theta}) T_k(y) \\right\\}.\\] 77.3 Natural Single Parameter EFD A natural single parameter EFD simplifies to the scenario where \\(d=1\\) and \\(T(y) = y\\) \\[f(y ; \\eta) = h(y) \\exp \\left\\{ \\eta(\\theta) y - A(\\eta(\\theta)) \\right\\} \\] where without loss of generality we can write \\({\\operatorname{E}}[Y] = \\theta\\). 77.4 Dispersion EFDs The family of distributions for which GLMs are most typically developed are dispersion EFDs. An example of a dispersion EFD that extends the natural single parameter EFD is \\[f(y ; \\eta) = h(y, \\phi) \\exp \\left\\{ \\frac{\\eta(\\theta) y - A(\\eta(\\theta))}{\\phi} \\right\\} \\] where \\(\\phi\\) is the dispersion parameter. 77.5 Example: Normal Let \\(Y \\sim \\mbox{Normal}(\\mu, \\sigma^2)\\). Then: \\[ \\theta = \\mu, \\eta(\\mu) = \\mu \\] \\[ \\phi = \\sigma^2 \\] \\[ A(\\mu) = \\frac{\\mu^2}{2} \\] \\[ h(y, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2}\\frac{y^2}{\\sigma^2}} \\] 77.6 EFD for GLMs There has been a very broad development of GLMs and extensions. A common setting for introducting GLMs is the dispersion EFD with a general link function \\(g(\\cdot)\\). See the classic text Generalized Linear Models, by McCullagh and Nelder, for such a development. 77.7 Components of a GLM Random: The particular exponential family distribution. \\[ Y \\sim f(y ; \\eta, \\phi) \\] Systematic: The determination of each \\(\\eta_i\\) from \\({\\boldsymbol{X}}_i\\) and \\({\\boldsymbol{\\beta}}\\). \\[ \\eta_i = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\] Parametric Link: The connection between \\(E[Y_i|{\\boldsymbol{X}}_i]\\) and \\({\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\). \\[ g(E[Y_i|{\\boldsymbol{X}}_i]) = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}\\] 77.8 Link Functions Even though the link function \\(g(\\cdot)\\) can be considered in a fairly general framework, the canonical link function \\(\\eta(\\cdot)\\) is often utilized. The canonical link function is the function that maps the expected value into the natural paramater. In this case, \\(Y | {\\boldsymbol{X}}\\) is distributed according to an exponential family distribution with \\[ \\eta \\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}. \\] 77.9 Calculating MLEs Given the model \\(g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}]\\right) = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\), the EFD should be fully parameterized. The Newton-Raphson method or Fisher’s scoring method can be utilized to find the MLE of \\({\\boldsymbol{\\beta}}\\). 77.10 Newton-Raphson Initialize \\({\\boldsymbol{\\beta}}^{(0)}\\). For \\(t = 1, 2, \\ldots\\) Calculate the score \\(s({\\boldsymbol{\\beta}}^{(t)}) = \\nabla \\ell({\\boldsymbol{\\beta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\mid_{{\\boldsymbol{\\beta}}= {\\boldsymbol{\\beta}}^{(t)}}\\) and observed Fisher information \\[H({\\boldsymbol{\\beta}}^{(t)}) = - \\nabla \\nabla^T \\ell({\\boldsymbol{\\beta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\mid_{{\\boldsymbol{\\beta}}= {\\boldsymbol{\\beta}}^{(t)}}\\]. Note that the observed Fisher information is also the negative Hessian matrix. Update \\({\\boldsymbol{\\beta}}^{(t+1)} = {\\boldsymbol{\\beta}}^{(t)} + H({\\boldsymbol{\\beta}}^{(t)})^{-1} s({\\boldsymbol{\\beta}}^{(t)})\\). Iterate until convergence, and set \\(\\hat{{\\boldsymbol{\\beta}}} = {\\boldsymbol{\\beta}}^{(\\infty)}\\). 77.11 Fisher’s scoring Initialize \\({\\boldsymbol{\\beta}}^{(0)}\\). For \\(t = 1, 2, \\ldots\\) Calculate the score \\(s({\\boldsymbol{\\beta}}^{(t)}) = \\nabla \\ell({\\boldsymbol{\\beta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\mid_{{\\boldsymbol{\\beta}}= {\\boldsymbol{\\beta}}^{(t)}}\\) and expected Fisher information \\[I({\\boldsymbol{\\beta}}^{(t)}) = - {\\operatorname{E}}\\left[\\nabla \\nabla^T \\ell({\\boldsymbol{\\beta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\mid_{{\\boldsymbol{\\beta}}= {\\boldsymbol{\\beta}}^{(t)}} \\right].\\] Update \\({\\boldsymbol{\\beta}}^{(t+1)} = {\\boldsymbol{\\beta}}^{(t)} + I({\\boldsymbol{\\beta}}^{(t)})^{-1} s({\\boldsymbol{\\beta}}^{(t)})\\). Iterate until convergence, and set \\(\\hat{{\\boldsymbol{\\beta}}} = {\\boldsymbol{\\beta}}^{(\\infty)}\\). When the canonical link function is used, the Newton-Raphson algorithm and Fisher’s scoring algorithm are equivalent. Exercise: Prove this. 77.12 Iteratively Reweighted Least Squares For the canonical link, Fisher’s scoring method can be written as an iteratively reweighted least squares algorithm, as shown earlier for logistic regression. Note that the Fisher information is \\[ I({\\boldsymbol{\\beta}}^{(t)}) = {\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}}\\] where \\({\\boldsymbol{W}}\\) is an \\(n \\times n\\) diagonal matrix with \\((i, i)\\) entry equal to \\({\\operatorname{Var}}(Y_i | {\\boldsymbol{X}}; {\\boldsymbol{\\beta}}^{(t)})\\). The score function is \\[ s({\\boldsymbol{\\beta}}^{(t)}) = {\\boldsymbol{X}}^T \\left( {\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}^{(t)} \\right) \\] and the current coefficient value \\({\\boldsymbol{\\beta}}^{(t)}\\) can be written as \\[ {\\boldsymbol{\\beta}}^{(t)} = ({\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}}{\\boldsymbol{\\beta}}^{(t)}. \\] Putting this together we get \\[ {\\boldsymbol{\\beta}}^{(t)} + I({\\boldsymbol{\\beta}}^{(t)})^{-1} s({\\boldsymbol{\\beta}}^{(t)}) = ({\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{z}}^{(t)} \\] where \\[ {\\boldsymbol{z}}^{(t)} = {\\boldsymbol{X}}{\\boldsymbol{\\beta}}^{(t)} + {\\boldsymbol{W}}^{-1} \\left( {\\boldsymbol{Y}}- {\\boldsymbol{X}}{\\boldsymbol{\\beta}}^{(t)} \\right). \\] This is a generalization of the iteratively reweighted least squares algorithm we showed earlier for logistic regression. 77.13 Estimating Dispersion For the simple dispersion model above, it is typically straightforward to calculate the MLE \\(\\hat{\\phi}\\) once \\(\\hat{{\\boldsymbol{\\beta}}}\\) has been calculated. 77.14 CLT Applied to the MLE Given that \\(\\hat{{\\boldsymbol{\\beta}}}\\) is a maximum likelihood estimate, we have the following CLT result on its distribution as \\(n \\rightarrow \\infty\\): \\[ \\sqrt{n} (\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}}) \\stackrel{D}{\\longrightarrow} \\mbox{MVN}_{p}({\\boldsymbol{0}}, \\phi ({\\boldsymbol{X}}^T {\\boldsymbol{W}}{\\boldsymbol{X}})^{-1}) \\] 77.15 Approximately Pivotal Statistics The previous CLT gives us the following two approximations for pivtoal statistics. The first statistic facilitates getting overall measures of uncertainty on the estimate \\(\\hat{{\\boldsymbol{\\beta}}}\\). \\[ \\hat{\\phi}^{-1} (\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}})^T ({\\boldsymbol{X}}^T \\hat{{\\boldsymbol{W}}} {\\boldsymbol{X}}) (\\hat{{\\boldsymbol{\\beta}}} - {\\boldsymbol{\\beta}}) {\\; \\stackrel{.}{\\sim}\\;}\\chi^2_1 \\] This second pivotal statistic allows for performing a Wald test or forming a confidence interval on each coefficient, \\(\\beta_j\\), for \\(j=1, \\ldots, p\\). \\[ \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\hat{\\phi} [({\\boldsymbol{X}}^T \\hat{{\\boldsymbol{W}}} {\\boldsymbol{X}})^{-1}]_{jj}}} {\\; \\stackrel{.}{\\sim}\\;}\\mbox{Normal}(0,1) \\] 77.16 Deviance Let \\(\\hat{\\boldsymbol{\\eta}}\\) be the estimated natural parameters from a GLM. For example, \\(\\hat{\\boldsymbol{\\eta}} = {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}\\) when the canonical link function is used. Let \\(\\hat{\\boldsymbol{\\eta}}_n\\) be the saturated model wwhere \\(Y_i\\) is directly used to estimate \\(\\eta_i\\) without model constraints. For example, in the Bernoulli logistic regression model \\(\\hat{\\boldsymbol{\\eta}}_n = {\\boldsymbol{Y}}\\), the observed outcomes. The deviance for the model is defined to be \\[ D\\left(\\hat{\\boldsymbol{\\eta}}\\right) = 2 \\ell(\\hat{\\boldsymbol{\\eta}}_n; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) - 2 \\ell(\\hat{\\boldsymbol{\\eta}}; {\\boldsymbol{X}}, {\\boldsymbol{Y}}) \\] 77.17 Generalized LRT Let \\({\\boldsymbol{X}}_0\\) be a subset of \\(p_0\\) columns of \\({\\boldsymbol{X}}\\) and let \\({\\boldsymbol{X}}_1\\) be a subset of \\(p_1\\) columns, where \\(1 \\leq p_0 &lt; p_1 \\leq p\\). Also, assume that the columns of \\({\\boldsymbol{X}}_0\\) are a subset of \\({\\boldsymbol{X}}_1\\). Without loss of generality, suppose that \\({\\boldsymbol{\\beta}}_0 = (\\beta_1, \\beta_2, \\ldots, \\beta_{p_0})^T\\) and \\({\\boldsymbol{\\beta}}_1 = (\\beta_1, \\beta_2, \\ldots, \\beta_{p_1})^T\\). Suppose we wish to test \\(H_0: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) = {\\boldsymbol{0}}\\) vs \\(H_1: (\\beta_{p_0+1}, \\beta_{p_0 + 2}, \\ldots, \\beta_{p_1}) \\not= {\\boldsymbol{0}}\\) We can form \\(\\hat{\\boldsymbol{\\eta}}_0 = {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}_0\\) from the GLM model \\(g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}_0]\\right) = {\\boldsymbol{X}}_0 {\\boldsymbol{\\beta}}_0\\). We can analogously form \\(\\hat{\\boldsymbol{\\eta}}_1 = {\\boldsymbol{X}}\\hat{{\\boldsymbol{\\beta}}}_1\\) from the GLM model \\(g\\left({\\operatorname{E}}[Y | {\\boldsymbol{X}}_1]\\right) = {\\boldsymbol{X}}_1 {\\boldsymbol{\\beta}}_1\\). The \\(2\\log\\) generalized LRT can then be formed from the two deviance statistics \\[ 2 \\log \\lambda({\\boldsymbol{X}}, {\\boldsymbol{Y}}) = 2 \\log \\frac{L(\\hat{\\boldsymbol{\\eta}}_1; {\\boldsymbol{X}}, {\\boldsymbol{Y}})}{L(\\hat{\\boldsymbol{\\eta}}_0; {\\boldsymbol{X}}, {\\boldsymbol{Y}})} = D\\left(\\hat{\\boldsymbol{\\eta}}_0\\right) - D\\left(\\hat{\\boldsymbol{\\eta}}_1\\right) \\] where the null distribution is \\(\\chi^2_{p_1-p_0}\\). 77.18 Example: Grad School Admissions Let’s revisit a logistic regression example now that we know how the various statistics are calculated. &gt; mydata &lt;- + read.csv(&quot;http://www.ats.ucla.edu/stat/data/binary.csv&quot;) &gt; dim(mydata) &gt; head(mydata) Fit the model with basic output. Note the argument family = &quot;binomial&quot;. &gt; mydata$rank &lt;- factor(mydata$rank, levels=c(1, 2, 3, 4)) &gt; myfit &lt;- glm(admit ~ gre + gpa + rank, + data = mydata, family = &quot;binomial&quot;) &gt; myfit Call: glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, data = mydata) Coefficients: (Intercept) gre gpa rank2 rank3 -3.989979 0.002264 0.804038 -0.675443 -1.340204 rank4 -1.551464 Degrees of Freedom: 399 Total (i.e. Null); 394 Residual Null Deviance: 500 Residual Deviance: 458.5 AIC: 470.5 This shows the fitted coefficient values, which is on the link function scale – logit aka log odds here. Also, a Wald test is performed for each coefficient. &gt; summary(myfit) Call: glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, data = mydata) Deviance Residuals: Min 1Q Median 3Q Max -1.6268 -0.8662 -0.6388 1.1490 2.0790 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.989979 1.139951 -3.500 0.000465 *** gre 0.002264 0.001094 2.070 0.038465 * gpa 0.804038 0.331819 2.423 0.015388 * rank2 -0.675443 0.316490 -2.134 0.032829 * rank3 -1.340204 0.345306 -3.881 0.000104 *** rank4 -1.551464 0.417832 -3.713 0.000205 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 458.52 on 394 degrees of freedom AIC: 470.52 Number of Fisher Scoring iterations: 4 Here we perform a generalized LRT on each variable. Note the rank variable is now tested as a single factor variable as opposed to each dummy variable. &gt; anova(myfit, test=&quot;LRT&quot;) Analysis of Deviance Table Model: binomial, link: logit Response: admit Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 399 499.98 gre 1 13.9204 398 486.06 0.0001907 *** gpa 1 5.7122 397 480.34 0.0168478 * rank 3 21.8265 394 458.52 7.088e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; mydata &lt;- data.frame(mydata, probs = myfit$fitted.values) &gt; ggplot(mydata) + geom_point(aes(x=gpa, y=probs, color=rank)) + + geom_jitter(aes(x=gpa, y=admit), width=0, height=0.01, alpha=0.3) &gt; ggplot(mydata) + geom_point(aes(x=gre, y=probs, color=rank)) + + geom_jitter(aes(x=gre, y=admit), width=0, height=0.01, alpha=0.3) &gt; ggplot(mydata) + geom_boxplot(aes(x=rank, y=probs)) + + geom_jitter(aes(x=rank, y=probs), width=0.1, height=0.01, alpha=0.3) 77.19 glm() Function The glm() function has many different options available to the user. glm(formula, family = gaussian, data, weights, subset, na.action, start = NULL, etastart, mustart, offset, control = list(...), model = TRUE, method = &quot;glm.fit&quot;, x = FALSE, y = TRUE, contrasts = NULL, ...) To see the different link functions available, type: help(family) "],
["nonparametric-regression.html", "78 Nonparametric Regression 78.1 Simple Linear Regression 78.2 Simple Nonparametric Regression 78.3 Smooth Functions 78.4 Smoothness Parameter \\(\\lambda\\) 78.5 The Solution 78.6 Natural Cubic Splines 78.7 Basis Functions 78.8 Calculating the Solution 78.9 Linear Operator 78.10 Degrees of Freedom 78.11 Bayesian Intepretation 78.12 Bias and Variance Trade-off 78.13 Choosing \\(\\lambda\\) 78.14 Smoothers and Spline Models 78.15 Smoothers in R 78.16 Example", " 78 Nonparametric Regression 78.1 Simple Linear Regression Recall the set up for simple linear regression. For random variables \\((X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\), simple linear regression estimates the model \\[ Y_i = \\beta_1 + \\beta_2 X_i + E_i \\] where \\({\\operatorname{E}}[E_i | X_i] = 0\\), \\({\\operatorname{Var}}(E_i | X_i) = \\sigma^2\\), and \\({\\operatorname{Cov}}(E_i, E_j | X_i, X_j) = 0\\) for all \\(1 \\leq i, j \\leq n\\) and \\(i \\not= j\\). Note that in this model \\({\\operatorname{E}}[Y | X] = \\beta_1 + \\beta_2 X.\\) 78.2 Simple Nonparametric Regression In simple nonparametric regression, we define a similar model while eliminating the linear assumption: \\[ Y_i = s(X_i) + E_i \\] for some function \\(s(\\cdot)\\) with the same assumptions on the distribution of \\({\\boldsymbol{E}}| {\\boldsymbol{X}}\\). In this model, we also have \\[ {\\operatorname{E}}[Y | X] = s(X). \\] 78.3 Smooth Functions Suppose we consider fitting the model \\(Y_i = s(X_i) + E_i\\) with the restriction that \\(s \\in C^2\\), the class of functions with continuous second derivatives. We can set up an objective function that regularizes how smooth vs wiggly \\(s\\) is. Specifically, suppose for a given set of observed data \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\) we wish to identify a function \\(s \\in C^2\\) that minimizes for some \\(\\lambda\\) \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s&#39;&#39;(x)|^2 dx \\] 78.4 Smoothness Parameter \\(\\lambda\\) When minimizing \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s^{\\prime\\prime}(x)|^2 dx \\] it follows that if \\(\\lambda=0\\) then any function \\(s \\in C^2\\) that interpolates the data is a solution. As \\(\\lambda \\rightarrow \\infty\\), then the minimizing function is the simple linear regression solution. 78.5 The Solution For an observed data set \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\) where \\(n \\geq 4\\) and a fixed value \\(\\lambda\\), there is an exact solution to minimizing \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s^{\\prime\\prime}(x)|^2 dx. \\] The solution is called a natural cubic spline, which is constructed to have knots at \\(x_1, x_2, \\ldots, x_n\\). 78.6 Natural Cubic Splines Suppose without loss of generality that we have ordered \\(x_1 &lt; x_2 &lt; \\cdots &lt; x_n\\). We assume all \\(x_i\\) are unique to simplify the explanation here, but ties can be deal with. A natural cubic spline (NCS) is a function constructed from a set of piecewise cubic functions over the range \\([x_1, x_n]\\) joined at the knots so that the second derivative is continuous at the knots. Outside of the range (\\(&lt; x_1\\) or \\(&gt; x_n\\)), the spline is linear and it has continuous second derivatives at the endpoint knots. 78.7 Basis Functions Depending on the value \\(\\lambda\\), a different ncs will be constructed, but the entire family of ncs solutions over \\(0 &lt; \\lambda &lt; \\infty\\) can be constructed from a common set of basis functions. We construct \\(n\\) basis functions \\(N_1(x), N_2(x), \\ldots, N_n(x)\\) with coefficients \\(\\theta_1(\\lambda), \\theta_2(\\lambda), \\ldots, \\theta_n(\\lambda)\\). The NCS takes the form \\[ s(x) = \\sum_{i=1}^n \\theta_i(\\lambda) N_i(x). \\] Define \\(N_1(x) = 1\\) and \\(N_2(x) = x\\). For \\(i = 3, \\ldots, n\\), define \\(N_i(x) = d_{i-1}(x) - d_{i-2}(x)\\) where \\[ d_{i}(x) = \\frac{(x - x_i)^3 - (x - x_n)^3}{x_n - x_i}. \\] Recall that we’ve labeled indices so that \\(x_1 &lt; x_2 &lt; \\cdots &lt; x_n\\). 78.8 Calculating the Solution Let \\({\\boldsymbol{\\theta}}_\\lambda = (\\theta_1(\\lambda), \\theta_2(\\lambda), \\ldots, \\theta_n(\\lambda))^T\\) and let \\({\\boldsymbol{N}}\\) be the \\(n \\times n\\) matrix with \\((i, j)\\) entry equal to \\(N_j(x_i)\\). Finally, let \\({\\boldsymbol{\\Omega}}\\) be the \\(n \\times n\\) matrix with \\((i,j)\\) entry equal to \\(\\int N_i^{\\prime\\prime}(x) N_j^{\\prime\\prime}(x) dx\\). The solution to \\({\\boldsymbol{\\theta}}_\\lambda\\) are the values that minimize \\[ ({\\boldsymbol{y}}- {\\boldsymbol{N}}{\\boldsymbol{\\theta}})^T ({\\boldsymbol{y}}- {\\boldsymbol{N}}{\\boldsymbol{\\theta}}) + \\lambda {\\boldsymbol{\\theta}}^T {\\boldsymbol{\\Omega}}{\\boldsymbol{\\theta}}. \\] which results in \\[ \\hat{{\\boldsymbol{\\theta}}}_\\lambda = ({\\boldsymbol{N}}^T {\\boldsymbol{N}}+ \\lambda {\\boldsymbol{\\Omega}})^{-1} {\\boldsymbol{N}}^T {\\boldsymbol{y}}. \\] 78.9 Linear Operator Letting \\[ {\\boldsymbol{S}}_\\lambda = {\\boldsymbol{N}}({\\boldsymbol{N}}^T {\\boldsymbol{N}}+ \\lambda {\\boldsymbol{\\Omega}})^{-1} {\\boldsymbol{N}}^T \\] it folows that the fitted values are \\[ \\hat{{\\boldsymbol{y}}} = {\\boldsymbol{S}}_\\lambda {\\boldsymbol{y}}. \\] Thus, the fitted values from a NCS are contructed by taking linear combination of the response variable values \\(y_1, y_2, \\ldots, y_n\\). 78.10 Degrees of Freedom Recall that in OLS, we formed projection matrix \\({\\boldsymbol{P}}= {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T\\) and noted that the number of columns \\(p\\) of \\({\\boldsymbol{X}}\\) is also found in the trace of \\({\\boldsymbol{P}}\\) where \\(\\operatorname{tr}({\\boldsymbol{P}}) = p\\). The effective degrees of freedom for a model fit by a linear operator is calculated as the trace of the operator. Therefore, for a given \\(\\lambda\\), the effective degrees of freedom is \\[ \\operatorname{df}_\\lambda = \\operatorname{tr}({\\boldsymbol{S}}_\\lambda). \\] 78.11 Bayesian Intepretation Minimizing \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s^{\\prime\\prime}(x)|^2 dx \\] is equivalent to maximizing \\[ \\exp\\left\\{ -\\frac{\\sum_{i=1}^n (y_i - s(x_i))^2}{2\\sigma^2} \\right\\} \\exp\\left\\{-\\frac{\\lambda}{2\\sigma^2} \\int |s^{\\prime\\prime}(x)|^2 dx\\right\\}. \\] Therefore, the NCS solution can be interpreted as calculting the MAP where \\(Y | X\\) is Normal and there’s an Exponential prior on the smoothness of \\(s\\). 78.12 Bias and Variance Trade-off Typically we will choose some \\(0 &lt; \\lambda &lt; \\infty\\) in an effort to balance the bias and variance. Let \\(\\hat{Y} = \\hat{s}(X; \\lambda)\\) where \\(\\hat{s}(\\cdot; \\lambda)\\) minimizes the above for some chosen \\(\\lambda\\) on an independent data set. Then \\[ \\begin{aligned} {\\operatorname{E}}\\left[\\left(Y - \\hat{Y}\\right)^2\\right] &amp; = {\\rm E}\\left[\\left(s(x) + E - \\hat{s}(x; \\lambda)\\right)^2 \\right] \\\\ \\ &amp; = {\\rm E}\\left[\\left( s(x) - \\hat{s}(x; \\lambda) \\right)^2 \\right] + {\\rm Var}(E) \\\\ \\ &amp; = \\left( s(x) - {\\rm E}[\\hat{s}(x; \\lambda)]\\right)^2 + {\\rm Var}\\left(\\hat{s}(x; \\lambda)\\right) + {\\rm Var}(E) \\\\ \\ &amp; = \\mbox{bias}^2_{\\lambda} + \\mbox{variance}_{\\lambda} + {\\rm Var}(E) \\end{aligned} \\] where all of the above calculations are conditioned on \\(X=x\\). In minimizing \\[ \\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int |s^{\\prime\\prime}(x)|^2 dx \\] the relationship is such that: \\[ \\uparrow \\lambda \\Longrightarrow \\mbox{bias}^2 \\uparrow, \\mbox{variance} \\downarrow \\] \\[ \\downarrow \\lambda \\Longrightarrow \\mbox{bias}^2 \\downarrow, \\mbox{variance} \\uparrow \\] 78.13 Choosing \\(\\lambda\\) There are several approaches that are commonly used to identify a value of \\(\\lambda\\), including: Scientific knowledge that guides the acceptable value of \\(\\operatorname{df}_\\lambda\\) Cross-validation or some other prediction quality measure Model selection measures, such as Akaike information criterion (AIC) or Mallows \\(C_p\\) 78.14 Smoothers and Spline Models We investigated one type of nonparametric regression model here, the NCS. However, in general there are many such “smoother” methods available in the simple nonparametric regression scenario. Splines are particularly popular since splines are constructed from putting together polynomials and are therefore usually tractable to compute and analyze. 78.15 Smoothers in R There are several functions and packages available in R for computing smoothers and tuning smoothness parameters. Examples include: splines library smooth.spline() loess() lowess() 78.16 Example &gt; y2 &lt;- smooth.spline(x=x, y=y, df=2) &gt; y5 &lt;- smooth.spline(x=x, y=y, df=5) &gt; y25 &lt;- smooth.spline(x=x, y=y, df=25) &gt; ycv &lt;- smooth.spline(x=x, y=y) &gt; ycv Call: smooth.spline(x = x, y = y) Smoothing Parameter spar= 0.5162045 lambda= 0.0002730906 (11 iterations) Equivalent Degrees of Freedom (Df): 7.293673 Penalized Criterion (RSS): 14.80602 GCV: 1.180651 "],
["generalized-additive-models-1.html", "79 Generalized Additive Models 79.1 Ordinary Least Squares 79.2 Additive Models 79.3 Backfitting 79.4 GAM Definition 79.5 Overview of Fitting GAMs 79.6 GAMs in R 79.7 Example", " 79 Generalized Additive Models 79.1 Ordinary Least Squares Recall that OLS estimates the model \\[ \\begin{aligned} Y_i &amp; = \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + E_i \\\\ &amp; = {\\boldsymbol{X}}_i {\\boldsymbol{\\beta}}+ E_i \\end{aligned} \\] where \\({\\operatorname{E}}[{\\boldsymbol{E}}| {\\boldsymbol{X}}] = {\\boldsymbol{0}}\\) and \\({\\operatorname{Cov}}({\\boldsymbol{E}}| {\\boldsymbol{X}}) = \\sigma^2 {\\boldsymbol{I}}\\). 79.2 Additive Models The additive model (which could also be called “ordinary nonparametric additive regression”) is of the form \\[ \\begin{aligned} Y_i &amp; = s_1(X_{i1}) + s_2(X_{i2}) + \\ldots + s_p(X_{ip}) + E_i \\\\ &amp; = \\sum_{j=1}^p s_j(X_{ij}) + E_i \\end{aligned} \\] where the \\(s_j(\\cdot)\\) for \\(j = 1, \\ldots, p\\) are a set of nonparametric (or flexible) functions. Again, we assume that \\({\\operatorname{E}}[{\\boldsymbol{E}}| {\\boldsymbol{X}}] = {\\boldsymbol{0}}\\) and \\({\\operatorname{Cov}}({\\boldsymbol{E}}| {\\boldsymbol{X}}) = \\sigma^2 {\\boldsymbol{I}}\\). 79.3 Backfitting The additive model can be fit through a technique called backfitting. Intialize \\(s^{(0)}_j(x)\\) for \\(j = 1, \\ldots, p\\). For \\(t=1, 2, \\ldots\\), fit \\(s_j^{(t)}(x)\\) on response variable \\[y_i - \\sum_{k \\not= j} s_k^{(t-1)}(x_{ij}).\\] Repeat until convergence. Note that some extra steps have to be taken to deal with the intercept. 79.4 GAM Definition \\(Y | {\\boldsymbol{X}}\\) is distributed according to an exponential family distribution. The extension of additive models to this family of response variable is called generalized additive models (GAMs). The model is of the form \\[ g\\left({\\operatorname{E}}[Y_i | {\\boldsymbol{X}}_i]\\right) = \\sum_{j=1}^p s_j(X_{ij}) \\] where \\(g(\\cdot)\\) is the link function and the \\(s_j(\\cdot)\\) are flexible and/or nonparametric functions. 79.5 Overview of Fitting GAMs Fitting GAMs involves putting together the following three tools: We know how to fit a GLM via IRLS We know how to fit a smoother of a single explanatory variable via a least squares solution, as seen for the NCS We know how to combine additive smoothers by backfitting 79.6 GAMs in R Three common ways to fit GAMs in R: Utilize glm() on explanatory variables constructed from ns() or bs() The gam library The mgcv library 79.7 Example &gt; set.seed(508) &gt; x1 &lt;- seq(1, 10, length.out=50) &gt; n &lt;- length(x1) &gt; x2 &lt;- rnorm(n) &gt; f &lt;- 4*log(x1) + sin(x1) - 7 + 0.5*x2 &gt; p &lt;- exp(f)/(1+exp(f)) &gt; summary(p) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.001842 0.074171 0.310674 0.436162 0.860387 0.944761 &gt; y &lt;- rbinom(n, size=1, prob=p) &gt; mean(y) [1] 0.42 &gt; df &lt;- data.frame(x1=x1, x2=x2, y=y) Here, we use the gam() function from the mgcv library. It automates choosing the smoothness of the splines. &gt; library(mgcv) &gt; mygam &lt;- gam(y ~ s(x1) + s(x2), family = binomial(), data=df) &gt; library(broom) &gt; tidy(mygam) # A tibble: 2 x 5 term edf ref.df statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 s(x1) 1.87 2.37 12.7 0.00531 2 s(x2) 1.00 1.00 1.16 0.281 &gt; summary(mygam) Family: binomial Link function: logit Formula: y ~ s(x1) + s(x2) Parametric coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.1380 0.6723 -1.693 0.0905 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Approximate significance of smooth terms: edf Ref.df Chi.sq p-value s(x1) 1.87 2.375 12.743 0.00531 ** s(x2) 1.00 1.000 1.163 0.28084 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 R-sq.(adj) = 0.488 Deviance explained = 47% UBRE = -0.12392 Scale est. = 1 n = 50 True probabilities vs. estimated probabilities. &gt; plot(p, mygam$fitted.values, pch=19); abline(0,1) Smoother estimated for x1. &gt; plot(mygam, select=1) Smoother estimated for x2. &gt; plot(mygam, select=2) Here, we use the glm() function and include as an explanatory variable a NCS built from the ns() function from the splines library. We include a df argument in the ns() call. &gt; library(splines) &gt; myglm &lt;- glm(y ~ ns(x1, df=2) + x2, family = binomial(), data=df) &gt; tidy(myglm) # A tibble: 4 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -10.9 5.31 -2.06 0.0396 2 ns(x1, df = 2)1 21.4 10.1 2.11 0.0348 3 ns(x1, df = 2)2 6.33 2.11 3.00 0.00272 4 x2 0.734 0.609 1.21 0.228 The spline basis evaluated at x1 values. &gt; ns(x1, df=2) 1 2 [1,] 0.00000000 0.00000000 [2,] 0.03114456 -0.02075171 [3,] 0.06220870 -0.04138180 [4,] 0.09311200 -0.06176867 [5,] 0.12377405 -0.08179071 [6,] 0.15411442 -0.10132630 [7,] 0.18405270 -0.12025384 [8,] 0.21350847 -0.13845171 [9,] 0.24240131 -0.15579831 [10,] 0.27065081 -0.17217201 [11,] 0.29817654 -0.18745121 [12,] 0.32489808 -0.20151430 [13,] 0.35073503 -0.21423967 [14,] 0.37560695 -0.22550571 [15,] 0.39943343 -0.23519080 [16,] 0.42213406 -0.24317334 [17,] 0.44362840 -0.24933170 [18,] 0.46383606 -0.25354429 [19,] 0.48267660 -0.25568949 [20,] 0.50006961 -0.25564569 [21,] 0.51593467 -0.25329128 [22,] 0.53019136 -0.24850464 [23,] 0.54275927 -0.24116417 [24,] 0.55355797 -0.23114825 [25,] 0.56250705 -0.21833528 [26,] 0.56952943 -0.20260871 [27,] 0.57462513 -0.18396854 [28,] 0.57787120 -0.16253131 [29,] 0.57934806 -0.13841863 [30,] 0.57913614 -0.11175212 [31,] 0.57731586 -0.08265339 [32,] 0.57396762 -0.05124405 [33,] 0.56917185 -0.01764570 [34,] 0.56300897 0.01802003 [35,] 0.55555939 0.05563154 [36,] 0.54690354 0.09506722 [37,] 0.53712183 0.13620546 [38,] 0.52629468 0.17892464 [39,] 0.51450251 0.22310315 [40,] 0.50182573 0.26861939 [41,] 0.48834478 0.31535174 [42,] 0.47414005 0.36317859 [43,] 0.45929198 0.41197833 [44,] 0.44388099 0.46162934 [45,] 0.42798748 0.51201003 [46,] 0.41169188 0.56299877 [47,] 0.39507460 0.61447395 [48,] 0.37821607 0.66631397 [49,] 0.36119670 0.71839720 [50,] 0.34409692 0.77060206 attr(,&quot;degree&quot;) [1] 3 attr(,&quot;knots&quot;) 50% 5.5 attr(,&quot;Boundary.knots&quot;) [1] 1 10 attr(,&quot;intercept&quot;) [1] FALSE attr(,&quot;class&quot;) [1] &quot;ns&quot; &quot;basis&quot; &quot;matrix&quot; Plot of basis function values vs x1. &gt; summary(myglm) Call: glm(formula = y ~ ns(x1, df = 2) + x2, family = binomial(), data = df) Deviance Residuals: Min 1Q Median 3Q Max -2.0214 -0.3730 -0.0162 0.5762 1.7616 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -10.9229 5.3079 -2.058 0.03960 * ns(x1, df = 2)1 21.3848 10.1318 2.111 0.03480 * ns(x1, df = 2)2 6.3266 2.1103 2.998 0.00272 ** x2 0.7342 0.6089 1.206 0.22795 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 68.029 on 49 degrees of freedom Residual deviance: 35.682 on 46 degrees of freedom AIC: 43.682 Number of Fisher Scoring iterations: 7 &gt; anova(myglm, test=&quot;LRT&quot;) Analysis of Deviance Table Model: binomial, link: logit Response: y Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 49 68.029 ns(x1, df = 2) 2 30.755 47 37.274 2.097e-07 *** x2 1 1.592 46 35.682 0.207 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 True probabilities vs. estimated probabilities. &gt; plot(p, myglm$fitted.values, pch=19); abline(0,1) "],
["bootstrap-for-statistical-models.html", "80 Bootstrap for Statistical Models 80.1 Homoskedastic Models 80.2 Residuals 80.3 Studentized Residuals 80.4 Confidence Intervals 80.5 Hypothesis Testing 80.6 Parametric Bootstrap", " 80 Bootstrap for Statistical Models 80.1 Homoskedastic Models Let’s first discuss how one can utilize the bootstrap on any of the three homoskedastic models: Simple linear regression Ordinary least squares Additive models 80.2 Residuals In each of these scenarios we sample data \\(({\\boldsymbol{X}}_1, Y_1), ({\\boldsymbol{X}}_2, Y_2), \\ldots, ({\\boldsymbol{X}}_n, Y_n)\\). Let suppose we calculate fitted values \\(\\hat{Y}_i\\) and they are unbiased: \\[ {\\operatorname{E}}[\\hat{Y}_i | {\\boldsymbol{X}}] = {\\operatorname{E}}[Y_i | {\\boldsymbol{X}}]. \\] We can calculate residuals \\(\\hat{E}_i = Y_i - \\hat{Y}_i\\) for \\(i=1, 2, \\ldots, n\\). 80.3 Studentized Residuals One complication is that the residuals have a covariance. For example, in OLS we showed that \\[ {\\operatorname{Cov}}(\\hat{{\\boldsymbol{E}}}) = \\sigma^2 ({\\boldsymbol{I}}- {\\boldsymbol{P}}) \\] where \\({\\boldsymbol{P}}= {\\boldsymbol{X}}({\\boldsymbol{X}}^T {\\boldsymbol{X}})^{-1} {\\boldsymbol{X}}^T\\). To correct for this induced heteroskedasticity, we studentize the residuals by calculating \\[ R_i = \\frac{\\hat{E}_i}{\\sqrt{1-P_{ii}}} \\] which gives \\({\\operatorname{Cov}}({\\boldsymbol{R}}) = \\sigma^2 {\\boldsymbol{I}}\\). 80.4 Confidence Intervals The following is a bootstrap procedure for calculating a confidence interval on some statistic \\(\\hat{\\theta}\\) calculated from a homoskedastic model fit. An example is \\(\\hat{\\beta}_j\\) in an OLS. Fit the model to obtain fitted values \\(\\hat{Y}_i\\), studentized residuals \\(R_i\\), and the statistic of interest \\(\\hat{\\theta}\\). For \\(b = 1, 2, \\ldots, B\\). Sample \\(n\\) observations with replacement from \\(\\{R_i\\}_{i=1}^n\\) to obtain bootstrap residuals \\(R_1^{*}, R_2^{*}, \\ldots, R_n^{*}\\). Form new response variables \\(Y_i^{*} = \\hat{Y}_i + R_i^{*}\\). Fit the model to obtain \\(\\hat{Y}^{*}_i\\) and all other fitted parameters. Calculate statistic of interest \\(\\hat{\\theta}^{*(b)}\\). The bootstrap statistics \\(\\hat{\\theta}^{*(1)}, \\hat{\\theta}^{*(2)}, \\ldots, \\hat{\\theta}^{*(B)}\\) are then utilized through one of the techniques discussed earlier (percentile, pivotal, studentized pivotal) to calculate a bootstrap CI. 80.5 Hypothesis Testing Suppose we are testing the hypothesis \\(H_0: {\\operatorname{E}}[Y | {\\boldsymbol{X}}] = f_0({\\boldsymbol{X}})\\) vs \\(H_1: {\\operatorname{E}}[Y | {\\boldsymbol{X}}] = f_1({\\boldsymbol{X}})\\). Suppose it is possible to form unbiased estimates \\(f_0({\\boldsymbol{X}})\\) and \\(f_1({\\boldsymbol{X}})\\) given \\({\\boldsymbol{X}}\\), and \\(f_0\\) is a restricted version of \\(f_1\\). Suppose also we have a statistic \\(T(\\hat{f}_0, \\hat{f}_1)\\) for performing this test so that the larger the statistic, the more evidence there is against the null hypothesis in favor of the alternative. The big picture strategy is to bootstrap studentized residuals from the unconstrained (alternative hypothesis) fitted model and then add those to the constrained (null hypothesis) fitted model to generate bootstrap null data sets. Fit the models to obtain fitted values \\(\\hat{f}_0({\\boldsymbol{X}}_i)\\) and \\(\\hat{f}_1({\\boldsymbol{X}}_i)\\), studentized residuals \\(R_i\\) from the fit \\(\\hat{f}_1({\\boldsymbol{X}}_i)\\), and the observed statistic \\(T(\\hat{f}_0, \\hat{f}_1)\\). For \\(b = 1, 2, \\ldots, B\\). Sample \\(n\\) observations with replacement from \\(\\{R_i\\}_{i=1}^n\\) to obtain bootstrap residuals \\(R_1^{*}, R_2^{*}, \\ldots, R_n^{*}\\). Form new response variables \\(Y_i^{*} = \\hat{f}_0({\\boldsymbol{X}}_i) + R_i^{*}\\). Fit the models on the response variables \\(Y_i^{*}\\) to obtain \\(\\hat{f}^{*}_0\\) and \\(\\hat{f}^{*}_1\\). Calculate statistic \\(T(\\hat{f}^{*(b)}_0, \\hat{f}^{*(b)}_1)\\). The p-value is then calculated as \\[ \\frac{\\sum_{b=1}^B 1\\left(T(\\hat{f}^{*(b)}_0, \\hat{f}^{*(b)}_1) \\geq T(\\hat{f}_0, \\hat{f}_1) \\right) }{B} \\] 80.6 Parametric Bootstrap For more complex scenarios, such as GLMs, GAMs, and heteroskedastic models, it is typically more straightforward to utilize a parametric bootstrap. "],
["high-dimensional-data-and-inference.html", "81 High-Dimensional Data and Inference 81.1 Definition 81.2 Examples 81.3 HD Gene Expression Data 81.4 Many Responses Model 81.5 HD SNP Data 81.6 Many Regressors Model 81.7 Goals 81.8 Challenges", " 81 High-Dimensional Data and Inference 81.1 Definition High-dimesional inference is the scenario where we perform inference simultaneously on “many” paramaters. “Many” can be as few as three parameters (which is where things start to get interesting), but in modern applications this is typically on the order of thousands to billions of paramaters. High-dimesional data is a data set where the number of variables measured is many. Large same size data is a data set where few variables are measured, but many observations are measured. Big data is a data set where there are so many data points that it cannot be managed straightforwardly in memory, but must rather be stored and accessed elsewhere. Big data can be high-dimensional, large sample size, or both. We will abbreviate high-dimensional with HD. 81.2 Examples In all of these examples, many measurements are taken and the goal is often to perform inference on many paramaters simultaneously. Spatial epidemiology Environmental monitoring Internet user behavior Genomic profiling Neuroscience imaging Financial time series 81.3 HD Gene Expression Data Gene Expression It’s possible to measure the level of gene expression – how much mRNA is being transcribed – from thousands of cell simultaneously in a single biological sample. Typically, gene expression is measured over varying biological conditions, and the goal is to perform inference on the relationship between expression and the varying conditions. This results in thousands of simultaneous inferences. The typical sizes of these data sets are 1000 to 50,000 genes and 10 to 1000 observations. The gene expression values are typically modeled as approximately Normal or overdispersed Poisson. There is usually shared signal among the genes, and there are often unobserved latent variables. \\[ \\begin{array}{rc} &amp; {\\boldsymbol{Y}}_{m \\times n} \\\\ &amp; \\text{observations} \\\\ \\text{genes} &amp; \\begin{bmatrix} y_{11} &amp; y_{12} &amp; \\cdots &amp; y_{1n} \\\\ y_{21} &amp; y_{22} &amp; \\cdots &amp; y_{2n} \\\\ &amp; &amp; &amp; \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; \\\\ y_{m1} &amp; y_{m2} &amp; \\cdots &amp; y_{mn} \\end{bmatrix} \\\\ &amp; \\\\ &amp; {\\boldsymbol{X}}_{d \\times n} \\ \\text{study design} \\\\ &amp; \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{d1} &amp; x_{d2} &amp; \\cdots &amp; x_{dn} \\end{bmatrix} \\end{array} \\] The \\({\\boldsymbol{Y}}\\) matrix contains gene expression measurements for \\(m\\) genes (rows) by \\(n\\) observations (columns). The values \\(y_{ij}\\) are either in \\(\\mathbb{R}\\) or \\(\\mathbb{Z}^{+} = \\{0, 1, 2, \\ldots \\}\\). The \\({\\boldsymbol{X}}\\) matrix contains the study design of \\(d\\) explanatory variables (rows) by the \\(n\\) observations (columns). Note that \\(m \\gg n \\gg d\\). 81.4 Many Responses Model Gene expression is an example of what I call the many responses model. We’re interested in performing simultaneous inference on \\(d\\) paramaters for each of \\(m\\) models such as: \\[ \\begin{aligned} &amp; {\\boldsymbol{Y}}_{1} = {\\boldsymbol{\\beta}}_1 {\\boldsymbol{X}}+ {\\boldsymbol{E}}_1 \\\\ &amp; {\\boldsymbol{Y}}_{2} = {\\boldsymbol{\\beta}}_2 {\\boldsymbol{X}}+ {\\boldsymbol{E}}_2 \\\\ &amp; \\vdots \\\\ &amp; {\\boldsymbol{Y}}_{m} = {\\boldsymbol{\\beta}}_m {\\boldsymbol{X}}+ {\\boldsymbol{E}}_m \\\\ \\end{aligned} \\] For example, \\({\\boldsymbol{Y}}_{1} = {\\boldsymbol{\\beta}}_1 {\\boldsymbol{X}}+ {\\boldsymbol{E}}_1\\) is vector notation of (in terms of observations \\(j\\)): \\[ \\left\\{ Y_{1j} = \\beta_{11} X_{1j} + \\beta_{12} X_{2j} + \\cdots + \\beta_{1d} X_{dj} + E_{1j} \\right\\}_{j=1}^n \\] We have made two changes from last week: We have transposed \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{\\beta}}\\). We have changed the number of explanatory variables from \\(p\\) to \\(d\\). Let \\({\\boldsymbol{B}}_{m \\times d}\\) be the matrix of parameters \\((\\beta_{ik})\\) relating the \\(m\\) response variables to the \\(d\\) explanatory variables. The full HD model is \\[ \\begin{array}{cccccc} {\\boldsymbol{Y}}_{m \\times n} &amp; = &amp; {\\boldsymbol{B}}_{m \\times d} &amp; {\\boldsymbol{X}}_{d \\times n} &amp; + &amp; {\\boldsymbol{E}}_{m \\times n} \\\\ \\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ Y_{i1} &amp; \\cdots &amp; Y_{in}\\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix} &amp; = &amp; \\begin{bmatrix} &amp; \\\\ &amp; \\\\ &amp; \\\\ \\beta_{i1} &amp; \\beta_{id} \\\\ &amp; \\\\ &amp; \\\\ &amp; \\\\ \\end{bmatrix} &amp; \\begin{bmatrix} X_{11} &amp; \\cdots &amp; X_{1n} \\\\ X_{d1} &amp; \\cdots &amp; X_{dn} \\\\ \\end{bmatrix} &amp; + &amp; \\begin{bmatrix} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ E_{i1} &amp; \\cdots &amp; E_{in} \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{bmatrix} \\end{array} \\] Note that if we make OLS assumptions, then we can calculate: \\[ \\hat{{\\boldsymbol{B}}}^{\\text{OLS}} = {\\boldsymbol{Y}}{\\boldsymbol{X}}^T ({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} \\] \\[ \\hat{{\\boldsymbol{Y}}} = \\hat{{\\boldsymbol{B}}} {\\boldsymbol{X}}= {\\boldsymbol{Y}}{\\boldsymbol{X}}^T ({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} {\\boldsymbol{X}}\\] so here the projection matrix is \\({\\boldsymbol{P}}= {\\boldsymbol{X}}^T ({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} {\\boldsymbol{X}}\\) and acts from the RHS, \\(\\hat{{\\boldsymbol{Y}}} = {\\boldsymbol{Y}}{\\boldsymbol{P}}\\). We will see this week and next that \\(\\hat{{\\boldsymbol{B}}}^{\\text{OLS}}\\) has nontrivial drawbacks. Thefore, we will be exploring other ways of estimating \\({\\boldsymbol{B}}\\). We of course aren’t limited to OLS models. We could consider the many response GLM: \\[ g\\left({\\operatorname{E}}\\left[ \\left. {\\boldsymbol{Y}}_{m \\times n} \\right| {\\boldsymbol{X}}\\right]\\right) = {\\boldsymbol{B}}_{m \\times d} {\\boldsymbol{X}}_{d \\times n} \\] and we could even replace \\({\\boldsymbol{B}}_{m \\times d} {\\boldsymbol{X}}_{d \\times n}\\) with \\(d\\) smoothers for each of the \\(m\\) response variable. 81.5 HD SNP Data SNPs It is possible to measure single nucleotide polymorphisms at millions of locations across the genome. The base (A, C, G, or T) is measured from one of the strands. For example, on the figure to the left, the individual is heterozygous CT at this SNP location. \\[ \\begin{array}{rc} &amp; {\\boldsymbol{X}}_{m \\times n} \\\\ &amp; \\text{individuals} \\\\ \\text{SNPs} &amp; \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1n} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2n} \\\\ &amp; &amp; &amp; \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; \\\\ x_{m1} &amp; x_{m2} &amp; \\cdots &amp; x_{mn} \\end{bmatrix} \\\\ &amp; \\\\ &amp; {\\boldsymbol{y}}_{1 \\times n} \\ \\text{trait} \\\\ &amp; \\begin{bmatrix} y_{11} &amp; y_{12} &amp; \\cdots &amp; y_{1n} \\end{bmatrix} \\end{array} \\] The \\({\\boldsymbol{X}}\\) matrix contains SNP genotypes for \\(m\\) SNPs (rows) by \\(n\\) individuals (columns). The values \\(x_{ij} \\in \\{0, 1, 2\\}\\) are conversions of genotypes (e.g., CC, CT, TT) to counts of one of the alleles. The \\({\\boldsymbol{y}}\\) vector contains the trait values of the \\(n\\) individuals. Note that \\(m \\gg n\\). 81.6 Many Regressors Model The SNP-trait model is an example of what I call the many regressors model. A single model is fit of a response variable on many regressors (i.e., explanatory variables) simultaneously. This involves simultaneously inferring \\(m\\) paramaters \\({\\boldsymbol{\\beta}}= (\\beta_1, \\beta_2, \\ldots, \\beta_m)\\) in models such as: \\[ {\\boldsymbol{Y}}= \\alpha \\boldsymbol{1} + {\\boldsymbol{\\beta}}{\\boldsymbol{X}}+ {\\boldsymbol{E}}\\] which is an \\(n\\)-vector with component \\(j\\) being: \\[ Y_j = \\alpha + \\sum_{i=1}^m \\beta_i X_{ij} + E_{j} \\] As with the many responses model, we do not need to limit the model to the OLS type where the response variable is approximately Normal distributed. Instead we can consider more general models such as \\[ g\\left({\\operatorname{E}}\\left[ \\left. {\\boldsymbol{Y}}\\right| {\\boldsymbol{X}}\\right]\\right) = \\alpha \\boldsymbol{1} + {\\boldsymbol{\\beta}}{\\boldsymbol{X}}\\] for some link function \\(g(\\cdot)\\). 81.7 Goals In both types of models we are interested in: Forming point estimates Testing statistical hypothesis Calculating posterior distributions Leveraging the HD data to increase our power and accuracy Sometimes we are also interested in confidence intervals in high dimensions, but this is less common. 81.8 Challenges Here are several of the new challenges we face when analyzing high-dimensional data: Standard estimation methods may be suboptimal in high dimensions New measures of significance are needed There may be dependence and latent variables among the high-dimensional variables The fact that \\(m \\gg n\\) poses challenges, especially in the many regressors model HD data provide new challenges, but they also provide opportunities to model variation in the data in ways not possible for low-dimensional data. "],
["many-responses-model-1.html", "82 Many Responses Model", " 82 Many Responses Model "],
["shrinkage-and-empirical-bayes.html", "83 Shrinkage and Empirical Bayes 83.1 Estimating Several Means 83.2 Usual MLE 83.3 Loss Function 83.4 Stein’s Paradox 83.5 Inverse Regression Approach 83.6 Empirical Bayes Estimate 83.7 EB for a Many Responses Model", " 83 Shrinkage and Empirical Bayes 83.1 Estimating Several Means Let’s start with the simplest many responses model where there is only an interncept and only one observation per variable. This means that \\(n=1\\) and \\(d=1\\) where \\({\\boldsymbol{X}}= 1\\). This model can be written as \\(Y_i \\sim \\mbox{Normal}(\\beta_i, 1)\\) for the \\(i=1, 2, \\ldots, m\\) response variables. Suppose also that \\(Y_1, Y_2, \\ldots, Y_m\\) are jointly independent. Let’s assume that \\(\\beta_1, \\beta_2, \\ldots, \\beta_m\\) are fixed, nonrandom parameters. 83.2 Usual MLE The usual estimates of \\(\\beta_i\\) are to set \\[ \\hat{\\beta}_i^{\\text{MLE}} = {\\boldsymbol{Y}}_i. \\] This is also the OLS solution. 83.3 Loss Function Suppose we are interested in the simultaneous loss function \\[ L({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}) = \\sum_{i=1} (\\beta_i - \\hat{\\beta}_i)^2 \\] with risk \\(R({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}) = {\\operatorname{E}}[L({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}})]\\). 83.4 Stein’s Paradox Consider the following James-Stein estimator: \\[ \\hat{\\beta}_i^{\\text{JS}} = \\left(1 - \\frac{m-2}{\\sum_{k=1}^m Y_k^2} \\right) Y_i. \\] In a shocking result called Stein’s paradox, it was shown that when \\(m \\geq 3\\) then \\[ R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{JS}}\\right) &lt; R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{MLE}}\\right). \\] This means that the usual MLE is dominated by this JS estimator for any, even nonrandom, configuration of \\(\\beta_1, \\beta_2, \\ldots, \\beta_m\\)! What is going on? Let’s first take a linear regression point of view to better understand this paradox. Then we will return to the empirical Bayes example from earlier. &gt; beta &lt;- seq(-1, 1, length.out=50) &gt; y &lt;- beta + rnorm(length(beta)) The blue line is the least squares regression line. &gt; beta &lt;- seq(-10, 10, length.out=50) &gt; y &lt;- beta + rnorm(length(beta)) The blue line is the least squares regression line. 83.5 Inverse Regression Approach While \\(Y_i = \\beta_i + E_i\\) where \\(E_i \\sim \\mbox{Normal}(0,1)\\), it is also the case that \\(\\beta_i = Y_i - E_i\\) where \\(-E_i \\sim \\mbox{Normal}(0,1)\\). Even though we’re assuming the \\(\\beta_i\\) are fixed, suppose we imagine for the moment that the \\(\\beta_i\\) are random and take a least squares appraoch. We will try to estimate the linear model \\[ {\\operatorname{E}}[\\beta_i | Y_i] = a + b Y_i. \\] Why would we do this? The loss function is \\[ \\sum_{i=1}^m (\\beta_i - \\hat{\\beta}_i)^2 \\] so it makes sense to estimate \\(\\beta_i\\) by setting \\(\\hat{\\beta}_i\\) to a regression line. The least squares solution tells us to set \\[ \\begin{aligned} \\hat{\\beta}_i &amp; = \\hat{a} + \\hat{b} Y_i \\\\ &amp; = (\\bar{\\beta} - \\hat{b} \\bar{Y}) + \\hat{b} Y_i \\\\ &amp; = \\bar{\\beta} + \\hat{b} (Y_i - \\bar{Y}) \\end{aligned} \\] where \\[ \\hat{b} = \\frac{\\sum_{i=1}^m (Y_i - \\bar{Y})(\\beta_i - \\bar{\\beta})}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2}. \\] We can estimate \\(\\bar{\\beta}\\) with \\(\\bar{Y}\\) since \\({\\operatorname{E}}[\\bar{\\beta}] = {\\operatorname{E}}[\\bar{Y}]\\). We also need to find an estimate of \\(\\sum_{i=1}^m (Y_i - \\bar{Y})(\\beta_i - \\bar{\\beta})\\). Note that \\[ \\beta_i - \\bar{\\beta} = Y_i - \\bar{Y} - (E_i + \\bar{E}) \\] so that \\[ \\begin{aligned} \\sum_{i=1}^m (Y_i - \\bar{Y})(\\beta_i - \\bar{\\beta}) = &amp; \\sum_{i=1}^m (Y_i - \\bar{Y})(Y_i - \\bar{Y}) \\\\ &amp; + \\sum_{i=1}^m (Y_i - \\bar{Y})(E_i - \\bar{E}) \\end{aligned} \\] Since \\(Y_i = \\beta_i + E_i\\) it follows that \\[ {\\operatorname{E}}\\left[\\sum_{i=1}^m (Y_i - \\bar{Y})(E_i - \\bar{E})\\right] = {\\operatorname{E}}\\left[\\sum_{i=1}^m (E_i - \\bar{E})(E_i - \\bar{E})\\right] = m-1. \\] Therefore, \\[ {\\operatorname{E}}\\left[\\sum_{i=1}^m (Y_i - \\bar{Y})(\\beta_i - \\bar{\\beta})\\right] = {\\operatorname{E}}\\left[\\sum_{i=1}^m (Y_i - \\bar{Y})^2 - (m-1)\\right]. \\] This yields \\[ \\hat{b} = \\frac{\\sum_{i=1}^m (Y_i - \\bar{Y})^2 - (m-1)}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2} = 1 - \\frac{m-1}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2} \\] and \\[ \\hat{\\beta}_i^{\\text{IR}} = \\bar{Y} + \\left(1 - \\frac{m-1}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2}\\right) (Y_i - \\bar{Y}) \\] If instead we had started with the no intercept model \\[ {\\operatorname{E}}[\\beta_i | Y_i] = b Y_i. \\] we would have ended up with \\[ \\hat{\\beta}_i^{\\text{IR}} = \\left(1 - \\frac{m-1}{\\sum_{i=1}^m (Y_i - \\bar{Y})^2}\\right) Y_i \\] In either case, it can be shown that \\[ R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{IR}}\\right) &lt; R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{MLE}}\\right). \\] The blue line is the least squares regression line of \\(\\beta_i\\) on \\(Y_i\\), and the red line is \\(\\hat{\\beta}_i^{\\text{IR}}\\). 83.6 Empirical Bayes Estimate Suppose that \\(Y_i | \\beta_i \\sim \\mbox{Normal}(\\beta_i, 1)\\) where these rv’s are jointly independent. Also suppose that \\(\\beta_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Normal}(a, b^2)\\). Taking the empirical Bayes approach, we get: \\[ f(y_i ; a, b) = \\int f(y_i | \\beta_i) f(\\beta_i; a, b) d\\beta_i \\sim \\mbox{Normal}(a, 1+b^2). \\] \\[ \\implies \\hat{a} = \\overline{Y}, \\ 1+\\hat{b}^2 = \\frac{\\sum_{k=1}^m (Y_k - \\overline{Y})^2}{n} \\] \\[ \\begin{aligned} {\\operatorname{E}}[\\beta_i | Y_i] &amp; = \\frac{1}{1+b^2}a + \\frac{b^2}{1+b^2}Y_i \\implies \\\\ &amp; \\\\ \\hat{\\beta}_i^{\\text{EB}} &amp; = \\hat{{\\operatorname{E}}}[\\beta_i | Y_i] = \\frac{1}{1+\\hat{b}^2}\\hat{a} + \\frac{\\hat{b}^2}{1+\\hat{b}^2}Y_i \\\\ &amp; = \\frac{m}{\\sum_{k=1}^m (Y_k - \\overline{Y})^2} \\overline{Y} + \\left(1-\\frac{m}{\\sum_{k=1}^m (Y_k - \\overline{Y})^2}\\right) Y_i \\end{aligned} \\] As with \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{JS}}\\) and \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{IR}}\\), we have \\[ R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{EB}}\\right) &lt; R\\left({\\boldsymbol{\\beta}}, \\hat{{\\boldsymbol{\\beta}}}^{\\text{MLE}}\\right). \\] 83.7 EB for a Many Responses Model Consider the many responses model where \\({\\boldsymbol{Y}}_{i} | {\\boldsymbol{X}}\\sim \\mbox{MVN}_n({\\boldsymbol{\\beta}}_i {\\boldsymbol{X}}, \\sigma^2 {\\boldsymbol{I}})\\) where the vectors \\({\\boldsymbol{Y}}_{i} | {\\boldsymbol{X}}\\) are jointly independent (\\(i=1, 2, \\ldots, m\\)). Here we’ve made the simplifying assumption that the variance \\(\\sigma^2\\) is equal across all responses, but this would not be generally true. The OLS (and MLE) solution is \\[ \\hat{{\\boldsymbol{B}}}= {\\boldsymbol{Y}}{\\boldsymbol{X}}^T ({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1}. \\] Suppose we extend this so that \\({\\boldsymbol{Y}}_{i} | {\\boldsymbol{X}}, {\\boldsymbol{\\beta}}_i \\sim \\mbox{MVN}_n({\\boldsymbol{\\beta}}_i {\\boldsymbol{X}}, \\sigma^2 {\\boldsymbol{I}})\\) and \\({\\boldsymbol{\\beta}}_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{MVN}_d({\\boldsymbol{u}}, {\\boldsymbol{V}})\\). Since \\(\\hat{{\\boldsymbol{\\beta}}}_i | {\\boldsymbol{\\beta}}_i \\sim \\mbox{MVN}_d({\\boldsymbol{\\beta}}_i, \\sigma^2({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1})\\), it follows that marginally \\[ \\hat{{\\boldsymbol{\\beta}}}_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{MVN}_d({\\boldsymbol{u}}, \\sigma^2({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} + {\\boldsymbol{V}}). \\] Therefore, \\[ \\hat{{\\boldsymbol{u}}} = \\frac{\\sum_{i=1}^m \\hat{{\\boldsymbol{\\beta}}}_i}{m} \\] \\[ \\hat{{\\boldsymbol{V}}} = \\hat{{\\operatorname{Cov}}}\\left(\\hat{{\\boldsymbol{\\beta}}}\\right) - \\hat{\\sigma}^2({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1} \\] where \\(\\hat{{\\operatorname{Cov}}}\\left(\\hat{{\\boldsymbol{\\beta}}}\\right)\\) is the \\(d \\times d\\) sample covariance (or MLE covariance) of the \\(\\hat{{\\boldsymbol{\\beta}}}_i\\) estimates. Also, \\(\\hat{\\sigma}^2\\) is obtained by averaging the estimate over all \\(m\\) regressions. We then do inference based on the prior distribution \\({\\boldsymbol{\\beta}}_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{MVN}_d(\\hat{{\\boldsymbol{u}}}, \\hat{{\\boldsymbol{V}}})\\). The posterior distribution of \\({\\boldsymbol{\\beta}}_i | {\\boldsymbol{Y}}, {\\boldsymbol{X}}\\) is MVN with mean \\[ \\left(\\frac{1}{\\hat{\\sigma}^2}({\\boldsymbol{X}}{\\boldsymbol{X}}^T) + \\hat{{\\boldsymbol{V}}}^{-1}\\right)^{-1} \\left(\\frac{1}{\\hat{\\sigma}^2}({\\boldsymbol{X}}{\\boldsymbol{X}}^T) \\hat{{\\boldsymbol{\\beta}}}_i + \\hat{{\\boldsymbol{V}}}^{-1} \\hat{{\\boldsymbol{u}}} \\right) \\] and covariance \\[ \\left(\\frac{1}{\\hat{\\sigma}^2}({\\boldsymbol{X}}{\\boldsymbol{X}}^T) + \\hat{{\\boldsymbol{V}}}^{-1}\\right)^{-1}. \\] "],
["multiple-testing.html", "84 Multiple Testing 84.1 Motivating Example 84.2 Challenges 84.3 Outcomes 84.4 Error Rates 84.5 Bonferroni Correction 84.6 False Discovery Rate 84.7 Point Estimate 84.8 Adaptive Threshold 84.9 Conservative Properties 84.10 Q-Values 84.11 Bayesian Mixture Model 84.12 Bayesian-Frequentist Connection 84.13 Local FDR", " 84 Multiple Testing 84.1 Motivating Example Hedenfalk et al. (2001) NEJM measured gene expression in three different breast cancer tumor types. In your homework, you have analyzed these data and have specifically compared BRCA1 mutation positive tumors to BRCA2 mutation positive tumors. The qvalue package has the p-values when testing for a difference in population means between these two groups (called “differential expression”). There are 3170 genes tested, resulting in 3170 p-values. Note that this analysis is a version of the many responses model. &gt; library(qvalue) &gt; data(hedenfalk); df &lt;- data.frame(p=hedenfalk$p) &gt; ggplot(df, aes(x = p)) + + ggtitle(&quot;p-value density histogram&quot;) + + geom_histogram(aes_string(y = &#39;..density..&#39;), colour = &quot;black&quot;, + fill = &quot;white&quot;, binwidth = 0.04, center=0.02) 84.2 Challenges Traditional p-value thresholds such as 0.01 or 0.05 may result in too many false positives. For example, in the above example, a 0.05 threshold could result in 158 false positives. A careful balance of true positives and false positives must be achieved in a manner that is scientifically interpretable. There is information in the joint distribution of the p-values that can be leveraged. Dependent p-values may make this type of analysis especially difficult (next week’s topic). &gt; qobj &lt;- qvalue(hedenfalk$p) &gt; hist(qobj) 84.3 Outcomes Possible outcomes from \\(m\\) hypothesis tests based on applying a significance threshold \\(0 &lt; t \\leq 1\\) to their corresponding p-values. Not Significant Significant Total Null True \\(U\\) \\(V\\) \\(m_0\\) Alternative True \\(T\\) \\(S\\) \\(m_1\\) Total \\(W\\) \\(R\\) \\(m\\) 84.4 Error Rates Suppose we are testing \\(m\\) hypotheses based on p-values \\(p_1, p_2, \\ldots, p_m\\). Multiple hypothesis testing is the process of deciding which of these p-values should be called statistically significant. This requires formulating and estimating a compound error rate that quantifies the quality of the decision. 84.5 Bonferroni Correction The family-wise error rate is the probability of any false positive occurring among all tests called significant. The Bonferroni correction is a result that shows that utilizing a p-value threshold of \\(\\alpha/m\\) results in FWER \\(\\leq \\alpha\\). Specifically, \\[ \\begin{aligned} \\text{FWER} &amp; \\leq \\Pr(\\cup \\{P_i \\leq \\alpha/m\\}) \\\\ &amp; \\leq \\sum_{i=1}^m \\Pr(P_i \\leq \\alpha/m) = \\sum_{i=1}^m \\alpha/m = \\alpha \\end{aligned} \\] where the above probability calculations are done under the assumption that all \\(H_0\\) are true. 84.6 False Discovery Rate The false discovery rate (FDR) measures the proportion of Type I errors — or “false discoveries” — among all hypothesis tests called statistically significant. It is defined as \\[ {{\\rm FDR}}= {\\operatorname{E}}\\left[ \\frac{V}{R \\vee 1} \\right] = {\\operatorname{E}}\\left[ \\left. \\frac{V}{R} \\right| R&gt;0 \\right] \\Pr(R&gt;0). \\] This is less conservative than the FWER and it offers a clearer balance between true positives and false positives. There are two other false discovery rate definitions, where the main difference is in how the \\(R=0\\) event is handled. These quantities are called the positive false discovery rate (pFDR) and the marginal false discovery rate (mFDR), defined as follows: \\[ {{\\rm pFDR}}= {\\operatorname{E}}\\left[ \\left. \\frac{V}{R} \\right| R&gt;0 \\right], \\] \\[ {{\\rm mFDR}}= \\frac{{\\operatorname{E}}\\left[ V \\right]}{{\\operatorname{E}}\\left[ R \\right]}. \\] Note that \\({{\\rm pFDR}}= {{\\rm mFDR}}= 1\\) whenever all null hypotheses are true, whereas FDR can always be made arbitrarily small because of the extra term \\(\\Pr(R &gt; 0)\\). 84.7 Point Estimate Let \\({{\\rm FDR}}(t)\\) denote the FDR when calling null hypotheses significant whenever \\(p_i \\leq t\\), for \\(i = 1, 2, \\ldots, m\\). For \\(0 &lt; t \\leq 1\\), we define the following random variables: \\[ \\begin{aligned} V(t) &amp; = \\#\\{\\mbox{true null } p_i: p_i \\leq t \\} \\\\ R(t) &amp; = \\#\\{p_i: p_i \\leq t\\} \\end{aligned} \\] In terms of these, we have \\[ {{\\rm FDR}}(t) = {\\operatorname{E}}\\left[ \\frac{V(t)}{R(t) \\vee 1} \\right]. \\] For fixed \\(t\\), the following defines a family of conservatively biased point estimates of \\({{\\rm FDR}}(t)\\): \\[ \\hat{{{\\rm FDR}}}(t) = \\frac{\\hat{m}_0(\\lambda) \\cdot t}{[R(t) \\vee 1]}. \\] The term \\(\\hat{m}_0({\\lambda})\\) is an estimate of \\(m_0\\), the number of true null hypotheses. This estimate depends on the tuning parameter \\({\\lambda}\\), and it is defined as \\[ \\hat{m}_0({\\lambda}) = \\frac{m - R({\\lambda})}{(1-{\\lambda})}. \\] Sometimes instead of \\(m_0\\), the quantity \\[ \\pi_0 = \\frac{m_0}{m} \\] is estimated, where simply \\[ \\hat{\\pi}_0({\\lambda}) = \\frac{\\hat{m}_0({\\lambda})}{m} = \\frac{m - R({\\lambda})}{m(1-{\\lambda})}. \\] It can be shown that \\({\\operatorname{E}}[\\hat{m}_0({\\lambda})] \\geq m_0\\) when the p-values corresponding to the true null hypotheses are Uniform(0,1) distributed (or stochastically greater). There is an inherent bias/variance trade-off in the choice of \\({\\lambda}\\). In most cases, when \\({\\lambda}\\) gets smaller, the bias of \\(\\hat{m}_0({\\lambda})\\) gets larger, but the variance gets smaller. Therefore, \\({\\lambda}\\) can be chosen to try to balance this trade-off. 84.8 Adaptive Threshold If we desire a FDR level of \\(\\alpha\\), it is tempting to use the p-value threshold \\[ t^*_\\alpha = \\max \\left\\{ t: \\hat{{{\\rm FDR}}}(t) \\leq \\alpha \\right\\} \\] which identifies the largest estimated FDR less than or equal to \\(\\alpha\\). 84.9 Conservative Properties When the p-value corresponding to true null hypothesis are distributed iid Uniform(0,1), then we have the following two conservative properties. \\[ \\begin{aligned} &amp; {\\operatorname{E}}\\left[ \\hat{{{\\rm FDR}}}(t) \\right] \\geq {{\\rm FDR}}(t) \\\\ &amp; {\\operatorname{E}}\\left[ \\hat{{{\\rm FDR}}}(t^*_\\alpha) \\right] \\leq \\alpha \\end{aligned} \\] 84.10 Q-Values In single hypothesis testing, it is common to report the p-value as a measure of significance. The q-value is the FDR based measure of significance that can be calculated simultaneously for multiple hypothesis tests. The p-value is constructed so that a threshold of \\(\\alpha\\) results in a Type I error rate \\(\\leq \\alpha\\). Likewise, the q-value is constructed so that a threshold of \\(\\alpha\\) results in a FDR \\(\\leq \\alpha\\). Initially it seems that the q-value should capture the FDR incurred when the significance threshold is set at the p-value itself, \\({{\\rm FDR}}(p_i)\\). However, unlike Type I error rates, the FDR is not necessarily strictly increasing with an increasing significance threshold. To accommodate this property, the q-value is defined to be the minimum FDR (or pFDR) at which the test is called significant: \\[ {\\operatorname{q}}{\\rm\\mbox{-}value}(p_i) = \\min_{t \\geq p_i} {{\\rm FDR}}(t) \\] or \\[ {\\operatorname{q}}{\\rm\\mbox{-}value}(p_i) = \\min_{t \\geq p_i} {{\\rm pFDR}}(t). \\] To estimate this in practice, a simple plug-in estimate is formed, for example: \\[ \\hat{{\\operatorname{q}}}{\\rm\\mbox{-}value}(p_i) = \\min_{t \\geq p_i} \\hat{{{\\rm FDR}}}(t). \\] Various theoretical properties have been shown for these estimates under certain conditions, notably that the estimated q-values of the entire set of tests are simultaneously conservative as the number of hypothesis tests grows large. &gt; plot(qobj) 84.11 Bayesian Mixture Model Let’s return to the Bayesian classification set up from earlier. Suppose that \\(H_i =\\) 0 or 1 according to whether the \\(i\\)th null hypothesis is true or not \\(H_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}\\mbox{Bernoulli}(1-\\pi_0)\\) so that \\(\\Pr(H_i=0)=\\pi_0\\) and \\(\\Pr(H_i=1)=1-\\pi_0\\) \\(P_i | H_i {\\; \\stackrel{\\text{iid}}{\\sim}\\;}(1-H_i) \\cdot F_0 + H_i \\cdot F_1\\), where \\(F_0\\) is the null distribution and \\(F_1\\) is the alternative distribution 84.12 Bayesian-Frequentist Connection Under these assumptions, it has been shown that \\[ \\begin{aligned} {{\\rm pFDR}}(t) &amp; = {\\operatorname{E}}\\left[ \\left. \\frac{V(t)}{R(t)} \\right| R(t) &gt; 0 \\right] \\\\ \\ &amp; = \\Pr(H_i = 0 | P_i \\leq t) \\end{aligned} \\] where \\(\\Pr(H_i = 0 | P_i \\leq t)\\) is the same for each \\(i\\) because of the iid assumptions. Under these modeling assumptions, it follows that \\[ \\mbox{q-value}(p_i) = \\min_{t \\geq p_i} \\Pr(H_i = 0 | P_i \\leq t) \\] which is a Bayesian analogue of the p-value — or rather a “Bayesian posterior Type I error rate”. 84.13 Local FDR In this scenario, it also follows that \\[ {{\\rm pFDR}}(t) = \\int \\Pr(H_i = 0 | P_i = p_i) dF(p_i | p_i \\leq t) \\] where \\(F = \\pi_0 F_0 + (1-\\pi_0) F_1\\). This connects the pFDR to the posterior error probability \\[\\Pr(H_i = 0 | P_i = p_i)\\] making this latter quantity sometimes interpreted as a local false discovery rate. &gt; hist(qobj) "],
["many-regressors-model-1.html", "85 Many Regressors Model", " 85 Many Regressors Model "],
["ridge-regression.html", "86 Ridge Regression 86.1 Motivation 86.2 Optimization Goal 86.3 Solution 86.4 Preprocessing 86.5 Shrinkage 86.6 Example 86.7 Existence of Solution 86.8 Effective Degrees of Freedom 86.9 Bias and Covariance 86.10 Ridge vs OLS 86.11 Bayesian Interpretation 86.12 Example: Diabetes Data 86.13 GLMs", " 86 Ridge Regression 86.1 Motivation Ridge regression is a technique for shrinking the coefficients towards zero in linear models. It also deals with collinearity among explanatory variables. Collinearity is the presence of strong correlation among two or more explanatory variables. 86.2 Optimization Goal Under the OLS model assumptions, ridge regression fits model by minimizing the following: \\[ \\sum_{j=1}^n \\left(y_j - \\sum_{i=1}^m \\beta_{i} x_{ij}\\right)^2 + \\lambda \\sum_{k=1}^m \\beta_k^2. \\] Recall the \\(\\ell_2\\) norm: \\(\\sum_{k=1}^m \\beta_k^2 = \\| {\\boldsymbol{\\beta}}\\|^2_2\\). Sometimes ridge regression is called \\(\\ell_2\\) penalized regression. As with natural cubic splines, the paramater \\(\\lambda\\) is a tuning paramater that controls how much shrinkage occurs. 86.3 Solution The ridge regression solution is \\[ \\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}} = {\\boldsymbol{y}}{\\boldsymbol{X}}^T \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1}. \\] As \\(\\lambda \\rightarrow 0\\), the \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}} \\rightarrow \\hat{{\\boldsymbol{\\beta}}}^{\\text{OLS}}\\). As \\(\\lambda \\rightarrow \\infty\\), the \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}} \\rightarrow {\\boldsymbol{0}}\\). 86.4 Preprocessing Implicitly… We mean center \\({\\boldsymbol{y}}\\). We also mean center and standard deviation scale each explanatory variable. Why? 86.5 Shrinkage When \\({\\boldsymbol{X}}{\\boldsymbol{X}}^T = {\\boldsymbol{I}}\\), then \\[ \\hat{\\beta}^{\\text{Ridge}}_{j} = \\frac{\\hat{\\beta}^{\\text{OLS}}_{j}}{1+\\lambda}. \\] This shows how ridge regression acts as a technique for shrinking regression coefficients towards zero. It also shows that when \\(\\hat{\\beta}^{\\text{OLS}}_{j} \\not= 0\\), then for all finite \\(\\lambda\\), \\(\\hat{\\beta}^{\\text{Ridge}}_{j} \\not= 0\\). 86.6 Example &gt; set.seed(508) &gt; x1 &lt;- rnorm(20) &gt; x2 &lt;- x1 + rnorm(20, sd=0.1) &gt; y &lt;- 1 + x1 + x2 + rnorm(20) &gt; tidy(lm(y~x1+x2)) # A tibble: 3 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 0.965 0.204 4.74 0.000191 2 x1 0.493 2.81 0.175 0.863 3 x2 1.26 2.89 0.436 0.668 &gt; lm.ridge(y~x1+x2, lambda=1) # from MASS package x1 x2 0.9486116 0.8252948 0.8751979 86.7 Existence of Solution When \\(m &gt; n\\) or when there is high collinearity, then \\(({\\boldsymbol{X}}{\\boldsymbol{X}}^T)^{-1}\\) will not exist. However, for \\(\\lambda &gt; 0\\), it is always the case that \\(\\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1}\\) exists. Therefore, one can always compute a unique \\(\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}}\\) for each \\(\\lambda &gt; 0\\). 86.8 Effective Degrees of Freedom Similarly to natural cubic splines, we can calculate an effective degrees of freedom by noting that: \\[ \\hat{{\\boldsymbol{y}}} = {\\boldsymbol{y}}{\\boldsymbol{X}}^T \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1} {\\boldsymbol{X}}\\] The effective degrees of freedom is then the trace of the linear operator: \\[ \\operatorname{tr} \\left({\\boldsymbol{X}}^T \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1} {\\boldsymbol{X}}\\right) \\] 86.9 Bias and Covariance Under the OLS model assumptions, \\[ {\\operatorname{Cov}}\\left(\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}}\\right) = \\sigma^2 \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1} {\\boldsymbol{X}}{\\boldsymbol{X}}^T \\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1} \\] and \\[ \\text{bias} = {\\operatorname{E}}\\left[\\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}}\\right] - {\\boldsymbol{\\beta}}= - \\lambda {\\boldsymbol{\\beta}}\\left({\\boldsymbol{X}}{\\boldsymbol{X}}^T + \\lambda {\\boldsymbol{I}}\\right)^{-1}. \\] 86.10 Ridge vs OLS When the OLS model is true, there exists a \\(\\lambda &gt; 0\\) such that the MSE of the ridge estimate is lower than than of the OLS estimate: \\[ {\\operatorname{E}}\\left[ \\| {\\boldsymbol{\\beta}}- \\hat{{\\boldsymbol{\\beta}}}^{\\text{Ridge}} \\|^2_2 \\right] &lt; {\\operatorname{E}}\\left[ \\| {\\boldsymbol{\\beta}}- \\hat{{\\boldsymbol{\\beta}}}^{\\text{OLS}} \\|^2_2 \\right]. \\] This says that by sacrificing some bias in the ridge estimator, we can obtain a smaller overall MSE, which is bias\\(^2\\) + variance. 86.11 Bayesian Interpretation The ridge regression solution is equivalent to maximizing \\[ -\\frac{1}{2\\sigma^2}\\sum_{j=1}^n \\left(y_j - \\sum_{i=1}^m \\beta_{i} x_{ij}\\right)^2 - \\frac{\\lambda}{2\\sigma^2} \\sum_{k=1}^m \\beta_k^2 \\] which means it can be interpreted as the MAP solution with a Normal prior on the \\(\\beta_i\\) values. 86.12 Example: Diabetes Data &gt; library(lars) &gt; data(diabetes) &gt; x &lt;- diabetes$x2 %&gt;% unclass() %&gt;% as.data.frame() &gt; y &lt;- diabetes$y &gt; dim(x) [1] 442 64 &gt; length(y) [1] 442 &gt; df &lt;- cbind(x,y) &gt; names(df) [1] &quot;age&quot; &quot;sex&quot; &quot;bmi&quot; &quot;map&quot; &quot;tc&quot; &quot;ldl&quot; &quot;hdl&quot; [8] &quot;tch&quot; &quot;ltg&quot; &quot;glu&quot; &quot;age^2&quot; &quot;bmi^2&quot; &quot;map^2&quot; &quot;tc^2&quot; [15] &quot;ldl^2&quot; &quot;hdl^2&quot; &quot;tch^2&quot; &quot;ltg^2&quot; &quot;glu^2&quot; &quot;age:sex&quot; &quot;age:bmi&quot; [22] &quot;age:map&quot; &quot;age:tc&quot; &quot;age:ldl&quot; &quot;age:hdl&quot; &quot;age:tch&quot; &quot;age:ltg&quot; &quot;age:glu&quot; [29] &quot;sex:bmi&quot; &quot;sex:map&quot; &quot;sex:tc&quot; &quot;sex:ldl&quot; &quot;sex:hdl&quot; &quot;sex:tch&quot; &quot;sex:ltg&quot; [36] &quot;sex:glu&quot; &quot;bmi:map&quot; &quot;bmi:tc&quot; &quot;bmi:ldl&quot; &quot;bmi:hdl&quot; &quot;bmi:tch&quot; &quot;bmi:ltg&quot; [43] &quot;bmi:glu&quot; &quot;map:tc&quot; &quot;map:ldl&quot; &quot;map:hdl&quot; &quot;map:tch&quot; &quot;map:ltg&quot; &quot;map:glu&quot; [50] &quot;tc:ldl&quot; &quot;tc:hdl&quot; &quot;tc:tch&quot; &quot;tc:ltg&quot; &quot;tc:glu&quot; &quot;ldl:hdl&quot; &quot;ldl:tch&quot; [57] &quot;ldl:ltg&quot; &quot;ldl:glu&quot; &quot;hdl:tch&quot; &quot;hdl:ltg&quot; &quot;hdl:glu&quot; &quot;tch:ltg&quot; &quot;tch:glu&quot; [64] &quot;ltg:glu&quot; &quot;y&quot; The glmnet() function will perform ridge regression when we set alpha=0. &gt; library(glmnetUtils) &gt; ridgefit &lt;- glmnetUtils::glmnet(y ~ ., data=df, alpha=0) &gt; plot(ridgefit) Cross-validation to tune the shrinkage parameter. &gt; cvridgefit &lt;- glmnetUtils::cv.glmnet(y ~ ., data=df, alpha=0) &gt; plot(cvridgefit) 86.13 GLMs The glmnet library (and the glmnetUtils wrapper library) allow one to perform ridge regression on generalized linear models. A penalized maximum likelihood estimate is calculated based on \\[ -\\lambda \\sum_{i=1}^m \\beta_i^2 \\] added to the log-likelihood. "],
["lasso-regression.html", "87 Lasso Regression 87.1 Motivation 87.2 Optimization Goal 87.3 Solution 87.4 Preprocessing 87.5 Bayesian Interpretation 87.6 Inference 87.7 GLMs", " 87 Lasso Regression 87.1 Motivation One drawback of the ridge regression approach is that coefficients will be small, but they will be nonzero. An alternative appraoch is the lasso, which stands for “Least Absolute Shrinkage and Selection Operator”. This performs a similar optimization as ridge, but with an \\(\\ell_1\\) penalty instead. This changes the geometry of the problem so that coefficients may be zero. 87.2 Optimization Goal Starting with the OLS model assumptions again, we wish to find \\({\\boldsymbol{\\beta}}\\) that minimizes \\[ \\sum_{j=1}^n \\left(y_j - \\sum_{i=1}^m \\beta_{i} x_{ij}\\right)^2 + \\lambda \\sum_{k=1}^m |\\beta_k|. \\] Note that \\(\\sum_{k=1}^m |\\beta_k| = \\| {\\boldsymbol{\\beta}}\\|_1\\), which is the \\(\\ell_1\\) vector norm. As before, the paramater \\(\\lambda\\) is a tuning paramater that controls how much shrinkage and selection occurs. 87.3 Solution There is no closed form solution to this optimization problem, so it must be solved numerically. Originally, a quadratic programming solution was proposed with has \\(O(n 2^m)\\) operations. Then a least angle regression solution reduced the solution to \\(O(nm^2)\\) operations. Modern coordinate descent methods have further reduced this to \\(O(nm)\\) operations. 87.4 Preprocessing Implicitly… We mean center \\({\\boldsymbol{y}}\\). We also mean center and standard deviation scale each explanatory variable. Why? Let’s return to the diabetes data set. To do lasso regression, we set alpha=1. &gt; lassofit &lt;- glmnetUtils::glmnet(y ~ ., data=df, alpha=1) &gt; plot(lassofit) Cross-validation to tune the shrinkage parameter. &gt; cvlassofit &lt;- glmnetUtils::cv.glmnet(y ~ ., data=df, alpha=1) &gt; plot(cvlassofit) 87.5 Bayesian Interpretation The ridge regression solution is equivalent to maximizing \\[ -\\frac{1}{2\\sigma^2}\\sum_{j=1}^n \\left(y_j - \\sum_{i=1}^m \\beta_{i} x_{ij}\\right)^2 - \\frac{\\lambda}{2\\sigma^2} \\sum_{k=1}^m |\\beta_k| \\] which means it can be interpreted as the MAP solution with a Exponential prior on the \\(\\beta_i\\) values. 87.6 Inference Inference on the lasso model fit is difficult. However, there has been recent progress. One idea proposes a conditional covariance statistic, but this requires all explanatory variables to be uncorrelated. Another idea called the knockoff filter controls the false discovery rate and allows for correlation among explanatory variables. Both of these ideas have some restrictive assumptions and require the number of observations to exceed the number of explanatory variables, \\(n &gt; m\\). 87.7 GLMs The glmnet library (and the glmnetUtils wrapper library) allow one to perform lasso regression on generalized linear models. A penalized maximum likelihood estimate is calculated based on \\[ -\\lambda \\sum_{i=1}^m |\\beta_i| \\] added to the log-likelihood. "],
["hd-latent-variable-models.html", "88 HD Latent Variable Models 88.1 Definition 88.2 Model 88.3 Estimation", " 88 HD Latent Variable Models 88.1 Definition Latent variables (or hidden variables) are random variables that are present in the underlying probabilistic model of the data, but they are unobserved. In high-dimensional data, there may be latent variables present that affect many variables simultaneously. These are latent variables that induce systematic variation. A topic of much interest is how to estimate these and incorporate them into further HD inference procedures. 88.2 Model Suppose we have observed data \\({\\boldsymbol{Y}}_{m \\times n}\\) of \\(m\\) variables with \\(n\\) observations each. Suppose there are \\(r\\) latent variables contained in the \\(r\\) rows of \\({\\boldsymbol{Z}}_{r \\times n}\\) where \\[ {\\operatorname{E}}\\left[{\\boldsymbol{Y}}_{m \\times n} \\left. \\right| {\\boldsymbol{Z}}_{r \\times n} \\right] = {\\boldsymbol{\\Phi}}_{m \\times r} {\\boldsymbol{Z}}_{r \\times n}. \\] Let’s also assume that \\(m \\gg n &gt; r\\). The latent variables \\({\\boldsymbol{Z}}\\) induce systematic variation in variable \\({\\boldsymbol{y}}_i\\) parameterized by \\({\\boldsymbol{\\phi}}_i\\) for \\(i = 1, 2, \\ldots, m\\). 88.3 Estimation There exist methods for estimating the row space of \\({\\boldsymbol{Z}}\\) with probability 1 as \\(m \\rightarrow \\infty\\) for a fixed \\(n\\) in two scenarios. Leek (2011) shows how to do this when \\({\\boldsymbol{y}}_i | {\\boldsymbol{Z}}\\sim \\text{MVN}({\\boldsymbol{\\phi}}_i {\\boldsymbol{Z}}, \\sigma^2_i {\\boldsymbol{I}})\\), and the \\({\\boldsymbol{y}}_i | {\\boldsymbol{Z}}\\) are jointly independent. Chen and Storey (2015) show how to do this when the \\({\\boldsymbol{y}}_i | {\\boldsymbol{Z}}\\) are distributed according to a single parameter exponential family distribution with mean \\({\\boldsymbol{\\phi}}_i {\\boldsymbol{Z}}\\), and the \\({\\boldsymbol{y}}_i | {\\boldsymbol{Z}}\\) are jointly independent. "],
["jackstraw.html", "89 Jackstraw 89.1 Procedure 89.2 Example: Yeast Cell Cycle", " 89 Jackstraw Suppose we have a reasonable method for estimating \\({\\boldsymbol{Z}}\\) in the model \\[ {\\operatorname{E}}\\left[{\\boldsymbol{Y}}\\left. \\right| {\\boldsymbol{Z}}\\right] = {\\boldsymbol{\\Phi}}{\\boldsymbol{Z}}. \\] The jackstraw method allows us to perform hypothesis tests of the form \\[ H_0: {\\boldsymbol{\\phi}}_i = {\\boldsymbol{0}}\\mbox{ vs } H_1: {\\boldsymbol{\\phi}}_i \\not= {\\boldsymbol{0}}. \\] We can also perform this hypothesis test on any subset of the columns of \\({\\boldsymbol{\\Phi}}\\). This is a challening problem because we have to “double dip” in the data \\({\\boldsymbol{Y}}\\), first to estimate \\({\\boldsymbol{Z}}\\), and second to perform significance tests on \\({\\boldsymbol{\\Phi}}\\). 89.1 Procedure The first step is to form estimate \\(\\hat{{\\boldsymbol{Z}}}\\) and then test statistic \\(t_i\\) that performs the hypothesis test for each \\({\\boldsymbol{\\phi}}_i\\) from \\({\\boldsymbol{y}}_i\\) and \\(\\hat{{\\boldsymbol{Z}}}\\) (\\(i=1, \\ldots, m\\)). Assume that the larger \\(t_i\\) is, the more evidence there is against the null hypothesis in favor of the alternative. Next we randomly select \\(s\\) rows of \\({\\boldsymbol{Y}}\\) and permute them to create data set \\({\\boldsymbol{Y}}^{0}\\). Let this set of \\(s\\) variables be indexed by \\(\\mathcal{S}\\). This breaks the relationship between \\({\\boldsymbol{y}}_i\\) and \\({\\boldsymbol{Z}}\\), thereby inducing a true \\(H_0\\), for each \\(i \\in \\mathcal{S}\\). We estimate \\(\\hat{{\\boldsymbol{Z}}}^{0}\\) from \\({\\boldsymbol{Y}}^{0}\\) and again obtain test statistics \\(t_i^{0}\\). Specifically, the test statistics \\(t_i^{0}\\) for \\(i \\in \\mathcal{S}\\) are saved as draws from the null distribution. We repeat permutation procedure \\(B\\) times, and then utilize all saved \\(sB\\) permutation null statistics to calculate empirical p-values: \\[ p_i = \\frac{1}{sB} \\sum_{b=1}^B \\sum_{k \\in \\mathcal{S}_b} 1\\left(t_i \\geq t_k^{0b} \\right). \\] 89.2 Example: Yeast Cell Cycle Recall the yeast cell cycle data from earlier. We will test which genes have expression significantly associated with PC1 and PC2 since these both capture cell cycle regulation. &gt; load(&quot;./data/spellman.RData&quot;) &gt; time [1] 0 30 60 90 120 150 180 210 240 270 330 360 390 &gt; dim(gene_expression) [1] 5981 13 &gt; dat &lt;- t(scale(t(gene_expression), center=TRUE, scale=FALSE)) Test for associations between PC1 and each gene, conditioning on PC1 and PC2 being relevant sources of systematic variation. &gt; jsobj &lt;- jackstraw_pca(dat, r1=1, r=2, B=500, s=50, verbose=FALSE) &gt; jsobj$p.value %&gt;% qvalue() %&gt;% hist() This is the most significant gene plotted with PC1. Test for associations between PC2 and each gene, conditioning on PC1 and PC2 being relevant sources of systematic variation. &gt; jsobj &lt;- jackstraw_pca(dat, r1=2, r=2, B=500, s=50, verbose=FALSE) &gt; jsobj$p.value %&gt;% qvalue() %&gt;% hist() This is the most significant gene plotted with PC2. "],
["surrogate-variable-analysis.html", "90 Surrogate Variable Analysis 90.1 Procedure 90.2 Example: Kidney Expr by Age", " 90 Surrogate Variable Analysis The surrogate variable analysis (SVA) model combines the many responses model with the latent variable model introduced above: \\[ {\\boldsymbol{Y}}_{m \\times n} = {\\boldsymbol{B}}_{m \\times d} {\\boldsymbol{X}}_{d \\times n} + {\\boldsymbol{\\Phi}}_{m \\times r} {\\boldsymbol{Z}}_{r \\times n} + {\\boldsymbol{E}}_{m \\times n} \\] where \\(m \\gg n &gt; d + r\\). Here, only \\({\\boldsymbol{Y}}\\) and \\({\\boldsymbol{X}}\\) are observed, so we must combine many regressors model fitting techniques with latent variable estimation. The variables \\({\\boldsymbol{Z}}\\) are called surrogate variables for what would be a complete model of all systematic variation. 90.1 Procedure The main challenge is that the row spaces of \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{Z}}\\) may overlap. Even when \\({\\boldsymbol{X}}\\) is the result of a randomized experiment, there will be a high probability that the row spaces of \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{Z}}\\) have some overlap. Therefore, one cannot simply estimate \\({\\boldsymbol{Z}}\\) by applying a latent variable esitmation method on the residuals \\({\\boldsymbol{Y}}- \\hat{{\\boldsymbol{B}}} {\\boldsymbol{X}}\\) or on the observed response data \\({\\boldsymbol{Y}}\\). In the former case, we will only estimate \\({\\boldsymbol{Z}}\\) in the space orthogonal to \\(\\hat{{\\boldsymbol{B}}} {\\boldsymbol{X}}\\). In the latter case, the estimate of \\({\\boldsymbol{Z}}\\) may modify the signal we can estimate in \\({\\boldsymbol{B}}{\\boldsymbol{X}}\\). A recent method, takes an EM approach to esitmating \\({\\boldsymbol{Z}}\\) in the model \\[ {\\boldsymbol{Y}}_{m \\times n} = {\\boldsymbol{B}}_{m \\times d} {\\boldsymbol{X}}_{d \\times n} + {\\boldsymbol{\\Phi}}_{m \\times r} {\\boldsymbol{Z}}_{r \\times n} + {\\boldsymbol{E}}_{m \\times n}. \\] It is shown to be necessary to penalize the likelihood in the estimation of \\({\\boldsymbol{B}}\\) — i.e., form shrinkage estimates of \\({\\boldsymbol{B}}\\) — in order to properly balance the row spaces of \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{Z}}\\). The regularized EM algorithm, called cross-dimensonal inference (CDI) iterates between Estimate \\({\\boldsymbol{Z}}\\) from \\({\\boldsymbol{Y}}- \\hat{{\\boldsymbol{B}}}^{\\text{Reg}} {\\boldsymbol{X}}\\) Estimate \\({\\boldsymbol{B}}\\) from \\({\\boldsymbol{Y}}- \\hat{{\\boldsymbol{\\Phi}}} \\hat{{\\boldsymbol{Z}}}\\) where \\(\\hat{{\\boldsymbol{B}}}^{\\text{Reg}}\\) is a regularized or shrunken estimate of \\({\\boldsymbol{B}}\\). It can be shown that when the regularization can be represented by a prior distribution on \\({\\boldsymbol{B}}\\) then this algorithm achieves the MAP. 90.2 Example: Kidney Expr by Age In Storey et al. (2005), we considered a study where kidney samples were obtained on individuals across a range of ages. The goal was to identify genes with expression associated with age. &gt; library(edge) &gt; library(splines) &gt; load(&quot;./data/kidney.RData&quot;) &gt; age &lt;- kidcov$age &gt; sex &lt;- kidcov$sex &gt; dim(kidexpr) [1] 34061 72 &gt; cov &lt;- data.frame(sex = sex, age = age) &gt; null_model &lt;- ~sex &gt; full_model &lt;- ~sex + ns(age, df = 3) &gt; de_obj &lt;- build_models(data = kidexpr, cov = cov, + null.model = null_model, + full.model = full_model) &gt; de_lrt &lt;- lrt(de_obj, nullDistn = &quot;bootstrap&quot;, bs.its = 100, verbose=FALSE) &gt; qobj1 &lt;- qvalueObj(de_lrt) &gt; hist(qobj1) Now that we have completed a standard generalized LRT, let’s estimate \\({\\boldsymbol{Z}}\\) (the surrogate variables) using the sva package as accessed via the edge package. &gt; dim(nullMatrix(de_obj)) [1] 72 2 &gt; de_sva &lt;- apply_sva(de_obj, n.sv=4, method=&quot;irw&quot;, B=10) Number of significant surrogate variables is: 4 Iteration (out of 10 ):1 2 3 4 5 6 7 8 9 10 &gt; dim(nullMatrix(de_sva)) [1] 72 6 &gt; de_svalrt &lt;- lrt(de_sva, nullDistn = &quot;bootstrap&quot;, bs.its = 100, verbose=FALSE) &gt; qobj2 &lt;- qvalueObj(de_svalrt) &gt; hist(qobj2) &gt; summary(qobj1) Call: qvalue(p = pval) pi0: 0.8059662 Cumulative number of significant calls: &lt;1e-04 &lt;0.001 &lt;0.01 &lt;0.025 &lt;0.05 &lt;0.1 &lt;1 p-value 28 175 879 1802 3064 5431 34061 q-value 0 0 2 4 16 30 34061 local FDR 0 0 2 2 8 21 34061 &gt; summary(qobj2) Call: qvalue(p = pval) pi0: 0.6708454 Cumulative number of significant calls: &lt;1e-04 &lt;0.001 &lt;0.01 &lt;0.025 &lt;0.05 &lt;0.1 &lt;1 p-value 26 151 1022 2081 3635 6279 34061 q-value 0 0 0 3 4 47 34061 local FDR 0 0 0 1 3 28 34049 P-values from two analyses are fairly different. &gt; data.frame(lrt=-log10(qobj1$pval), sva=-log10(qobj2$pval)) %&gt;% + ggplot() + geom_point(aes(x=lrt, y=sva), alpha=0.3) + geom_abline() "],
["references.html", "References", " References The following books have served as references while writing Foundations of Applied Statistics: All of Statistics, by Larry Wasserman All of Nonparametric Statistics, by Larry Wasserman The Elements of Data Analytic Style, by Jeff Leek The Elements of Statistical Learning, by Hastie, Tibshirani, and Friedman Nonparametric Regression and Generalized Linear Models: A roughness penalty approach, by Green and Silverman Pattern Recognition and Machine Learning, by Christopher Bishop R for Data Science, by Wickham and Grolemund R Programming for Data Science, by Roger Peng Statistical Inference, by Casella and Berger "]
]
